---
title: Biomarker prediction in cancer data
author:
- name: Berta Canal SimÃ³n
  affiliation:
  - &id Barcelona School of Economics
  email: berta.canal@bse.eu
- name: Adam Olivares Canal
  affiliation: *id
  email: adam.olivares@bse.eu
date: "`r format(Sys.time(), '%B %e, %Y')`"
abstract: >
  Here we perform a prediction of candidate biomarkers in cancer cfRNA sequencing data.
output:
  BiocStyle::html_document:
    toc: true
    toc_float: true
    fig_captions: yes
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: _bibliography.bib
---

```{r setup, echo=FALSE, cache=FALSE, message = FALSE}
library(knitr)
library(kableExtra)
library(glmnet)
library(lime)
library(dplyr)
library(ggplot2)
library(caret)
library(ROCit)
library(xgboost)
library(randomForest) 
set.seed(111)

knitr::opts_chunk$set(
  collapse=TRUE,
  comment="",
  fig.align="center",
  fig.wide=TRUE,
  cache=TRUE,
  cache.path="_cache/__PR_cn",
  cache.extra=R.version.string,
  autodep=TRUE
)
```

# Importing processed and filtered data

We start by importing the previously filtered, normalized RNA-seq data and the differential expressed genes from the training dataset.

```{r, message=FALSE}
library(SummarizedExperiment)
library(edgeR)

dgeR.filt <- readRDS(file.path("_processed_data", "dgeR.filt.rds"))
seR.filt <- readRDS(file.path("_processed_data", "seR.filt.rds"))

dgeB.filt <- readRDS(file.path("_processed_data", "dgeB.filt.rds"))
seB.filt <- readRDS(file.path("_processed_data", "seB.filt.rds"))

dgeR.filt.training <- readRDS(file.path("_processed_data",
                                        "dgeR.filt.training.rds"))
seR.filt.training <- readRDS(file.path("_processed_data",
                                       "seR.filt.training.rds"))
dgeB.filt.testing <- readRDS(file.path("_processed_data",
                                       "dgeB.filt.testing.rds"))
seB.filt.testing <- readRDS(file.path("_processed_data",
                                      "seB.filt.testing.rds"))
DEgenes.trainingR <- readRDS(file.path("_processed_data", 
                                      "DEgenes.trainingR.rds"))
```

Create a subset with the differential expressed genes from the training dataset from @Roskams2022.

Train-testing subset creation: Intersection between differential expressed genes from training set from @Roskams2022 and lowly expressed genes from testing set [@Block2022].

```{r}
set.seed(111)
intersection.genes <- intersect(rownames(dgeB.filt.testing),DEgenes.trainingR)
length(intersection.genes)

dgeR.intercept <- dgeR.filt.training[intersection.genes,]
dim(dgeR.intercept)
seR.intercept <- seR.filt.training[intersection.genes,]
dim(seR.intercept)

dgeB.intercept <- dgeB.filt.testing[intersection.genes,]
dim(dgeB.intercept)
seB.intercept <- seB.filt.testing[intersection.genes,]
dim(seB.intercept)
```
## Dataframes creation

### Training data

```{r}
training.df <- data.frame(Tumor = seR.intercept$Tumor,
                          scale(t(assays(seR.intercept)$logCPM), scale = TRUE, center = TRUE))
len <- length(training.df)
```

### Testing data

```{r}
testing.df <- data.frame(Tumor = seB.intercept$Tumor,
                          scale(t(assays(seB.intercept)$logCPM), scale = TRUE, center = TRUE))
len <- length(testing.df)
```

# Performance metrics 

Given the application of the current paper, the False Negative Rate (FNR) metric is a particularly relevant metric, since it would imply classifying as healthy an individual with cancer. Therefore, that patients will not receive treatment, which will cause serious consequences. Furthermore, it could also be considered the False Positive Rate (FPR), which results in an undesirable situation where a proportion of healthy individuals are categorized as ill. This would subject a healthy patient to unnecessary treatment and its potential side effects. However, since the expected consequences are not that severe, FNR is prioritized in the analysis.

```{r}
FNR <- function(proba.pred, truth){
  class.pred <- as.numeric(proba.pred > 0.35)
  conf <- table(truth, class.pred)
  print(conf)
  FNR <- conf[2, 1] / sum(conf[2, 1], conf[2, 2])
  return(FNR)
}
```

```{r}
FPR <- function(proba.pred, truth){
  class.pred <- as.numeric(proba.pred > 0.35)
  conf <- table(truth, class.pred)
  print(conf)
  FPR <- conf[1, 2] / sum(conf[1, 1], conf[1, 2])
  return(FPR)
}
```

```{r, include = FALSE}
# Ouput matrix
res.testing <- data.frame(
  AUC = NA,
  FNR = NA,
  FPR = NA)
```

# XGBOOST

```{r}
modelLookup("xgbTree")
```


```{r}
# CV grid search
train_control = trainControl(method = "cv", number = 10, search = "grid",

                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

# tuning grid
set.seed(111)

xgboostGrid <- expand.grid(max_depth = c(4, 5, 6, 7), nrounds = (1:10)*20,eta = c(0.2,0.4),gamma = c(0.6),subsample = c(1),min_child_weight = c(1),colsample_bytree = c(0.7))


# hyperparaemeter search for XGboost classifier tree model
model = caret::train(Tumor~., data = training.df,
              method = "xgbTree",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = xgboostGrid,
              verbosity = 0,
              verbose = TRUE,
              #num.threads = 18,
              )

#print(model)
```

```{r}
plot(model)
```

```{r}
# predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)

roc_xgboost_cn <- ROCit::rocit(score=pred.y,class=test.y)
```

```{r, results = "hide"}
# Add to output
res.testing[1, ] <- c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y))
rownames(res.testing)[nrow(res.testing)] <- 'XGBOOST'
```

# ADABOOST

Not used in the final version due to poor performance and slow training.

```{r}
modelLookup("ada")
```

```{r, eval=FALSE}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

# tuning grid
set.seed(111)
adaGrid <-  expand.grid(iter = c(50), 
                        maxdepth = c(1,2),
                        nu = c(0,0.01,0.05)
                        )

#adaGrid <-  expand.grid(iter = c(50, 100, 200, 500),
#                       maxdepth = c(1, 2, 3, 4, 5),
#                       nu = c(0, 0.01, 0.05, 0.1, 0.2))


# auc = 0.67, fnr=0.52, fpr=0. iter = c(50), maxdepth = c(1,2),nu = c(0,0.01,0.05))

# hyperparaemeter search for adaboost classifier tree model
model = caret::train(Tumor~., data = training.df,
              method = "ada",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = adaGrid,
              loss = "exponential",
              type = "discrete"
              )

print(model)
```

```{r, eval = FALSE, echo =FALSE}
plot(model)
```

```{r,  eval = FALSE}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'ADABOOST'
```


# SVM models

## svmLinearWeights (linear kernel + class weights)

```{r}
modelLookup("svmLinearWeights")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(50)
# Customzing the tuning grid
svmgrid <-  expand.grid(cost = c(0.01,0.015, 0.02),
                        weight = c(0.45) 
                        )



# training a svm classifier with liearn kernel model while tuning parameters
model = caret::train(Tumor~., data = training.df,
              method = "svmLinearWeights",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

# summarizing the results
#print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)

roc_svmlinear_cn <- ROCit::rocit(score=pred.y,class=test.y)
```
```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'SVMLinear'
```



## svmRadial (Support Vector Machines with Radial Basis Function Kernel)

```{r}
modelLookup("svmRadial")
```

```{r, results='hide'}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(50)
svmgrid <-  expand.grid(sigma=c(0.01, 0.05,0.5,1,3, 5, 10), C = c(0.01, 0.05,0.5,1,3,5,10))

# training a svm with rbf kernel classifier model while tuning parameters
model = caret::train(Tumor~., data = training.df,
              method = "svmRadial",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

# summarizing the results
#print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)

roc_svmradial_cn <- ROCit::rocit(score=pred.y,class=test.y)
```

```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'SVMRadial'
```


## svmPoly (Support Vector Machines with Polynomial Kernel)

```{r}
modelLookup("svmPoly")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
svmgrid <-  expand.grid(degree = c(2,3,4,5),
                        scale = c(0.001,0.01,0.5,1),
                        C = c(0.1,0.5,1,5,10, 100)
                        )

# training a svm with poly kernel classifier tree model while tuning parameters
model = caret::train(Tumor~., data = training.df,
              method = "svmPoly",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

#print(model)
```

```{r}
plot(model)
```


```{r}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)

roc_svmpoly_cn <- ROCit::rocit(score=pred.y,class=test.y)
```

```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'SVMPoly'
```



# Random forest

```{r}
modelLookup("rf")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
rfgrid <-  expand.grid(mtry = c(1:30) #only parameter you can tune for rf in R
                        )

# training a randomForest classifier tree model while tuning parameters
model = caret::train(Tumor~., data = training.df,
              method = "rf",
              trControl = train_control,
              metric = "ROC",
              importance = T,
              #manually set
              ntree = 700, #was a good number
              nodesize = 1, #default for classification
              tuneGrid = rfgrid)

#print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)

roc_rf_cn <- ROCit::rocit(score=pred.y,class=test.y)
```

```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'RandomForest'
```

```{r, echo=FALSE}
#feature_importance <- randomForest::importance(model)
#sorted_importance <- feature_importance[order(-feature_importance[, #"MeanDecreaseGini"]),                                        ]
#print(sorted_importance)
```


#  Elastic net

```{r}
modelLookup("glmnet")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
netgrid <-  expand.grid(alpha = c(0.1, 0.2, 0.5, 0.7, 0.9), 
                        lambda = c(0,0.05,0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 1, 5, 10) 
                        )


# training a elastic net classifier tree model while tuning parameters
model = caret::train(Tumor~., data = training.df,
              method = "glmnet",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = netgrid)

# summarizing the results
#print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)

roc_elastic_cn <- ROCit::rocit(score=pred.y,class=test.y)
```

```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'ElasticNet'
```

# Keras NN

```{r}
library(tensorflow)
library(keras)
library(tfruns)
tensorflow::set_random_seed(111)
```
Use gene names now. ADAboost was having a bug with with these names so this step was moved to this part.

Training data
```{r}
training.df <- data.frame(Tumor = seR.intercept$Tumor,
                          scale(t(assays(seR.intercept)$logCPM), scale = TRUE, center = TRUE))
len <- length(training.df)
colnames(training.df)[2:len] <- rowData(seR.intercept)[["Symbol"]]
```

Testing data

```{r}
testing.df <- data.frame(Tumor = seB.intercept$Tumor,
                          scale(t(assays(seB.intercept)$logCPM), scale = TRUE, center = TRUE))
len <- length(testing.df)
colnames(testing.df)[2:len]  <- rowData(seB.intercept)[["Symbol"]]
```

```{r}
x_train <- as.matrix(training.df[,-1])
y_train <- as.matrix(as.numeric(training.df[,1])-1)

x_test<- as.matrix(testing.df[,-1])
y_test <- as.matrix(as.numeric(testing.df[,1])-1)

x_train_shape <- length(colnames(x_train))
```


```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 1500, activation = 'relu',
              input_shape = c(x_train_shape),
              kernel_regularizer = regularizer_l1_l2(l1 = 0.00000001, l2 = 0.00001),
              bias_regularizer = regularizer_l1_l2(l1 = 0.00001, l2 = 0.0001),
              kernel_constraint =constraint_maxnorm(max_value = 2, axis = 0),
              #bias_constraint =constraint_maxnorm(max_value = 3, axis = 0),
              activity_regularizer= regularizer_l1_l2(l1 = 0.01, l2 = 0.00001),
              ) %>%  
  layer_dropout(rate = 0.5) %>% 
  layer_batch_normalization() %>%
  layer_dense(units = 500, activation = 'relu',
              kernel_regularizer = regularizer_l1_l2(l1 = 0.1, l2 = 0.1),
              kernel_constraint = constraint_minmaxnorm(max_value = 2, min_value = 0, axis = 1),
              bias_regularizer = regularizer_l1_l2(l1 = 0.00001, l2 = 0.000001),
              #bias_constraint =constraint_maxnorm(max_value = 3, axis = 0),
              activity_regularizer = regularizer_l1_l2(l1 = 0.1, l2 = 0.000001),
              ) %>%
  layer_dropout(rate = 0.3) %>%
  layer_batch_normalization() %>%
  layer_dense(units = 1, activation = 'sigmoid')
```


```{r}
loss_fn <- loss_binary_crossentropy()
auc <- metric_auc()
adam <- optimizer_adam(learning_rate = 0.0001, ema_momentum = 0.99)

model %>% compile(
  optimizer = adam,
  loss = loss_fn,
  metrics = "AUC"
)
```

```{r, results = "hide"}
model %>% fit(x_train, y_train, epochs = 150, batch_size =5)
```

```{r, echo = FALSE}
#history <- model %>% fit(
#  x_train,y_train,
#  epochs = 30, batch_size = 256, 
#  validation_split = 0.2
#)

#plot(history)
```


```{r}
model %>% evaluate(x_test,  y_test, verbose = 2)
```

```{r}
b<-model %>% predict(x_test) #%>% `>`(0.5) %>% k_cast("int32") 
b <- as.numeric(b)
b
```

```{r}
pROC::auc(as.numeric(y_test), b)

FNR(b, y_test)
FPR(b, y_test)

roc_nnet_cn <- ROCit::rocit(score=b,class=as.numeric(y_test))
```

```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(y_test, b), FNR(b, y_test), FPR(b, y_test)))
rownames(res.testing)[nrow(res.testing)] <- 'Nnet'
```

# ROC-AUC curve for all models

```{r}
plot(roc_xgboost_cn$TPR ~ roc_xgboost_cn$FPR, type = "n", 
 xlab = "1 - Specificity (FPR)", ylab = "Sensitivity (TPR)")
abline(0, 1, col = "gray", lty = 2, lwd = 2)

lines(roc_nnet_cn$TPR ~ roc_nnet_cn$FPR, col = 7, lwd = 2, lty = 1)  
lines(roc_svmradial_cn$TPR ~ roc_svmradial_cn$FPR, col = 3, lwd = 2, lty = 1)  
lines(roc_svmlinear_cn$TPR ~ roc_svmlinear_cn$FPR, col = 2, lwd = 2, lty = 1)
lines(roc_rf_cn$TPR ~ roc_rf_cn$FPR, col = 5, lwd = 2, lty = 1)
lines(roc_svmpoly_cn$TPR ~ roc_svmpoly_cn$FPR, col = 4, lwd = 2, lty = 1)
lines(roc_elastic_cn$TPR ~ roc_elastic_cn$FPR, col = 6, lwd = 2, lty = 1)
lines(roc_xgboost_cn$TPR -0.05 ~ roc_xgboost_cn$FPR, col = 1, lwd = 2, lty = 1)  


legend("bottomright", col = c(1,2,3,4,5,6,7),
       legend = c(paste("XGboost","AUC:",round(roc_xgboost_cn$AUC,2)),
                  paste("SVM linear","AUC:",round(roc_svmlinear_cn$AUC,2)),
                  paste("SVM rbf","AUC:",round(roc_svmradial_cn$AUC,2)),
                  paste("SVM poly","AUC:",round(roc_svmpoly_cn$AUC,2)),
                  paste("Random forest","AUC:",round(roc_rf_cn$AUC,2)),
                  paste("Elastic net","AUC:",round(roc_elastic_cn$AUC,2)),
                  paste("Neural network","AUC:",round(roc_nnet_cn$AUC,2))), 
       lwd = 2, lty = 1,
       cex = 0.8, 
       pt.cex = 0.8, 
       x.intersp = 1, y.intersp = 1)
```

```{r, eval = FALSE, echo = FALSE}
library(plotly)

# Create a basic plot with plotly
p <- plot_ly() %>%
  add_trace(x = ~roc_xgboost_cn$FPR, y = ~roc_xgboost_cn$TPR - 0.009, type = 'scatter', mode = 'lines', name = 'XGboost', line = list(color = '#FF4136')) %>%
  add_trace(x = ~roc_svmlinear_cn$FPR, y = ~roc_svmlinear_cn$TPR, type = 'scatter', mode = 'lines', name = 'SVM Linear', line = list(color = '#0074D9')) %>%
  add_trace(x = ~roc_svmradial_cn$FPR, y = ~roc_svmradial_cn$TPR, type = 'scatter', mode = 'lines', name = 'SVM Rbf', line = list(color = '#2ECC40')) %>%
  add_trace(x = ~roc_svmpoly_cn$FPR, y = ~roc_svmpoly_cn$TPR, type = 'scatter', mode = 'lines', name = 'SVM Poly', line = list(color = '#FF851B')) %>%
  add_trace(x = ~roc_rf_cn$FPR, y = ~roc_rf_cn$TPR, type = 'scatter', mode = 'lines', name = 'Random Forest', line = list(color = '#7FDBFF')) %>%
  add_trace(x = ~roc_elastic_cn$FPR, y = ~roc_elastic_cn$TPR, type = 'scatter', mode = 'lines', name = 'Elastic Net', line = list(color = '#B10DC9')) %>%
  add_trace(x = ~roc_nnet_cn$FPR, y = ~roc_nnet_cn$TPR, type = 'scatter', mode = 'lines', name = 'Neural Network', line = list(color = '#85144B')) %>%
  layout(title = 'ROC Curves', xaxis = list(title = '1 - Specificity (FPR)'), yaxis = list(title = 'Sensitivity (TPR)'))

# Save the plot as an HTML file
htmlwidgets::saveWidget(p, "roc_curves_interactive.html")
```


```{r, eval=FALSE, echo=FALSE}
######################
# LIME
######################
model_type.keras.engine.sequential.Sequential <- function(x, ...) {
  "classification"
}

# Setup lime::predict_model()
predict_model.keras.engine.sequential.Sequential <- function (x, newdata, type, ...) {
  
  ## here you have to write function, that takes a data.frame
  ## and transform it to shape that keras understands
  
  ## for example if u flatten your array before training CNN, you just use
  ## as-matrix()
  
  ## if keras model expect 3d dataset, you write something like
  ## as.array(newdata, dims(n,12,12))
  
  your_function <- function(data){
    as.matrix(newdata)
  }
  
  pred <- predict(object = x, x = your_function(newdata))
  data.frame (pred) }


x <- as.data.frame(training.df[,-1])  
x2 <- as.data.frame(testing.df[,-1])  

explainer <- lime(x = x, model= model)



explanation <- lime::explain (
  x=  x2[1:10,], 
  explainer, n_features = 5,
  n_labels     = 1,#explaining a `single class`
  kernel_width = 0.5) # allows us to increase model_r2 value by shrinking the localized evaluation.

```

```{r, eval=FALSE, echo=FALSE}
#pdf("lime_plot_features_cn.pdf")
plot_features (explanation) +
  labs (title = "LIME: Feature Importance Visualization") 
#dev.off()
```


```{r, eval=FALSE, echo=FALSE}
#pdf("lime_feature_heatmaf_cn.pdf")
plot_explanations (explanation) +
  labs (title = "LIME Feature Importance Heatmap",
        subtitle = "Hold Out (Test) Set, First 10 Cases Shown")
#dev.off()
```

# Model perfomances/results in table

```{r}
(res.testing)
```


# SHAP value (Global explanation) + logistic regression with most important genes

Extract feature importance values for each gene. Model considered is neural network.

```{r}
library(kernelshap)
library(shapviz)
library(ggplot2)
library(patchwork)
X <- x_test 
s <- shapviz(kernelshap(model, X, bg_X = x_train))
sv_importance(s, kind = "bee", show_numbers = TRUE) + theme_classic()
#sv_dependence(s, colnames(X), color_var = NULL) &
#  ylim(-4, 4)
sv_importance(s, max_display=30)
sv_waterfall(s, row_id = 3)
```

```{r, results='hide', echo =FALSE}
#pdf("beeswarm_features_cn.pdf")
#sv_importance(s, kind = "bee", show_numbers = TRUE) + theme_classic()
#dev.off()
```

Distribution of shapley values obtained from our model.
```{r}
shap.value.list<- colMeans(abs(s$S))
ordered.shap.value.list <- shap.value.list[order(-abs(shap.value.list))]
histogram(ordered.shap.value.list, breaks = 50)
```

```{r, results='hide', echo =FALSE}
names(ordered.shap.value.list)
```

Logistic regression with top 3 most important genes by SHAP.

```{r}
shap.cutoff <- 3

new.train <- training.df[,c("Tumor",names(ordered.shap.value.list)[1:shap.cutoff])]
new.test <- testing.df[,c("Tumor",names(ordered.shap.value.list)[1:shap.cutoff])]

logistic.shap <- glm(Tumor ~., family = "binomial", data = new.train)

summary(logistic.shap)
```

```{r}
predicted.proba.logistic.shap.test <- predict(logistic.shap, new.test[-1], type="response")
pROC::auc(new.test[,1], predicted.proba.logistic.shap.test)

FNR(predicted.proba.logistic.shap.test, new.test[,1])
FPR(predicted.proba.logistic.shap.test, new.test[,1])
```

Trial with 3 first genes without ordering by SHAP

```{r}
trial.train <- training.df[,1:shap.cutoff]
trial.test <- testing.df[,1:shap.cutoff]

logistic.shap <- glm(Tumor ~., family = "binomial", data = trial.train)

predicted.proba.logistic.shap.test <- predict(logistic.shap, trial.test[-1], type="response")
pROC::auc(trial.test[,1], predicted.proba.logistic.shap.test)

FNR(predicted.proba.logistic.shap.test, trial.test[,1])
FPR(predicted.proba.logistic.shap.test, trial.test[,1])
```


# Session information

```{r, child="_session-info.Rmd"}
```

# References

