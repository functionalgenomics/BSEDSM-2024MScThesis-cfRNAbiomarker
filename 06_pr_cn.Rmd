---
title: Biomarker prediction in cancer data
author:
- name: Berta Canal Simón
  affiliation:
  - &id Barcelona School of Economics
  email: berta.canal@bse.eu
- name: Adam Olivares Canal
  affiliation: *id
  email: adam.olivares@bse.eu
date: "`r format(Sys.time(), '%B %e, %Y')`"
abstract: >
  Here we perform a prediction of candidate biomarkers in cancer cfRNA sequencing data.
output:
  BiocStyle::html_document:
    toc: true
    toc_float: true
    fig_captions: yes
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: _bibliography.bib
---

```{r setup, echo=FALSE, cache=FALSE, message = FALSE}
library(knitr)
library(kableExtra)
library(glmnet)
library(lime)
library(dplyr)
library(ggplot2)
library(caret)
library(xgboost)
library(randomForest) 



knitr::opts_chunk$set(
  collapse=TRUE,
  comment="",
  fig.align="center",
  fig.wide=TRUE,
  cache=TRUE,
  cache.path="_cache/__PR_cn",
  cache.extra=R.version.string,
  autodep=TRUE
)
```

# Importing processed and filtered data

We start by importing the previously filtered, normalized RNA-seq data and the differential expressed genes from the training dataset.

```{r, message=FALSE}
library(SummarizedExperiment)
library(edgeR)

dgeR.filt <- readRDS(file.path("_processed_data", "dgeR.filt.rds"))
seR.filt <- readRDS(file.path("_processed_data", "seR.filt.rds"))
dgeB.filt <- readRDS(file.path("_processed_data", "dgeB.filt.rds"))
seB.filt <- readRDS(file.path("_processed_data", "seB.filt.rds"))

dgeR.filt.training <- readRDS(file.path("_processed_data",
                                        "dgeR.filt.training.rds"))
seR.filt.training <- readRDS(file.path("_processed_data",
                                       "seR.filt.training.rds"))
dgeB.filt.testing <- readRDS(file.path("_processed_data",
                                       "dgeB.filt.testing.rds"))
seB.filt.testing <- readRDS(file.path("_processed_data",
                                      "seB.filt.testing.rds"))
DEgenes.trainingR <- readRDS(file.path("_processed_data", 
                                      "DEgenes.trainingR.rds"))
```

Create a subset with the differential expressed genes from the training dataset from @Roskams2022.


Train-testing subset creation: Intersection between differential expressed genes from training set from @Roskams2022 and lowly expressed genes from testing set.

Train-testing subset creation: Intersection between differential expressed genes from training set from @Roskams2022 and lowly expressed genes from testing set.

```{r}
#set.seed(111)
#intersection.genes <- intersect(rownames(dgeB.filt.testing),DEgenes.trainingR)
#length(intersection.genes)

#dgeR.intersect <- dgeR.filt.training[intersection.genes,]
#dim(dgeR.intersect)
#seR.intersect <- seR.filt.training[intersection.genes,]
#dim(seR.intersect)

#dgeB.intersect <- dgeB.filt.testing[intersection.genes,]
#dim(dgeB.intersect)
#seB.intersect <- seB.filt.testing[intersection.genes,]
#dim(seB.intersect)
```


```{r}
set.seed(111)
mask.lowly.testing <- rownames(dgeB.filt.testing$counts)
intersection.genes <- intersect(mask.lowly.testing,DEgenes.trainingR)
dgeR.DEgenes <- dgeR.filt.training$counts[intersection.genes,]
dgeR.filt.training$counts <- dgeR.filt.training$counts[intersection.genes,]
dgeB.filt.testing$counts <- dgeB.filt.testing$counts[intersection.genes,]
```

Training data

```{r}
#training.df <- data.frame(cbind(as.numeric(dgeR.intersect$samples$Tumor)-1, scale(t(dgeR.intersect$counts), scale = TRUE, center = TRUE)))
#colnames(training.df)[1] <- "Tumor"
```

```{r}
training.df <- data.frame(cbind(as.numeric(dgeR.filt.training$samples$Tumor)-1, scale(t(dgeR.filt.training$counts), scale = TRUE, center = TRUE)))
colnames(training.df)[1] <- "Tumor"
```

```{r}
training.df$Tumor <- factor(training.df$Tumor)
levels(training.df$Tumor) <- c("No", "Yes")
```

Testing data

```{r}
#testing.df <- data.frame(cbind(as.numeric(dgeB.intersect$samples$Tumor)-1, scale(t(dgeB.intersect$counts), scale = TRUE, center = TRUE)))
#colnames(testing.df)[1] <- "Tumor"
```

```{r}
testing.df <- data.frame(cbind(as.numeric(dgeB.filt.testing$samples$Tumor)-1, scale(t(dgeB.filt.testing$counts), scale = TRUE, center = TRUE)))
colnames(testing.df)[1] <- "Tumor"
```

```{r}
testing.df$Tumor <- factor(testing.df$Tumor)
levels(testing.df$Tumor) <- c("No", "Yes")
```

# Performance metrics 

Given the application of the current paper, the False Negative Rate (FNR) metric is a particularly relevant metric, since it would imply classifying as healthy an individual with cancer. Therefore, that patients will not receive treatment, which will cause serious consequences. Furthermore, it could also be considered the False Positive Rate (FPR), which results in an undesirable situation where a proportion of healthy individuals are categorized as ill. This would subject a healthy patient to unnecessary treatment and its potential side effects. However, since the expected consequences are not that severe, FNR is prioritized in the analysis.

```{r}
FNR <- function(proba.pred, truth){
  class.pred <- as.numeric(proba.pred > 0.5)
  conf <- table(truth, class.pred)
  print(conf)
  FNR <- conf[2, 1] / sum(conf[2, 1], conf[2, 2])
  return(FNR)
}
```

```{r}
FPR <- function(proba.pred, truth){
  class.pred <- as.numeric(proba.pred > 0.5)
  conf <- table(truth, class.pred)
  print(conf)
  FPR <- conf[1, 2] / sum(conf[1, 1], conf[1, 2])
  return(FPR)
}
```


# LASSO-CV (Via cross-validation)

## Fitting the model

Define the function for LASSO where the penalization parameter λ is set through cross-validation (LASSO-CV).

```{r}
kfoldCV.lasso.logistic <- function(y,x,K=10,seed,criterion='cv') {
## Perform K-fold cross-validation for LASSO regression estimate 
  #(lambda set either via cross-val or BIC or EBIC)
## Input
## - y: response
## - x: data.frame with predictors, intercept should not be present
## - K: number of folds in K-fold cross-validation
## - seed: random number generator seed (optional)
## - criterion: the criterion to select the penalization parameter, 
  #either cross-val or BIC or EBIC
## Output
## - pred: cross-validated predictions for y
## - ssr: residual sum of squares, sum((y-pred)^2)
  require(glmnet)
  if (!missing(seed)) set.seed(seed)
  subset <- rep(1:K,ceiling(nrow(x)/K))[1:nrow(x)]
  subset <- sample(subset,size=nrow(x),replace=FALSE)
  pred <- double(nrow(x))
  pred_0 <- double(nrow(x))
  cat("Starting cross-validation")
  if (ncol(x)>0) {  #if there are some covariates
    for (k in 1:K) {
        sel <- subset==k
        pred_0[sel] <- mean(y[!sel])
        if (criterion=='cv') {
            fit <- cv.glmnet(x=x[!sel,,drop=FALSE], y=y[!sel], 
                             alpha = 1, nfolds=10, family = 'binomial')
            pred[sel] <- predict(fit,newx=x[sel,,drop=FALSE],type='response',
                                 s='lambda.min')
        } else if (criterion=='bic'){
            fit <- lasso.bic.logistic(y=y[!sel],x=x[!sel,,drop=FALSE])
            pred[sel] <- predict(fit$model,newx=x[sel,,drop=FALSE],
                                 type='response', s = fit$lambda.opt)
        } else if (criterion=='ebic'){
            fit <- lasso.bic.logistic(y=y[!sel],x=x[!sel,,drop=FALSE],extended = TRUE)
            pred[sel] <- predict(fit$model,newx=x[sel,,drop=FALSE],
                                 type='response', s = fit$lambda.opt)
        } else { stop("method.lambda not implemented") }
        cat(".")
    }
  } else { #if there are no covariates, just use the intercept
    for (k in 1:K) {
      sel <- subset==k
      pred[sel] <- mean(y[!sel],na.rm=TRUE)
    }
  }
  cat("\n")
  return(list(pred=pred, pred_0= pred_0,ssr=sum((pred-y)^2,na.rm=TRUE)))
}
```

Convert the dependent variable from the training dataset into a factor variable (with factors 0 and 1) and fit the LASSO-CV model using the previous function.

```{r}
set.seed(1234)
t0 <- Sys.time()

tumor.training <- as.numeric(dgeR.filt.training$samples$Tumor)-1

fit.lassocv <- cv.glmnet(x=t(dgeR.DEgenes),y=tumor.training, family ='binomial', nfolds = 10, aplha = 1)
t1 <- Sys.time()
cat('\nTime elapsed: ')
```

```{r}
print(round(t1-t0,3))
```

```{r}
fit.lassocv
plot(fit.lassocv)
```

The model selects 26 genes. 

```{r}
b.lassocv = as.vector(coef(fit.lassocv, s='lambda.min'))
sum(b.lassocv[-1] != 0)
```

```{r}
coef.matrix <- coef(fit.lassocv, s = 'lambda.min')
non.zero.coefs <- rownames(coef.matrix)[coef.matrix[,1]!= 0 & rownames(coef.matrix)!= '(Intercept)']
```

```{r}
mask <- rownames(assay(seR.filt.training)) %in% non.zero.coefs
selected.genes <- rowData(seR.filt.training)$Symbol[mask]
```

## Model performance

### In-sample AUC

```{r}
lassocv.pred<- predict(fit.lassocv, newx=t(dgeR.DEgenes), s = fit.lassocv$lambda.min)

(auc.lassoCV.training<- pROC::auc(tumor.training,lassocv.pred[,1]))
```

### Out-sample AUC using cross-validation

```{r}
cv.pred.lasso.cv <- kfoldCV.lasso.logistic(y=tumor.training,x=t(dgeR.DEgenes), K=10,seed=1,criterion="cv")

(auc.lassocv.test <- pROC::auc(tumor.training,cv.pred.lasso.cv$pred))
```

```{r}
FNR(cv.pred.lasso.cv$pred,tumor.training)
```
```{r}
FPR(cv.pred.lasso.cv$pred,tumor.training)
```
```{r}
#mcfadden_pseudo_r2(cv.pred.lasso.cv$pred,cv.pred.lasso.cv$pred_0, tumor.training)
```

### Out-sample AUC using testing dataset from Block et al. (2022)

Convert the dependent variable from the testing dataset into a factor variable (with factors 0 and 1).

```{r}
tumor.testing <- as.numeric(dgeB.filt.testing$samples$Tumor)-1
```

Predict the categories using the previously trained model and compute the out-sample AUC, FNR and FPR.

```{r}
lassocv.pred2<- predict(fit.lassocv, newx=t(dgeB.filt.testing$counts),
                       s = fit.lassocv$lambda.min)
(auc.lassocv.test2 <- pROC::auc(tumor.testing,lassocv.pred2[,1]))
```

```{r}
FNR(lassocv.pred2,tumor.testing)
```

```{r}
FPR(lassocv.pred2,tumor.testing)
```

# Random Forest

```{r, echo = FALSE}
library(caTools) 
library(randomForest) 
```

## Fitting the model

Standardize the training set.

```{r}
standardized.training <- scale(t(dgeR.DEgenes), scale = TRUE, center = TRUE)
```

```{r}
library(randomForest)

tumor.training.forest <- as.numeric(dgeR.filt.training$samples$Tumor)

# Fitting a random forest model
model <- randomForest(x = standardized.training, y = as.factor(tumor.training.forest), ntree = 400, importance = TRUE)

# Summary of the model
print(model)
```

Calculates how much the model accuracy decreases without a certain predictor (feature)

```{r, results = 'hide'}
png("plot.png", width = 1200, height = 800)
varImpPlot <- varImpPlot(model, pch = 16, col = "blue", type = TRUE, main = NULL)
dev.off()
```

<figure style="display: flex; justify-content: center; align-items: center; flex-direction: column; height: 100%; margin-bottom: 20px;">
    <img src="/Users/bertacanal/Desktop/cfRNAanalysis/plot.png" style="width: 700px; height: auto;">
    <figcaption style="text-align: center; margin-top: 20px;">Figure XX: Variance important plot for @Roskams2022.</figcaption>
</figure>


## Model performance

### Out-sample AUC using cross-validation

Create the function.

```{r}
kfoldCV.randomforest.logistic <- function(y,x,K=10, ntree= 500, mtry =1, seed) {
## Perform K-fold cross-validation for least-squares regression estimate
## Input
## - y: response
## - x: data.frame with predictors, intercept should not be present
## - K: number of folds in K-fold cross-validation
## - seed: random number generator seed (optional)
## Output
## - pred: cross-validated predictions for y
## - ssr: residual sum of squares, sum((y-pred)^2)
  if (!missing(seed)) set.seed(seed)
  subset <- rep(1:K,ceiling(nrow(x)/K))[1:nrow(x)]
  subset <- sample(subset,size=nrow(x),replace=FALSE)
  pred <- double(nrow(x))
  pred_0 <- double(nrow(x))
  if (ncol(x)>0) {
    for (k in 1:K) {
      sel <- subset==k
      #fit <- glm(y[!sel] ~ ., data=x[!sel,,drop=FALSE], family = 'binomial')
      fit <- randomForest(x = x[!sel,,drop=FALSE],
                          y = as.factor(y[!sel]), ntree, mtry)
      pred[sel] <- predict(fit, newdata=x[sel,,drop=FALSE],
                           type='response')
      pred_0[sel] <- mean(y[!sel])
    }
  } else {
    for (k in 1:K) {
      sel <- subset==k
      pred[sel] <- mean(y[!sel],na.rm=TRUE)
    }
  }
  return(list(pred=pred, pred_0 =pred_0,
              ssr=sum((pred-y)^2,na.rm=TRUE)))
}
```

Compute AUC, FNR and FPR.

```{r}
cv.pred.RandomForest <- kfoldCV.randomforest.logistic(y=as.factor(tumor.training.forest),
                                x=standardized.training, K=10,seed=1, mtry = 3)

(auc.randomForest.cv <- pROC::auc(tumor.training,cv.pred.RandomForest$pred))
```

```{r}
FNR(cv.pred.RandomForest$pred -1,tumor.training)
```

```{r}
FPR(cv.pred.RandomForest$pred -1,tumor.training)
```

### Out-sample AUC using using testing dataset from Block et al. (2022)

Standardize the subset that includes those counts of the differential expressed genes from the training and 0 otherwise.

```{r}
standardized.testing <- scale(t(dgeB.filt.testing$counts), scale = TRUE, center = TRUE)
```

```{r}
# Extracting probabilities for class 1
predicted_probs2 <- predict(model, standardized.testing, type = "prob")[, 2] 

# Compute AUC
roc_obj <- pROC::roc(tumor.testing, predicted_probs2)
auc <- pROC::auc(roc_obj)
print(paste("AUC:", auc))

# Compute False Negative Rate (FNR)
threshold <- 0.5
predictions2 <- ifelse(predicted_probs2 > threshold, 1, 0)
conf_matrix <- table(Actual = tumor.testing, Predicted = predictions2)
fnr <- conf_matrix[2, 1] / sum(conf_matrix[2, ])
print(paste("FNR:", fnr))
```

```{r}
# Get feature importance
#model <- randomForest(x = x_train, y = as.factor(y_train), ntree = 500)
#feature_importance <- randomForest::importance(model)
#sorted_importance <- feature_importance[order(-feature_importance[, #"MeanDecreaseGini"]),                                        ]
#print(sorted_importance)
```

# XGBOOST

```{r}
modelLookup("xgbTree")
```


```{r}
# CV technique which will be passed into the train() function
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

# tuning grid
set.seed(111)
xgboostGrid <-  expand.grid(max_depth = c(3, 5, 7, 9), 
                        nrounds = (1:10)*20,    # number of trees
                        eta = 0.01,
                        gamma = c(0.5),
                        subsample = 1,
                        min_child_weight = c(1),
                        colsample_bytree = 0.8
                        )

# hyperparaemeter search for XGboost classifier tree model
model = train(Tumor~., data = training.df,
              ######ALTERNATIVE#######
              #x = trainMNX,
              #y = trainMNY,
              ########################
              method = "xgbTree",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = xgboostGrid,
              verbosity = 0,
              verbose = FALSE,
              #nthreads = 4 #cores in use
              )

print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- as.numeric(predict(model, testing.df)) -1

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

## Feature importance

```{r}
importance_matrix = xgb.importance(colnames(training.df[-1]), model = model)
xgb.plot.importance(importance_matrix[1:5,])
```


LIME

```{r}
explainer_caret <- lime(training.df, model, n_bins = 5)
```

```{r}
explanation_caret <- explain(
  x = training.df, 
  explainer = explainer_caret, 
  n_permutations = 5000,
  dist_fun = "gower",
  kernel_width = .75,
  n_features = 10, 
  feature_select = "highest_weights",
  labels = "Yes"
  )

```

```{r}
plot_features(explanation_caret)
```

```{r}
plot_explanations(explanation_caret)
```

# KNN

```{r}
modelLookup("knn")
```


```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(50)
# Customzing the tuning grid
knngrid <-  expand.grid(k = seq(1,12,by = 1)
                        )

# training a knn classifier model while tuning parameters
model = train(Tumor~., data = training.df,
              method = "knn",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = knngrid)

# summarising the results
print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- as.numeric(predict(model, testing.df)) -1

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

# SVM models

## svmLinearWeights (linear kernel + class weights)

```{r}
modelLookup("svmLinearWeights")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(50)
# Customzing the tuning grid
svmgrid <-  expand.grid(cost = c(0.01, 0.05,0.5,1,3, 5, 10),
                        weight = c(0.01, 0.05,0.5,1,3,5,10)
                        )

# training a svm classifier with liearn kernel model while tuning parameters
model = train(Tumor~., data = training.df,
              method = "svmLinearWeights",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

# summarizing the results
print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- as.numeric(predict(model, testing.df)) -1

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

## svmRadial (Support Vector Machines with Radial Basis Function Kernel)

```{r}
modelLookup("svmRadial")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(50)
# Customzing the tuning grid
svmgrid <-  expand.grid(sigma = c(0.01, 0.05,0.5,1,3, 5, 10),
                        C = c(0.01, 0.05,0.5,1,3,5,10)
                        )

# training a svm with rbf kernel classifier model while tuning parameters
model = train(Tumor~., data = training.df,
              method = "svmRadial",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

# summarizing the results
print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- as.numeric(predict(model, testing.df)) -1

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

## svmPoly (Support Vector Machines with Polynomial Kernel)

```{r}
modelLookup("svmPoly")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
svmgrid <-  expand.grid(degree = c(3, 5, 10),
                        scale = c(1, 3, 5),
                        C = c(0.5,1,3,5,10)
                        )

# training a svm with poly kernel classifier tree model while tuning parameters
model = train(Tumor~., data = training.df,
              method = "svmPoly",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- as.numeric(predict(model, testing.df)) -1

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

## dwdPoly (EXPERIMENTAL STUFF)

```{r}
modelLookup("dwdPoly")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
knngrid <-  expand.grid(lambda = c(0.5,1,2),
                        qval = c(0.5,1,2,3),
                        degree = c(1,2,3),
                        scale = c(0.1,0.5,1,2,3)
                        )

# training a dwdPoly classifier tree model while tuning parameters
model = train(Tumor~., data = training.df,
              method = "dwdPoly",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = knngrid)

print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- as.numeric(predict(model, testing.df)) -1

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

# Random forest

```{r}
modelLookup("rf")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
rfgrid <-  expand.grid(mtry = c(1:5) #only parameter you can tune for rf in R
                        )

# training a randomForest classifier tree model while tuning parameters
model = train(Tumor~., data = training.df,
              method = "rf",
              trControl = train_control,
              metric = "ROC",
              importance = T,
              #manually set
              ntree = 200, 
              nodesize = 1, #default
              tuneGrid = rfgrid)

print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- as.numeric(predict(model, testing.df)) -1

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

```{r}
#feature_importance <- randomForest::importance(model)
#sorted_importance <- feature_importance[order(-feature_importance[, #"MeanDecreaseGini"]),                                        ]
#print(sorted_importance)
```


#  Elastic net

```{r}
modelLookup("glmnet")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
netgrid <-  expand.grid(alpha = c(0, 0.005, 0.01, 0.015, 0.1, 0.2, 0.5, 0.7, 1),
                        lambda = c(0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5,1)
                        )

# training a elastic net classifier tree model while tuning parameters
model = train(Tumor~., data = training.df,
              method = "glmnet",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = netgrid)

# summarizing the results
print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- as.numeric(predict(model, testing.df)) -1

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

# Keras NN

```{r}
library(tensorflow)
library(keras)
library(tfruns)
```

```{r}
#c(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()
#x_train <- x_train / 255
#x_test <-  x_test / 255
```

```{r}
x_train <- as.matrix(training.df[,-1])
y_train <- as.matrix(as.numeric(training.df[,1]))

x_test<- as.matrix(testing.df[,-1])
y_test <- as.matrix(as.numeric(testing.df[,1]))
```


```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 200, activation = 'relu') %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 100, activation = 'tanh') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 50, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 10, activation = 'softmax')
```

2024-05-10 19:52:32.709468: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory


```{r}
loss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)

model %>% compile(
  optimizer = "adam",
  loss = loss_fn,
  metrics = "accuracy"
)
```

```{r}
model %>% fit(x_train, y_train, epochs = 10, batch_size = NULL)
```

```{r}
#history <- model %>% fit(
#  x_train,y_train,
#  epochs = 30, batch_size = 256, 
#  validation_split = 0.2
#)

#plot(history)
```


```{r}
model %>% evaluate(x_test,  y_test, verbose = 2)
```

```{r}
model %>% predict(x_test) %>% k_argmax() #only for softmax
# model %>% predict(x) %>% `>`(0.5) %>% k_cast("int32") #for sigmoid.0
```



HYPERPARAMETER TUNING FEED-FORWARD NETWORK (INCOMPLETE)

Here’s a declaration of 2 flags that control dropout rate within a model:

```{r}
FLAGS <- flags(
  flag_numeric("dropout1", 0.4),
  flag_numeric("dropout2", 0.3)
)
```

These flags are then used in the definition of the model here:
```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%
  layer_dropout(rate = FLAGS$dropout1) %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = FLAGS$dropout2) %>%
  layer_dense(units = 10, activation = 'sigmoid')

```

Once we’ve defined flags, we can pass alternate flag values to training_run() as follows:

```{r}
#training_run('mnist_mlp.R', flags = list(dropout1 = 0.2, dropout2 = 0.2))
```

TUNING RUNS

```{r}
# run various combinations of dropout1 and dropout2
runs <- tuning_run("mnist_mlp.R", flags = list(
  dropout1 = c(0.2, 0.3, 0.4),
  dropout2 = c(0.2, 0.3, 0.4)
))
# find the best evaluation accuracy
runs[order(runs$eval_acc, decreasing = TRUE), ]
```


# Session information

```{r, child="_session-info.Rmd"}
```

# References

