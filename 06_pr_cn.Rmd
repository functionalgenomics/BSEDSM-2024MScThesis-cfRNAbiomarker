---
title: Biomarker prediction in cancer data
author:
- name: Berta Canal SimÃ³n
  affiliation:
  - &id Barcelona School of Economics
  email: berta.canal@bse.eu
- name: Adam Olivares Canal
  affiliation: *id
  email: adam.olivares@bse.eu
date: "`r format(Sys.time(), '%B %e, %Y')`"
abstract: >
  Here we perform a prediction of candidate biomarkers in cancer cfRNA sequencing data.
output:
  BiocStyle::html_document:
    toc: true
    toc_float: true
    fig_captions: yes
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: _bibliography.bib
---

```{r setup, echo=FALSE, cache=FALSE, message = FALSE}
library(knitr)
library(kableExtra)
library(glmnet)
library(lime)
library(dplyr)
library(ggplot2)
library(caret)
library(xgboost)
library(randomForest) 
set.seed(111)

knitr::opts_chunk$set(
  collapse=TRUE,
  comment="",
  fig.align="center",
  fig.wide=TRUE,
  cache=TRUE,
  cache.path="_cache/__PR_cn",
  cache.extra=R.version.string,
  autodep=TRUE
)
```

# Importing processed and filtered data

We start by importing the previously filtered, normalized RNA-seq data and the differential expressed genes from the training dataset.

```{r, message=FALSE}
library(SummarizedExperiment)
library(edgeR)

dgeR.filt <- readRDS(file.path("_processed_data", "dgeR.filt.rds"))
seR.filt <- readRDS(file.path("_processed_data", "seR.filt.rds"))

dgeB.filt <- readRDS(file.path("_processed_data", "dgeB.filt.rds"))
seB.filt <- readRDS(file.path("_processed_data", "seB.filt.rds"))

dgeR.filt.training <- readRDS(file.path("_processed_data",
                                        "dgeR.filt.training.rds"))
seR.filt.training <- readRDS(file.path("_processed_data",
                                       "seR.filt.training.rds"))
dgeB.filt.testing <- readRDS(file.path("_processed_data",
                                       "dgeB.filt.testing.rds"))
seB.filt.testing <- readRDS(file.path("_processed_data",
                                      "seB.filt.testing.rds"))
DEgenes.trainingR <- readRDS(file.path("_processed_data", 
                                      "DEgenes.trainingR.rds"))
```

Create a subset with the differential expressed genes from the training dataset from @Roskams2022.

Train-testing subset creation: Intersection between differential expressed genes from training set from @Roskams2022 and lowly expressed genes from testing set.

```{r}
set.seed(111)
intersection.genes <- intersect(rownames(dgeB.filt.testing),DEgenes.trainingR)
length(intersection.genes)

dgeR.intercept <- dgeR.filt.training[intersection.genes,]
dim(dgeR.intercept)
seR.intercept <- seR.filt.training[intersection.genes,]
dim(seR.intercept)

dgeB.intercept <- dgeB.filt.testing[intersection.genes,]
dim(dgeB.intercept)
seB.intercept <- seB.filt.testing[intersection.genes,]
dim(seB.intercept)
```
## Dataframes creation

### Training data

```{r}
training.df <- data.frame(Tumor = seR.intercept$Tumor,
                          scale(t(assays(seR.intercept)$logCPM), scale = TRUE, center = TRUE))
len <- length(training.df)
#colnames(training.df)[2:len] <- rowData(seR.intercept)[["Symbol"]]
```

### Testing data

```{r}
testing.df <- data.frame(Tumor = seB.intercept$Tumor,
                          scale(t(assays(seB.intercept)$logCPM), scale = TRUE, center = TRUE))
len <- length(testing.df)
#colnames(testing.df)[2:len]  <- rowData(seB.intercept)[["Symbol"]]
```

# Performance metrics 

Given the application of the current paper, the False Negative Rate (FNR) metric is a particularly relevant metric, since it would imply classifying as healthy an individual with cancer. Therefore, that patients will not receive treatment, which will cause serious consequences. Furthermore, it could also be considered the False Positive Rate (FPR), which results in an undesirable situation where a proportion of healthy individuals are categorized as ill. This would subject a healthy patient to unnecessary treatment and its potential side effects. However, since the expected consequences are not that severe, FNR is prioritized in the analysis.

```{r}
FNR <- function(proba.pred, truth){
  class.pred <- as.numeric(proba.pred > 0.35)
  conf <- table(truth, class.pred)
  print(conf)
  FNR <- conf[2, 1] / sum(conf[2, 1], conf[2, 2])
  return(FNR)
}
```

```{r}
FPR <- function(proba.pred, truth){
  class.pred <- as.numeric(proba.pred > 0.35)
  conf <- table(truth, class.pred)
  print(conf)
  FPR <- conf[1, 2] / sum(conf[1, 1], conf[1, 2])
  return(FPR)
}
```

```{r, include = FALSE}
# Ouput matrix
res.testing <- data.frame(
  AUC = NA,
  FNR = NA,
  FPR = NA)
```

# XGBOOST

```{r}
modelLookup("xgbTree")
```


```{r}
# CV technique which will be passed into the train() function
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

# tuning grid
set.seed(111)
#xgboostGrid <-  expand.grid(max_depth = c(3, 5, 7, 9), 
#                        nrounds = (1:10)*20,    # number of trees
#                        eta = c(0.2,0.3,0.4),
#                        gamma = c(0.5,1),
#                        subsample = c(0.5, 0.6, 0.7), # common value: between 0.5 and 1
#                        min_child_weight = c(1,2,3),
#                        colsample_bytree = c(0.5, 0.6, 0.7) # common value: between 0.5 and 1
#                        )

xgboostGrid <- expand.grid(max_depth = c(4, 5, 6, 7), nrounds = (1:10)*20,eta = c(0.2,0.4),gamma = c(0.6),subsample = c(1),min_child_weight = c(1),colsample_bytree = c(0.7))

# auc = 0.64 fnr= 0.47 fpr = 0. max_depth = c(3, 5, 7, 9), nrounds = (1:10)*20,eta = c(0.2,0.3,0.4),gamma = c(0.5),subsample = c(1),min_child_weight = c(1),colsample_bytree = c(0.8)
# auc = 0.71 fnr= 0.47 fpr = 0.16. max_depth = c(3, 4, 5, 6, 7)
# auc = 0.75 fnr= 0.47 fpr = 0.16. colsample_bytree = c(0.7)
# auc = 0.58, fnr = 0.52, fpr=0. min_child_weight = c(2)
# auc = 0.68, fnr = 0.47, fpr=0. min_child_weight = c(3)
# auc = 0.56, fnr=0.57, fpr=0.eta = c(0.2,0.3,0.4, 0.5),gamma = c(0.5,1) colsample_bytree = c(0.7,0.8)
# auc = 0.73, fnr=0.47, fpr=0.subsample = c(0.8)
# auc = 0.57, fnr=0.47, fpr=0.33.subsample = c(0.8)
#auc = 0.52, fnr=0.57, fpr=0. max_depth = c(4, 5, 6, 7), nrounds = (1:10)*20,eta = c(0.2,0.3,0.4),gamma = c(0.5),subsample = c(0.8),min_child_weight = c(1),colsample_bytree = c(0.7))
# auc = 0.74, fnr = 0.52, fpr=0.eta = c(0.2,0.4)


# hyperparaemeter search for XGboost classifier tree model
model = caret::train(Tumor~., data = training.df,
              ######ALTERNATIVE#######
              #x = trainMNX,
              #y = trainMNY,
              ########################
              method = "xgbTree",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = xgboostGrid,
              verbosity = 0,
              verbose = TRUE,
              #num.threads = 16,
              #nthreads = 16 #cores in use
              )

print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

```{r, results = "hide"}
# Add to output
res.testing[1, ] <- c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y))
rownames(res.testing)[nrow(res.testing)] <- 'XGBOOST'
```

# ADABOOST

Not used in the final version due to poor performance and slow training.

```{r}
modelLookup("ada")
```

```{r, eval=FALSE}
# CV technique which will be passed into the train() function
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

# tuning grid
set.seed(111)
adaGrid <-  expand.grid(iter = c(50), 
                        maxdepth = c(1,2),
                        nu = c(0,0.01,0.05)
                        )

#adaGrid <-  expand.grid(iter = c(50, 100, 200, 500),
#                       maxdepth = c(1, 2, 3, 4, 5),
#                       nu = c(0, 0.01, 0.05, 0.1, 0.2))


# auc = 0.67, fnr=0.52, fpr=0. iter = c(50), maxdepth = c(1,2),nu = c(0,0.01,0.05))

# hyperparaemeter search for adaboost classifier tree model
model = caret::train(Tumor~., data = training.df,
              method = "ada",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = adaGrid,
              loss = "exponential",
              type = "discrete"
              )

print(model)
```

```{r, eval = FALSE}
plot(model)
```

```{r,  eval = FALSE}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```
```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'ADABOOST'
```


# SVM models

## svmLinearWeights (linear kernel + class weights)

```{r}
modelLookup("svmLinearWeights")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(50)
# Customzing the tuning grid
svmgrid <-  expand.grid(cost = c(0.01,0.015, 0.02),
                        weight = c(0.45) 
                        )

#svmgrid <-  expand.grid(cost = c(0.01, 0.05,0.5,1,3, 5, 10), weight = c(0.01, 0.05,0.5,1,3,5,10))

# auc = 0.70, fnr = 0.2, fpr = 0.33. cost = c(0.01, 0.05,0.5,1,3, 5, 10), weight = c(0.01, 0.05,0.5,1,3,5,10)
# auc = 0.72, fnr = 0.31, fpr = 0.33. (cost = c(0.01,0.015, 0.02),weight = c(0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5)
#empitjora. cost = c(0.015, 0.02)
#auc = 0.73, fnr=0.31, fpr=0.33. cost = c(0.01,0.015, 0.02), weight = c(0.4, 0.5) 
#auc = 0.74, fnr=0.31, fpr=0.33. weight = c(0.45)



# training a svm classifier with liearn kernel model while tuning parameters
model = caret::train(Tumor~., data = training.df,
              method = "svmLinearWeights",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

# summarizing the results
print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```
```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'SVMLinear'
```



## svmRadial (Support Vector Machines with Radial Basis Function Kernel)

```{r}
modelLookup("svmRadial")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(50)
svmgrid <-  expand.grid(sigma=c(0.01, 0.05,0.5,1,3, 5, 10), C = c(0.01, 0.05,0.5,1,3,5,10))
# Customzing the tuning grid
#svmgrid <-  expand.grid(sigma = c(0.01, 0.05,0.5,1,3, 5, 10),
#                        C = c(1, 1.1, 1.2, 1.3, 1.4,1.6,1.8)
#                        )

# auc = 0.67. sigma = 0.01 and C = 3. sigma = c(0.01, 0.05,0.5,1,3, 5, 10), C = c(0.01, 0.05,0.5,1,3,5,10)
# auc = 0.67. sigma = 0.01 and C = 2. sigma = c(0.01, 0.05,0.5,1,3, 5, 10), C = c(0.01, 0.05,0.5,1,2,3,4,5,10)
# auc = 0.67. sigma = 0.01 and C = 1.1. sigma = c(0.01, 0.05,0.5,1,3, 5, 10),C = c(1, 1.1, 1.2, 1.3, 1.4,1.6,1.8)


# training a svm with rbf kernel classifier model while tuning parameters
model = caret::train(Tumor~., data = training.df,
              method = "svmRadial",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

# summarizing the results
print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'SVMRadial'
```


## svmPoly (Support Vector Machines with Polynomial Kernel)

```{r}
modelLookup("svmPoly")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
svmgrid <-  expand.grid(degree = c(2,3,4,5),
                        scale = c(0.001,0.01,0.5,1),
                        C = c(0.1,0.5,1,5,10, 100)
                        )

# training a svm with poly kernel classifier tree model while tuning parameters
model = caret::train(Tumor~., data = training.df,
              method = "svmPoly",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

print(model)
```

```{r}
plot(model)
```


```{r}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'SVMPoly'
```



# Random forest

```{r}
modelLookup("rf")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
rfgrid <-  expand.grid(mtry = c(1:30) #only parameter you can tune for rf in R
                        )

# training a randomForest classifier tree model while tuning parameters
model = caret::train(Tumor~., data = training.df,
              method = "rf",
              trControl = train_control,
              metric = "ROC",
              importance = T,
              #manually set
              ntree = 700, #was a good number
              nodesize = 1, #default for classification
              tuneGrid = rfgrid)

print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'RandomForest'
```

```{r}
#feature_importance <- randomForest::importance(model)
#sorted_importance <- feature_importance[order(-feature_importance[, #"MeanDecreaseGini"]),                                        ]
#print(sorted_importance)
```


#  Elastic net

```{r}
modelLookup("glmnet")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
netgrid <-  expand.grid(alpha = c(0.1, 0.2, 0.5, 0.7, 0.9), # every time model selected 0 auc was 1
                        lambda = c(0,0.05,0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 1, 5, 10) # 0 is logistic
                        )

# auc = 0.59. alpha = c(0, 0.005, 0.01, 0.015, 0.1, 0.2, 0.5, 0.7, 1),lambda = c(0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5,1). alpha = 0.005 and lambda = 1.
# auc = 0.70. lambda = c(0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5). alpha = 0.005 and lambda = 0.5.
# auc = 0.68. alpha = c(0.01, 0.015, 0.1, 0.2, 0.5, 0.7, 1) alpha = 0.01 and lambda = 0.5.
# auc = 0.70. alpha = c(0, 0.005, 0.01, 0.015, 0.1, 0.2, 0.5), lambda = c(0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4). alpha = 0.005 and lambda = 0.4.

# training a elastic net classifier tree model while tuning parameters
model = caret::train(Tumor~., data = training.df,
              method = "glmnet",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = netgrid)

# summarizing the results
print(model)
```

```{r}
plot(model)
```

```{r}
#predict on test data
pred.y <- predict(model, testing.df, type = "prob")[,2]

# out of sample performance metrics
test.y <- as.numeric(testing.df[, 1]) -1

pROC::auc(test.y, pred.y)

FNR(pred.y, test.y)
FPR(pred.y, test.y)
```

```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(test.y, pred.y), FNR(pred.y, test.y), FPR(pred.y, test.y)))
rownames(res.testing)[nrow(res.testing)] <- 'ElasticNet'
```

# Keras NN

```{r}
library(tensorflow)
library(keras)
library(tfruns)
tensorflow::set_random_seed(111)
```
Use gene names now. ADAboost was having a bug with with these names so this step was moved to this part.

Training data
```{r}
training.df <- data.frame(Tumor = seR.intercept$Tumor,
                          scale(t(assays(seR.intercept)$logCPM), scale = TRUE, center = TRUE))
len <- length(training.df)
colnames(training.df)[2:len] <- rowData(seR.intercept)[["Symbol"]]
```

Testing data

```{r}
testing.df <- data.frame(Tumor = seB.intercept$Tumor,
                          scale(t(assays(seB.intercept)$logCPM), scale = TRUE, center = TRUE))
len <- length(testing.df)
colnames(testing.df)[2:len]  <- rowData(seB.intercept)[["Symbol"]]
```

```{r}
x_train <- as.matrix(training.df[,-1])
y_train <- as.matrix(as.numeric(training.df[,1])-1)

x_test<- as.matrix(testing.df[,-1])
y_test <- as.matrix(as.numeric(testing.df[,1])-1)

x_train_shape <- length(colnames(x_train))
```


```{r}
#constraint_maxnorm(max_value = 2, axis = 0)
# bias_regularizer = regularizer_l2(0.01)
model <- keras_model_sequential()
model %>%
  layer_dense(units = 1500, activation = 'relu',
              input_shape = c(x_train_shape),
              kernel_regularizer = regularizer_l1_l2(l1 = 0.00000001, l2 = 0.00001),
              bias_regularizer = regularizer_l1_l2(l1 = 0.00001, l2 = 0.0001),
              kernel_constraint =constraint_maxnorm(max_value = 2, axis = 0),
              #bias_constraint =constraint_maxnorm(max_value = 3, axis = 0),
              activity_regularizer= regularizer_l1_l2(l1 = 0.01, l2 = 0.00001),
              ) %>%  
  layer_dropout(rate = 0.5) %>% 
  layer_batch_normalization() %>%
  layer_dense(units = 500, activation = 'relu',
              kernel_regularizer = regularizer_l1_l2(l1 = 0.1, l2 = 0.1),
              kernel_constraint = constraint_minmaxnorm(max_value = 2, min_value = 0, axis = 1),
              bias_regularizer = regularizer_l1_l2(l1 = 0.00001, l2 = 0.000001),
              #bias_constraint =constraint_maxnorm(max_value = 3, axis = 0),
              activity_regularizer = regularizer_l1_l2(l1 = 0.1, l2 = 0.000001),
              ) %>%
  layer_dropout(rate = 0.3) %>%
  layer_batch_normalization() %>%
  layer_dense(units = 1, activation = 'sigmoid')
```


```{r}
loss_fn <- loss_binary_crossentropy()
auc <- metric_auc()
adam <- optimizer_adam(learning_rate = 0.0001, ema_momentum = 0.99)

model %>% compile(
  optimizer = adam,
  loss = loss_fn,
  metrics = "AUC"
)
```

```{r}
model %>% fit(x_train, y_train, epochs = 150, batch_size =5)
```

```{r}
#history <- model %>% fit(
#  x_train,y_train,
#  epochs = 30, batch_size = 256, 
#  validation_split = 0.2
#)

#plot(history)
```


```{r}
model %>% evaluate(x_test,  y_test, verbose = 2)
```

```{r}
#model %>% predict(x_test) %>% k_argmax() #only for softmax
b<-model %>% predict(x_test) #%>% `>`(0.5) %>% k_cast("int32") #for sigmoid.0
b <- as.numeric(b)
b
```

```{r}
pROC::auc(as.numeric(y_test), b)

FNR(b, y_test)
FPR(b, y_test)
```

```{r, results = "hide"}
# Add to output
res.testing <- rbind.data.frame(res.testing, c(pROC::auc(y_test, b), FNR(b, y_test), FPR(b, y_test)))
rownames(res.testing)[nrow(res.testing)] <- 'Nnet'
```

plot roc-auc curve
```{r}
curve_values <- pROC::roc(as.numeric(y_test), b)
plot(curve_values ,main ="ROC curve -- Feedforward Nnet")
```


```{r, eval=FALSE, echo=FALSE}
######################
# LIME
######################
model_type.keras.engine.sequential.Sequential <- function(x, ...) {
  "classification"
}

# Setup lime::predict_model()
predict_model.keras.engine.sequential.Sequential <- function (x, newdata, type, ...) {
  
  ## here you have to write function, that takes a data.frame
  ## and transform it to shape that keras understands
  
  ## for example if u flatten your array before training CNN, you just use
  ## as-matrix()
  
  ## if keras model expect 3d dataset, you write something like
  ## as.array(newdata, dims(n,12,12))
  
  your_function <- function(data){
    as.matrix(newdata)
  }
  
  pred <- predict(object = x, x = your_function(newdata))
  data.frame (pred) }


x <- as.data.frame(training.df[,-1])  
x2 <- as.data.frame(testing.df[,-1])  

explainer <- lime(x = x, model= model)



explanation <- lime::explain (
  x=  x2[1:10,], 
  explainer, n_features = 5,
  n_labels     = 1,#explaining a `single class`
  kernel_width = 0.5) # allows us to increase model_r2 value by shrinking the localized evaluation.

```

```{r, eval=FALSE, echo=FALSE}
#pdf("lime_plot_features_cn.pdf")
plot_features (explanation) +
  labs (title = "LIME: Feature Importance Visualization") 
#dev.off()
```


```{r, eval=FALSE, echo=FALSE}
#pdf("lime_feature_heatmaf_cn.pdf")
plot_explanations (explanation) +
  labs (title = "LIME Feature Importance Heatmap",
        subtitle = "Hold Out (Test) Set, First 10 Cases Shown")
#dev.off()
```

# Results

```{r}
(res.testing)
```


# SHAP value (Global explanation)

```{r}
library(kernelshap)
library(shapviz)
library(ggplot2)
library(patchwork)
X <- x_test 
s <- shapviz(kernelshap(model, X, bg_X = x_train))
sv_importance(s, kind = "bee", show_numbers = TRUE) + theme_classic()
#sv_dependence(s, colnames(X), color_var = NULL) &
#  ylim(-4, 4)
sv_importance(s, max_display=30)
sv_waterfall(s, row_id = 3)
```

```{r}
#pdf("beeswarm_features_cn.pdf")
#sv_importance(s, kind = "bee", show_numbers = TRUE) + theme_classic()
#dev.off()
```

Mean shap value for each gene
```{r}
shap.value.list<- colMeans(abs(s$S))
ordered.shap.value.list <- shap.value.list[order(-abs(shap.value.list))]
histogram(ordered.shap.value.list, breaks = 50)
```

```{r}
names(ordered.shap.value.list)
```

Logistic regression with top 3 most important genes by shap.
```{r}
shap.cutoff <- 6

new.train <- training.df[,c("Tumor",names(ordered.shap.value.list)[1:shap.cutoff])]
new.test <- testing.df[,c("Tumor",names(ordered.shap.value.list)[1:shap.cutoff])]

logistic.shap <- glm(Tumor ~., family = "binomial", data = new.train)

summary(logistic.shap)
```

```{r}
predicted.proba.logistic.shap.test <- predict(logistic.shap, new.test[-1], type="response")
pROC::auc(new.test[,1], predicted.proba.logistic.shap.test)

FNR(predicted.proba.logistic.shap.test, new.test[,1])
FPR(predicted.proba.logistic.shap.test, new.test[,1])

names(ordered.shap.value.list)[1:shap.cutoff]
```



Trial with 3 first genes without ordering by shap (unfinished: I would like to test random combinations and compute averaged metrics)

```{r}
trial.train <- training.df[,1:shap.cutoff]
trial.test <- testing.df[,1:shap.cutoff]

logistic.shap <- glm(Tumor ~., family = "binomial", data = trial.train)

predicted.proba.logistic.shap.test <- predict(logistic.shap, trial.test[-1], type="response")
pROC::auc(trial.test[,1], predicted.proba.logistic.shap.test)

FNR(predicted.proba.logistic.shap.test, trial.test[,1])
FPR(predicted.proba.logistic.shap.test, trial.test[,1])
```


# Session information

```{r, child="_session-info.Rmd"}
```

# References

