<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Biomarker prediction in preeclampsia data</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       </style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
  p.abstract{
    text-align: center;
    font-weight: bold;
  }
  div.abstract{
    margin: auto;
    width: 90%;
  }
</style>


<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="/home/adam/R/x86_64-pc-linux-gnu-library/4.4/BiocStyle/resources/html/bioconductor.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 828px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {

}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 246px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



<script>
function toggle_visibility(id1) {
  var e = document.getElementById(id1);
  e.style.display = ((e.style.display!="none") ? "none" : "block");
}
</script>

</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Biomarker Discovery in cfRNA data</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="01_qa_pe.html">QA Preeclampsia</a>
</li>
<li>
  <a href="02_de_pe.html">DE Preeclampsia</a>
</li>
<li>
  <a href="03_qa_cn.html">QA Cancer</a>
</li>
<li>
  <a href="04_de_cn.html">DE Cancer</a>
</li>
<li>
  <a href="05_pr_pe.html">Prediction preeclampsia</a>
</li>
<li>
  <a href="06_pr_cn.html">Prediction cancer</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Biomarker prediction in preeclampsia data</h1>
<p class="author-name">Berta Canal Sim√≥n<span class="affil-mark">1*</span> and Adam Olivares Canal<span class="affil-mark">1**</span></p>
<p class="author-affiliation"><span class="affil-mark">1</span>Barcelona School of Economics</p>
<p class="author-email"><span class="affil-mark">*</span><a href="mailto:berta.canal@bse.eu">berta.canal@bse.eu</a><br><span class="affil-mark">**</span><a href="mailto:adam.olivares@bse.eu">adam.olivares@bse.eu</a></p>
<h4 class="date">junio 1, 2024</h4>
<h4 class="abstract">Abstract</h4>
<p>Here we perform a prediction of candidate biomarkers in preeclampsia cfRNA sequencing data.</p>

</div>


<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<div id="importing-processed-and-filtered-data" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Importing processed and filtered data</h1>
<p>We start by importing the previously filtered and normalized RNA-seq data.</p>
<pre class="r"><code>library(SummarizedExperiment)
library(edgeR)

dgeM.filt &lt;- readRDS(file.path(&quot;_processed_data&quot;, &quot;dgeM.filt.rds&quot;))
seM.filt &lt;- readRDS(file.path(&quot;_processed_data&quot;, &quot;seM.filt.rds&quot;))
dgeD.filt &lt;- readRDS(file.path(&quot;_processed_data&quot;, &quot;dgeD.filt.rds&quot;))
seD.filt &lt;- readRDS(file.path(&quot;_processed_data&quot;, &quot;seD.filt.rds&quot;))
dgeM.filt.training &lt;- readRDS(file.path(&quot;_processed_data&quot;,
                                        &quot;dgeM.filt.training.rds&quot;))
seM.filt.training &lt;- readRDS(file.path(&quot;_processed_data&quot;,
                                       &quot;seM.filt.training.rds&quot;))
dgeM.filt.testing &lt;- readRDS(file.path(&quot;_processed_data&quot;,
                                       &quot;dgeM.filt.testing.rds&quot;))
seM.filt.testing &lt;- readRDS(file.path(&quot;_processed_data&quot;,
                                      &quot;seM.filt.testing.rds&quot;))
dgeD.filt.subset &lt;- readRDS(file.path(&quot;_processed_data&quot;,
                                      &quot;dgeD.filt.subset.rds&quot;))
seD.filt.subset &lt;- readRDS(file.path(&quot;_processed_data&quot;,
                                     &quot;seD.filt.subset.rds&quot;))
DEgenes.trainingM &lt;- readRDS(file.path(&quot;_processed_data&quot;, 
                                      &quot;DEgenes.trainingM.rds&quot;))
DEgenes.testingM &lt;- readRDS(file.path(&quot;_processed_data&quot;, 
                                      &quot;DEgenes.testingM.rds&quot;))
DEgenes.testingD &lt;- readRDS(file.path(&quot;_processed_data&quot;, 
                                      &quot;DEgenes.testingD.rds&quot;))</code></pre>
<p>Train-testing subset creation: Intersection between differential expressed genes from training set from <span class="citation">Roskams-Hieter et al. (<a href="#ref-Roskams2022">2022</a>)</span> and lowly expressed genes from testing set.</p>
<pre class="r"><code>set.seed(111)
intersection.genes &lt;- Reduce(intersect, list(DEgenes.trainingM, rownames(dgeM.filt.testing), rownames(dgeD.filt.subset)))
length(intersection.genes)
[1] 912</code></pre>
<pre class="r"><code>
#intersection.genes &lt;- intersect(intersect(DEgenes.trainingM, rownames(dgeM.filt.testing)), rownames(dgeD.filt.subset))
#length(intersection.genes)

dgeM.intercept.training &lt;- dgeM.filt.training[intersection.genes,]
dim(dgeM.intercept.training)
[1] 912 244</code></pre>
<pre class="r"><code>seM.intercept.training &lt;- seM.filt.training[intersection.genes,]
dim(seM.intercept.training)
[1] 912 244</code></pre>
<pre class="r"><code>
dgeM.intercept.testing &lt;- dgeM.filt.testing[intersection.genes,]
dim(dgeM.intercept.testing)
[1] 912  89</code></pre>
<pre class="r"><code>seM.intercept.testing &lt;- seM.filt.testing[intersection.genes,]
dim(seM.intercept.testing)
[1] 912  89</code></pre>
<pre class="r"><code>
dgeD.intercept.testing &lt;- dgeD.filt.subset[intersection.genes,]
dim(dgeD.intercept.testing)
[1] 912  76</code></pre>
<pre class="r"><code>seD.intercept.testing &lt;- seD.filt.subset[intersection.genes,]
dim(seD.intercept.testing)
[1] 912  76</code></pre>
<div id="pregnancy-trimesters" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Pregnancy trimesters</h2>
<p>Gestational age is used to classify the samples into its corresponding trimester of pregnancy according to <a href="https://www.nichd.nih.gov/health/topics/factsheets/pregnancy">NIH</a>: first trimester (week 1 to week 12), second trimester (week 13 to week 28) and third trimester (week 29 to week 40).</p>
<div id="training-data" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Training data</h3>
<pre class="r"><code>table(as.factor(seM.intercept.training$SamplingGAgroup))

  ‚â§12 weeks gestation   ‚â•23 weeks gestation 13-20 weeks gestation 
                   72                    84                    88 </code></pre>
<pre class="r"><code>mask &lt;- seM.intercept.training$SamplingGA &lt;= 16
seM.intercept.training$SamplingGAgroup17[mask] &lt;- &quot;Pre17weeks&quot;
dgeM.intercept.training$samples$SamplingGAgroup17[mask] &lt;- &quot;Pre17weeks&quot;

mask &lt;- seM.intercept.training$SamplingGA &gt;= 17
seM.intercept.training$SamplingGAgroup17[mask] &lt;- &quot;Post17weeks&quot;
dgeM.intercept.training$samples$SamplingGAgroup17[mask] &lt;- &quot;Post17weeks&quot;</code></pre>
<pre class="r"><code>table(as.factor(seM.intercept.training$SamplingGAgroup17))

Post17weeks  Pre17weeks 
        116         128 </code></pre>
</div>
<div id="testing-data-1" class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Testing data 1</h3>
<pre class="r"><code>table(as.factor(seM.intercept.testing$SamplingGAgroup))

  ‚â§12 weeks gestation 13-20 weeks gestation 
                   56                    33 </code></pre>
<pre class="r"><code>mask &lt;- seM.intercept.testing$SamplingGA &lt;= 16
seM.intercept.testing$SamplingGAgroup17[mask] &lt;- &quot;Pre17weeks&quot;
dgeM.intercept.testing$samples$SamplingGAgroup17[mask] &lt;- &quot;Pre17weeks&quot;

mask &lt;- seM.intercept.testing$SamplingGA &gt;= 17
seM.intercept.testing$SamplingGAgroup17[mask] &lt;- &quot;Post17weeks&quot;
dgeM.intercept.testing$samples$SamplingGAgroup17[mask] &lt;- &quot;Post17weeks&quot;</code></pre>
<pre class="r"><code>table(as.factor(seM.intercept.testing$SamplingGAgroup17))

Pre17weeks 
        89 </code></pre>
</div>
<div id="testing-data-2" class="section level3" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Testing data 2</h3>
<pre class="r"><code>table(as.factor(seD.intercept.testing$SamplingGAgroup))

1st Trimester 2nd Trimester 3rd Trimester 
           25            26            25 </code></pre>
<pre class="r"><code>mask &lt;- seD.intercept.testing$SamplingGA &lt;= 16
seD.intercept.testing$SamplingGAgroup17[mask] &lt;- &quot;Pre17weeks&quot;
dgeD.intercept.testing$samples$SamplingGAgroup17[mask] &lt;- &quot;Pre17weeks&quot;

mask &lt;- seD.intercept.testing$SamplingGA &gt;= 17
seD.intercept.testing$SamplingGAgroup17[mask] &lt;- &quot;Post17weeks&quot;
dgeD.intercept.testing$samples$SamplingGAgroup17[mask] &lt;- &quot;Post17weeks&quot;</code></pre>
<pre class="r"><code>table(as.factor(seD.intercept.testing$SamplingGAgroup17))

Post17weeks  Pre17weeks 
         55          21 </code></pre>
</div>
</div>
<div id="dataframes-creation" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Dataframes creation</h2>
<div id="training-data-1" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Training data</h3>
<pre class="r"><code># SamplingGA or samplingGAgroup?
training.df &lt;- data.frame(Preeclampsia = seM.intercept.training$Preeclampsia,
                          SamplingGA = scale(seM.intercept.training$SamplingGA, scale = TRUE, center = TRUE),
                          SamplingGAgroup17 = seM.intercept.training$SamplingGAgroup17,
                          scale(t(assays(seM.intercept.training)$logCPM), scale = TRUE, center = TRUE),
                          MotherID = seM.intercept.training$MotherID
                          )</code></pre>
<pre class="r"><code>mask &lt;- seM.intercept.training$SamplingGAgroup17 == &quot;Pre17weeks&quot;
pre17.training.df &lt;- training.df[mask,]</code></pre>
<pre class="r"><code>library(dplyr)

Attaching package: &#39;dplyr&#39;
The following object is masked from &#39;package:Biobase&#39;:

    combine
The following objects are masked from &#39;package:GenomicRanges&#39;:

    intersect, setdiff, union
The following object is masked from &#39;package:GenomeInfoDb&#39;:

    intersect
The following objects are masked from &#39;package:IRanges&#39;:

    collapse, desc, intersect, setdiff, slice, union
The following objects are masked from &#39;package:S4Vectors&#39;:

    first, intersect, rename, setdiff, setequal, union
The following objects are masked from &#39;package:BiocGenerics&#39;:

    combine, intersect, setdiff, union
The following object is masked from &#39;package:matrixStats&#39;:

    count
The following object is masked from &#39;package:kableExtra&#39;:

    group_rows
The following objects are masked from &#39;package:stats&#39;:

    filter, lag
The following objects are masked from &#39;package:base&#39;:

    intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>
pre17.training.df &lt;- pre17.training.df %&gt;%
group_by(MotherID) %&gt;%
slice_min(order_by = SamplingGA) %&gt;%
ungroup() %&gt;%
#select(-MotherID, -SamplingGAgroup17) 
select(-MotherID, -SamplingGAgroup17, -SamplingGA)</code></pre>
</div>
<div id="testing-data-1-1" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Testing data 1</h3>
<pre class="r"><code>testing.df1 &lt;- data.frame(Preeclampsia = seM.intercept.testing$Preeclampsia,
                          SamplingGA = scale(seM.intercept.testing$SamplingGA, scale = TRUE, center = TRUE),
                          SamplingGAgroup17 = seM.intercept.testing$SamplingGAgroup17,
                          scale(t(assays(seM.intercept.testing)$logCPM), scale = TRUE, center = TRUE),
                          MotherID = seM.intercept.testing$MotherID
                          )</code></pre>
<pre class="r"><code>mask &lt;- seM.intercept.testing$SamplingGAgroup17 == &quot;Pre17weeks&quot;
pre17.testing.df1 &lt;- testing.df1[mask,]</code></pre>
<pre class="r"><code>pre17.testing.df1 &lt;- pre17.testing.df1 %&gt;%
group_by(MotherID) %&gt;%
slice_min(order_by = SamplingGA) %&gt;%
ungroup() %&gt;%
#select(-MotherID, -SamplingGAgroup17) 
select(-MotherID, -SamplingGAgroup17, -SamplingGA)</code></pre>
</div>
<div id="testing-data-2-1" class="section level3" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Testing data 2</h3>
<pre class="r"><code>testing.df2 &lt;- data.frame(Preeclampsia = seD.intercept.testing$Preeclampsia,
                          SamplingGA = scale(seD.intercept.testing$SamplingGA, scale = TRUE, center = TRUE),
                          SamplingGAgroup17 = seD.intercept.testing$SamplingGAgroup17,
                          scale(t(assays(seD.intercept.testing)$logCPM), scale = TRUE, center = TRUE),
                          MotherID = seD.intercept.testing$MotherID
                          )</code></pre>
<pre class="r"><code>mask &lt;- seD.intercept.testing$SamplingGAgroup17 == &quot;Pre17weeks&quot;
pre17.testing.df2 &lt;- testing.df2[mask,]</code></pre>
<pre class="r"><code>pre17.testing.df2 &lt;- pre17.testing.df2 %&gt;%
group_by(MotherID) %&gt;%
slice_min(order_by = SamplingGA) %&gt;%
ungroup() %&gt;%
#select(-MotherID, -SamplingGAgroup17) 
select(-MotherID, -SamplingGAgroup17, -SamplingGA)</code></pre>
</div>
</div>
</div>
<div id="target-encoding" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Target encoding</h1>
<pre class="r"><code>#library(dataPreparation)

#target_encoding &lt;- build_target_encoding(
#  training.df,
#  &#39;SamplingGAgroup&#39;,
#  &#39;Preeclampsia&#39;,
#  functions = &quot;mean&quot;,
#  verbose = TRUE
#)

#target_encoding</code></pre>
<pre class="r"><code>#training.df &lt;- as.data.frame(target_encode(training.df, target_encoding, drop = TRUE, verbose = TRUE))
#testing.df1 &lt;- as.data.frame(target_encode(testing.df1, target_encoding, drop = TRUE, verbose = TRUE))
#testing.df2 &lt;- as.data.frame(target_encode(testing.df2, target_encoding, drop = TRUE, verbose = TRUE))</code></pre>
<pre class="r"><code>#training.df$Preeclampsia &lt;- factor(training.df$Preeclampsia, labels = c(&#39;yes&#39;, &#39;no&#39;))</code></pre>
</div>
<div id="performance-metrics" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Performance metrics</h1>
<p>Given the application of the current paper, the False Negative Rate (FNR) metric is a particularly relevant metric, since it would imply classifying as healthy an individual with cancer. Therefore, that patients will not receive treatment, which will cause serious consequences. Furthermore, it could also be considered the False Positive Rate (FPR), which results in an undesirable situation where a proportion of healthy individuals are categorized as ill.¬†This would subject a healthy patient to unnecessary treatment and its potential side effects. However, since the expected consequences are not that severe, FNR is prioritized in the analysis.</p>
<pre class="r"><code>FNR &lt;- function(proba.pred, truth){
  class.pred &lt;- as.numeric(proba.pred &gt; 0.35)
  conf &lt;- table(truth, class.pred)
  print(conf)
  FNR &lt;- conf[2, 1] / sum(conf[2, 1], conf[2, 2])
  return(FNR)
}</code></pre>
<pre class="r"><code>FPR &lt;- function(proba.pred, truth){
  class.pred &lt;- as.numeric(proba.pred &gt; 0.35)
  conf &lt;- table(truth, class.pred)
  print(conf)
  FPR &lt;- conf[1, 2] / sum(conf[1, 1], conf[1, 2])
  return(FPR)
}</code></pre>
</div>
<div id="xgboost" class="section level1" number="4">
<h1><span class="header-section-number">4</span> XGBOOST</h1>
<pre class="r"><code>modelLookup(&quot;xgbTree&quot;)
    model        parameter                          label forReg forClass
1 xgbTree          nrounds          # Boosting Iterations   TRUE     TRUE
2 xgbTree        max_depth                 Max Tree Depth   TRUE     TRUE
3 xgbTree              eta                      Shrinkage   TRUE     TRUE
4 xgbTree            gamma         Minimum Loss Reduction   TRUE     TRUE
5 xgbTree colsample_bytree     Subsample Ratio of Columns   TRUE     TRUE
6 xgbTree min_child_weight Minimum Sum of Instance Weight   TRUE     TRUE
7 xgbTree        subsample           Subsample Percentage   TRUE     TRUE
  probModel
1      TRUE
2      TRUE
3      TRUE
4      TRUE
5      TRUE
6      TRUE
7      TRUE</code></pre>
<pre class="r"><code># CV technique which will be passed into the train() function
train_control = trainControl(method = &quot;cv&quot;, number = 10, search = &quot;grid&quot;,
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

# tuning grid
set.seed(111)
#xgboostGrid &lt;-  expand.grid(max_depth = c(3, 5, 7, 9), 
#                        nrounds = (1:10)*20,    # number of trees
#                        eta = c(0.2,0.3,0.4),
#                        gamma = c(0.5,1),
#                        subsample = c(0.5, 0.6, 0.7), # common value: between 0.5 and 1
#                        min_child_weight = c(1,2,3),
#                        colsample_bytree = c(0.5, 0.6, 0.7) # common value: between 0.5 and 1
#                        )


xgboostGrid &lt;- expand.grid(max_depth = c(3, 4, 5, 6, 7), nrounds = (1:20)*10, eta = c(0.15,0.2,0.35, 0.4),gamma = c(0.5,1, 1.5),subsample = c(0.8),min_child_weight = c(2),colsample_bytree = c(0.8))


#auc = 0.61, 0.52 (fnr =0.95, 0.8) xgboostGrid &lt;- expand.grid(max_depth = c(3, 4, 5, 6), nrounds = (1:10)*20, eta = c(0.2),gamma = c(0.5,1),subsample = c(0.8),min_child_weight = c(2),colsample_bytree = c(0.8)). nrounds = 80, max_depth = 5, eta = 0.2, gamma = 0.5, colsample_bytree = 0.8, min_child_weight = 2 and subsample = 0.8.

# auc = 0.65, 0.56 fnr = 0.82, 0.87. max_depth = c(3, 4). nrounds = 200, max_depth = 4, eta = 0.2, gamma = 0.5, colsample_bytree = 0.8, min_child_weight = 2 and subsample = 0.8.

#nrounds = (1:10)*40 no canvia el auc, fnr
# auc = 0.42, 0.55. eta = c(0.01)
# auc = 0.63, 0.55 fnr= 0.67, 0.87. eta = c(0.05,0.1)
#auc = (0.66, 0.55) fnr=(0.89,0.92). gamma = c(2,3)
# auc = (0.65, 0.56), fnr=0.82, 0.83. gamma = c(1,1.5)
# auc = 0.65, 0.53 fnr= 0.75, 0.95. subsample = c(0.5)
#auc =0.68, 0.50 fnr= 0.82, 0.91. subsample =c(0.6)
#auc=0.68, 0.55 fnr=0.75, 0.95. min_child_weight = c(3)
# colsample_bytree = c(0.8) empitjora


# hyperparaemeter search for XGboost classifier tree model
model = caret::train(Preeclampsia~., data = pre17.training.df,
              ######ALTERNATIVE#######
              #x = trainMNX,
              #y = trainMNY,
              ########################
              method = &quot;xgbTree&quot;,
              trControl = train_control,
              metric = &quot;ROC&quot;,
              tuneGrid = xgboostGrid,
              verbosity = 0,
              verbose = FALSE,
              #nthreads = 4 #cores in use
              )

print(model)
eXtreme Gradient Boosting 

 90 samples
912 predictors
  2 classes: &#39;no&#39;, &#39;yes&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 80, 81, 81, 82, 80, 81, ... 
Resampling results across tuning parameters:

  eta   max_depth  gamma  nrounds  ROC        Sens       Spec      
  0.15  3          0.5     10      0.7238095  0.9238095  0.25000000
  0.15  3          0.5     20      0.8071429  0.9214286  0.20000000
  0.15  3          0.5     30      0.7710317  0.9214286  0.18333333
  0.15  3          0.5     40      0.7757937  0.9214286  0.26666667
  0.15  3          0.5     50      0.7996032  0.9214286  0.26666667
  0.15  3          0.5     60      0.8115079  0.9214286  0.31666667
  0.15  3          0.5     70      0.8202381  0.9214286  0.31666667
  0.15  3          0.5     80      0.8202381  0.9214286  0.31666667
  0.15  3          0.5     90      0.8154762  0.9214286  0.31666667
  0.15  3          0.5    100      0.8154762  0.9214286  0.31666667
  0.15  3          0.5    110      0.8250000  0.9214286  0.31666667
  0.15  3          0.5    120      0.8146825  0.9214286  0.31666667
  0.15  3          0.5    130      0.8099206  0.9214286  0.31666667
  0.15  3          0.5    140      0.8099206  0.9214286  0.31666667
  0.15  3          0.5    150      0.8099206  0.9214286  0.31666667
  0.15  3          0.5    160      0.8051587  0.9214286  0.31666667
  0.15  3          0.5    170      0.8003968  0.9214286  0.31666667
  0.15  3          0.5    180      0.8003968  0.9214286  0.31666667
  0.15  3          0.5    190      0.8003968  0.9214286  0.31666667
  0.15  3          0.5    200      0.8003968  0.9214286  0.31666667
  0.15  3          1.0     10      0.7341270  0.8904762  0.23333333
  0.15  3          1.0     20      0.7273810  0.9047619  0.18333333
  0.15  3          1.0     30      0.7599206  0.9214286  0.20000000
  0.15  3          1.0     40      0.7750000  0.9214286  0.20000000
  0.15  3          1.0     50      0.7924603  0.9214286  0.23333333
  0.15  3          1.0     60      0.7932540  0.9214286  0.23333333
  0.15  3          1.0     70      0.7861111  0.9214286  0.23333333
  0.15  3          1.0     80      0.7908730  0.9214286  0.23333333
  0.15  3          1.0     90      0.7908730  0.9214286  0.23333333
  0.15  3          1.0    100      0.7908730  0.9214286  0.23333333
  0.15  3          1.0    110      0.7908730  0.9214286  0.23333333
  0.15  3          1.0    120      0.7908730  0.9214286  0.23333333
  0.15  3          1.0    130      0.7908730  0.9214286  0.23333333
  0.15  3          1.0    140      0.7936508  0.9214286  0.23333333
  0.15  3          1.0    150      0.7936508  0.9214286  0.23333333
  0.15  3          1.0    160      0.7936508  0.9214286  0.23333333
  0.15  3          1.0    170      0.7936508  0.9214286  0.26666667
  0.15  3          1.0    180      0.7936508  0.9214286  0.20000000
  0.15  3          1.0    190      0.7936508  0.9214286  0.23333333
  0.15  3          1.0    200      0.7936508  0.9214286  0.26666667
  0.15  3          1.5     10      0.6984127  0.9047619  0.18333333
  0.15  3          1.5     20      0.7531746  0.9357143  0.13333333
  0.15  3          1.5     30      0.7888889  0.9071429  0.20000000
  0.15  3          1.5     40      0.7765873  0.9071429  0.20000000
  0.15  3          1.5     50      0.7765873  0.9071429  0.20000000
  0.15  3          1.5     60      0.7718254  0.8904762  0.20000000
  0.15  3          1.5     70      0.7718254  0.8904762  0.20000000
  0.15  3          1.5     80      0.7718254  0.8904762  0.20000000
  0.15  3          1.5     90      0.7670635  0.9047619  0.20000000
  0.15  3          1.5    100      0.7670635  0.9047619  0.20000000
  0.15  3          1.5    110      0.7615079  0.9047619  0.20000000
  0.15  3          1.5    120      0.7638889  0.9047619  0.20000000
  0.15  3          1.5    130      0.7638889  0.9047619  0.20000000
  0.15  3          1.5    140      0.7638889  0.9047619  0.20000000
  0.15  3          1.5    150      0.7734127  0.8904762  0.20000000
  0.15  3          1.5    160      0.7734127  0.8904762  0.20000000
  0.15  3          1.5    170      0.7734127  0.8904762  0.20000000
  0.15  3          1.5    180      0.7734127  0.8904762  0.20000000
  0.15  3          1.5    190      0.7710317  0.9047619  0.20000000
  0.15  3          1.5    200      0.7710317  0.9047619  0.20000000
  0.15  4          0.5     10      0.6900794  0.9357143  0.15000000
  0.15  4          0.5     20      0.6861111  0.9047619  0.06666667
  0.15  4          0.5     30      0.6968254  0.9214286  0.11666667
  0.15  4          0.5     40      0.7214286  0.9214286  0.15000000
  0.15  4          0.5     50      0.7321429  0.9047619  0.18333333
  0.15  4          0.5     60      0.7527778  0.9214286  0.18333333
  0.15  4          0.5     70      0.7472222  0.9214286  0.13333333
  0.15  4          0.5     80      0.7472222  0.9214286  0.13333333
  0.15  4          0.5     90      0.7472222  0.9214286  0.13333333
  0.15  4          0.5    100      0.7388889  0.9214286  0.13333333
  0.15  4          0.5    110      0.7341270  0.9214286  0.13333333
  0.15  4          0.5    120      0.7341270  0.9214286  0.13333333
  0.15  4          0.5    130      0.7293651  0.9214286  0.18333333
  0.15  4          0.5    140      0.7293651  0.9214286  0.18333333
  0.15  4          0.5    150      0.7341270  0.9214286  0.18333333
  0.15  4          0.5    160      0.7341270  0.9214286  0.18333333
  0.15  4          0.5    170      0.7285714  0.9214286  0.18333333
  0.15  4          0.5    180      0.7285714  0.9214286  0.18333333
  0.15  4          0.5    190      0.7285714  0.9214286  0.18333333
  0.15  4          0.5    200      0.7285714  0.9214286  0.18333333
  0.15  4          1.0     10      0.7289683  0.9190476  0.05000000
  0.15  4          1.0     20      0.7599206  0.9357143  0.13333333
  0.15  4          1.0     30      0.7607143  0.9357143  0.18333333
  0.15  4          1.0     40      0.7773810  0.9357143  0.20000000
  0.15  4          1.0     50      0.7813492  0.9214286  0.18333333
  0.15  4          1.0     60      0.7718254  0.9047619  0.26666667
  0.15  4          1.0     70      0.7646825  0.9214286  0.26666667
  0.15  4          1.0     80      0.7646825  0.9357143  0.26666667
  0.15  4          1.0     90      0.7742063  0.9214286  0.26666667
  0.15  4          1.0    100      0.7742063  0.9357143  0.26666667
  0.15  4          1.0    110      0.7646825  0.9357143  0.26666667
  0.15  4          1.0    120      0.7718254  0.9357143  0.26666667
  0.15  4          1.0    130      0.7718254  0.9214286  0.26666667
  0.15  4          1.0    140      0.7718254  0.9214286  0.26666667
  0.15  4          1.0    150      0.7670635  0.9214286  0.26666667
  0.15  4          1.0    160      0.7670635  0.9214286  0.26666667
  0.15  4          1.0    170      0.7670635  0.9214286  0.26666667
  0.15  4          1.0    180      0.7670635  0.9214286  0.26666667
  0.15  4          1.0    190      0.7670635  0.9214286  0.26666667
  0.15  4          1.0    200      0.7670635  0.9214286  0.21666667
  0.15  4          1.5     10      0.6980159  0.8904762  0.23333333
  0.15  4          1.5     20      0.7087302  0.9190476  0.13333333
  0.15  4          1.5     30      0.7146825  0.9047619  0.10000000
  0.15  4          1.5     40      0.7250000  0.9047619  0.11666667
  0.15  4          1.5     50      0.7468254  0.8904762  0.15000000
  0.15  4          1.5     60      0.7396825  0.8904762  0.20000000
  0.15  4          1.5     70      0.7547619  0.8904762  0.20000000
  0.15  4          1.5     80      0.7408730  0.8904762  0.20000000
  0.15  4          1.5     90      0.7408730  0.8904762  0.20000000
  0.15  4          1.5    100      0.7408730  0.8904762  0.20000000
  0.15  4          1.5    110      0.7551587  0.8904762  0.20000000
  0.15  4          1.5    120      0.7551587  0.8904762  0.20000000
  0.15  4          1.5    130      0.7503968  0.8904762  0.23333333
  0.15  4          1.5    140      0.7587302  0.8904762  0.23333333
  0.15  4          1.5    150      0.7825397  0.8904762  0.23333333
  0.15  4          1.5    160      0.7742063  0.8904762  0.23333333
  0.15  4          1.5    170      0.7742063  0.8904762  0.23333333
  0.15  4          1.5    180      0.7742063  0.8904762  0.23333333
  0.15  4          1.5    190      0.7742063  0.8904762  0.20000000
  0.15  4          1.5    200      0.7789683  0.8904762  0.20000000
  0.15  5          0.5     10      0.6992063  0.9238095  0.08333333
  0.15  5          0.5     20      0.6757937  0.8738095  0.16666667
  0.15  5          0.5     30      0.7111111  0.8738095  0.21666667
  0.15  5          0.5     40      0.7087302  0.8738095  0.25000000
  0.15  5          0.5     50      0.7170635  0.8738095  0.25000000
  0.15  5          0.5     60      0.7218254  0.8738095  0.25000000
  0.15  5          0.5     70      0.7194444  0.8738095  0.25000000
  0.15  5          0.5     80      0.7313492  0.8738095  0.25000000
  0.15  5          0.5     90      0.7337302  0.8738095  0.25000000
  0.15  5          0.5    100      0.7527778  0.8738095  0.25000000
  0.15  5          0.5    110      0.7527778  0.8738095  0.25000000
  0.15  5          0.5    120      0.7527778  0.8738095  0.25000000
  0.15  5          0.5    130      0.7527778  0.8738095  0.25000000
  0.15  5          0.5    140      0.7527778  0.8738095  0.25000000
  0.15  5          0.5    150      0.7480159  0.8738095  0.25000000
  0.15  5          0.5    160      0.7535714  0.8738095  0.25000000
  0.15  5          0.5    170      0.7535714  0.8738095  0.25000000
  0.15  5          0.5    180      0.7535714  0.8738095  0.25000000
  0.15  5          0.5    190      0.7583333  0.8738095  0.25000000
  0.15  5          0.5    200      0.7583333  0.8738095  0.25000000
  0.15  5          1.0     10      0.7376984  0.8928571  0.11666667
  0.15  5          1.0     20      0.7694444  0.8928571  0.16666667
  0.15  5          1.0     30      0.7853175  0.8928571  0.20000000
  0.15  5          1.0     40      0.7765873  0.8928571  0.23333333
  0.15  5          1.0     50      0.7539683  0.8619048  0.23333333
  0.15  5          1.0     60      0.7722222  0.8761905  0.23333333
  0.15  5          1.0     70      0.7722222  0.8761905  0.23333333
  0.15  5          1.0     80      0.7722222  0.8619048  0.23333333
  0.15  5          1.0     90      0.7674603  0.8761905  0.26666667
  0.15  5          1.0    100      0.7674603  0.8619048  0.26666667
  0.15  5          1.0    110      0.7674603  0.8619048  0.26666667
  0.15  5          1.0    120      0.7674603  0.8619048  0.26666667
  0.15  5          1.0    130      0.7674603  0.8785714  0.26666667
  0.15  5          1.0    140      0.7674603  0.8619048  0.26666667
  0.15  5          1.0    150      0.7674603  0.8785714  0.26666667
  0.15  5          1.0    160      0.7674603  0.8785714  0.26666667
  0.15  5          1.0    170      0.7674603  0.8785714  0.26666667
  0.15  5          1.0    180      0.7674603  0.8785714  0.26666667
  0.15  5          1.0    190      0.7674603  0.8785714  0.23333333
  0.15  5          1.0    200      0.7674603  0.8785714  0.26666667
  0.15  5          1.5     10      0.6853175  0.8452381  0.16666667
  0.15  5          1.5     20      0.6865079  0.8619048  0.20000000
  0.15  5          1.5     30      0.6865079  0.8595238  0.13333333
  0.15  5          1.5     40      0.7293651  0.8904762  0.11666667
  0.15  5          1.5     50      0.7305556  0.8904762  0.20000000
  0.15  5          1.5     60      0.7436508  0.8761905  0.15000000
  0.15  5          1.5     70      0.7492063  0.8761905  0.18333333
  0.15  5          1.5     80      0.7492063  0.8761905  0.23333333
  0.15  5          1.5     90      0.7420635  0.8761905  0.23333333
  0.15  5          1.5    100      0.7420635  0.8761905  0.23333333
  0.15  5          1.5    110      0.7492063  0.8761905  0.23333333
  0.15  5          1.5    120      0.7492063  0.8761905  0.23333333
  0.15  5          1.5    130      0.7492063  0.8761905  0.23333333
  0.15  5          1.5    140      0.7492063  0.8761905  0.23333333
  0.15  5          1.5    150      0.7420635  0.8904762  0.18333333
  0.15  5          1.5    160      0.7468254  0.8761905  0.23333333
  0.15  5          1.5    170      0.7420635  0.8761905  0.23333333
  0.15  5          1.5    180      0.7420635  0.8761905  0.23333333
  0.15  5          1.5    190      0.7420635  0.8761905  0.23333333
  0.15  5          1.5    200      0.7373016  0.8904762  0.23333333
  0.15  6          0.5     10      0.6333333  0.8738095  0.11666667
  0.15  6          0.5     20      0.6976190  0.8928571  0.03333333
  0.15  6          0.5     30      0.7456349  0.8904762  0.23333333
  0.15  6          0.5     40      0.7678571  0.9071429  0.18333333
  0.15  6          0.5     50      0.7468254  0.8761905  0.23333333
  0.15  6          0.5     60      0.7527778  0.8904762  0.26666667
  0.15  6          0.5     70      0.7670635  0.8761905  0.33333333
  0.15  6          0.5     80      0.7599206  0.8761905  0.28333333
  0.15  6          0.5     90      0.7599206  0.8761905  0.23333333
  0.15  6          0.5    100      0.7599206  0.8904762  0.23333333
  0.15  6          0.5    110      0.7599206  0.8761905  0.28333333
  0.15  6          0.5    120      0.7599206  0.8761905  0.28333333
  0.15  6          0.5    130      0.7599206  0.8761905  0.31666667
  0.15  6          0.5    140      0.7599206  0.8761905  0.31666667
  0.15  6          0.5    150      0.7599206  0.8904762  0.31666667
  0.15  6          0.5    160      0.7599206  0.8904762  0.31666667
  0.15  6          0.5    170      0.7599206  0.8904762  0.31666667
  0.15  6          0.5    180      0.7599206  0.8761905  0.31666667
  0.15  6          0.5    190      0.7599206  0.8904762  0.31666667
  0.15  6          0.5    200      0.7599206  0.8904762  0.31666667
  0.15  6          1.0     10      0.6281746  0.9071429  0.23333333
  0.15  6          1.0     20      0.7134921  0.8928571  0.18333333
  0.15  6          1.0     30      0.7492063  0.8928571  0.18333333
  0.15  6          1.0     40      0.7567460  0.8761905  0.23333333
  0.15  6          1.0     50      0.7555556  0.8619048  0.25000000
  0.15  6          1.0     60      0.7642857  0.8619048  0.28333333
  0.15  6          1.0     70      0.7714286  0.8619048  0.28333333
  0.15  6          1.0     80      0.7666667  0.8761905  0.28333333
  0.15  6          1.0     90      0.7666667  0.8761905  0.28333333
  0.15  6          1.0    100      0.7666667  0.8761905  0.28333333
  0.15  6          1.0    110      0.7583333  0.8761905  0.28333333
  0.15  6          1.0    120      0.7583333  0.8761905  0.28333333
  0.15  6          1.0    130      0.7666667  0.8761905  0.28333333
  0.15  6          1.0    140      0.7666667  0.8761905  0.28333333
  0.15  6          1.0    150      0.7583333  0.8761905  0.28333333
  0.15  6          1.0    160      0.7583333  0.8761905  0.28333333
  0.15  6          1.0    170      0.7583333  0.8761905  0.28333333
  0.15  6          1.0    180      0.7535714  0.8761905  0.28333333
  0.15  6          1.0    190      0.7535714  0.8761905  0.28333333
  0.15  6          1.0    200      0.7535714  0.8761905  0.28333333
  0.15  6          1.5     10      0.6392857  0.9214286  0.25000000
  0.15  6          1.5     20      0.6884921  0.9380952  0.21666667
  0.15  6          1.5     30      0.7325397  0.9071429  0.21666667
  0.15  6          1.5     40      0.7666667  0.9071429  0.21666667
  0.15  6          1.5     50      0.7555556  0.8928571  0.25000000
  0.15  6          1.5     60      0.7555556  0.8928571  0.21666667
  0.15  6          1.5     70      0.7555556  0.8928571  0.25000000
  0.15  6          1.5     80      0.7555556  0.8928571  0.28333333
  0.15  6          1.5     90      0.7603175  0.9071429  0.28333333
  0.15  6          1.5    100      0.7603175  0.9071429  0.28333333
  0.15  6          1.5    110      0.7686508  0.9071429  0.28333333
  0.15  6          1.5    120      0.7630952  0.9071429  0.28333333
  0.15  6          1.5    130      0.7630952  0.9071429  0.28333333
  0.15  6          1.5    140      0.7630952  0.8928571  0.28333333
  0.15  6          1.5    150      0.7630952  0.9071429  0.28333333
  0.15  6          1.5    160      0.7630952  0.9071429  0.28333333
  0.15  6          1.5    170      0.7630952  0.9071429  0.28333333
  0.15  6          1.5    180      0.7630952  0.9071429  0.28333333
  0.15  6          1.5    190      0.7714286  0.9071429  0.28333333
  0.15  6          1.5    200      0.7666667  0.9071429  0.25000000
  0.15  7          0.5     10      0.7321429  0.9047619  0.11666667
  0.15  7          0.5     20      0.7011905  0.8761905  0.20000000
  0.15  7          0.5     30      0.7460317  0.9071429  0.23333333
  0.15  7          0.5     40      0.7484127  0.8738095  0.28333333
  0.15  7          0.5     50      0.7531746  0.8904762  0.28333333
  0.15  7          0.5     60      0.7615079  0.8904762  0.28333333
  0.15  7          0.5     70      0.7615079  0.8904762  0.31666667
  0.15  7          0.5     80      0.7615079  0.8904762  0.31666667
  0.15  7          0.5     90      0.7615079  0.8904762  0.28333333
  0.15  7          0.5    100      0.7615079  0.8904762  0.31666667
  0.15  7          0.5    110      0.7615079  0.8904762  0.31666667
  0.15  7          0.5    120      0.7615079  0.8904762  0.31666667
  0.15  7          0.5    130      0.7615079  0.8904762  0.31666667
  0.15  7          0.5    140      0.7615079  0.8904762  0.31666667
  0.15  7          0.5    150      0.7615079  0.8904762  0.31666667
  0.15  7          0.5    160      0.7615079  0.8904762  0.31666667
  0.15  7          0.5    170      0.7615079  0.8904762  0.31666667
  0.15  7          0.5    180      0.7615079  0.8904762  0.31666667
  0.15  7          0.5    190      0.7615079  0.8904762  0.31666667
  0.15  7          0.5    200      0.7615079  0.8904762  0.31666667
  0.15  7          1.0     10      0.7277778  0.8761905  0.26666667
  0.15  7          1.0     20      0.7289683  0.9071429  0.21666667
  0.15  7          1.0     30      0.7547619  0.8904762  0.21666667
  0.15  7          1.0     40      0.7662698  0.9071429  0.21666667
  0.15  7          1.0     50      0.7670635  0.8595238  0.21666667
  0.15  7          1.0     60      0.7670635  0.8761905  0.21666667
  0.15  7          1.0     70      0.7599206  0.8761905  0.25000000
  0.15  7          1.0     80      0.7654762  0.8761905  0.25000000
  0.15  7          1.0     90      0.7797619  0.8761905  0.25000000
  0.15  7          1.0    100      0.7658730  0.8761905  0.25000000
  0.15  7          1.0    110      0.7658730  0.8761905  0.25000000
  0.15  7          1.0    120      0.7658730  0.8761905  0.25000000
  0.15  7          1.0    130      0.7658730  0.8761905  0.25000000
  0.15  7          1.0    140      0.7642857  0.8761905  0.25000000
  0.15  7          1.0    150      0.7642857  0.8761905  0.25000000
  0.15  7          1.0    160      0.7642857  0.8761905  0.25000000
  0.15  7          1.0    170      0.7642857  0.8761905  0.25000000
  0.15  7          1.0    180      0.7642857  0.8761905  0.25000000
  0.15  7          1.0    190      0.7595238  0.8761905  0.25000000
  0.15  7          1.0    200      0.7595238  0.8761905  0.25000000
  0.15  7          1.5     10      0.6519841  0.9095238  0.15000000
  0.15  7          1.5     20      0.7325397  0.8785714  0.21666667
  0.15  7          1.5     30      0.7146825  0.8785714  0.16666667
  0.15  7          1.5     40      0.7000000  0.8928571  0.20000000
  0.15  7          1.5     50      0.7369048  0.8928571  0.25000000
  0.15  7          1.5     60      0.7369048  0.8928571  0.20000000
  0.15  7          1.5     70      0.7369048  0.8928571  0.25000000
  0.15  7          1.5     80      0.7369048  0.8928571  0.20000000
  0.15  7          1.5     90      0.7480159  0.8928571  0.20000000
  0.15  7          1.5    100      0.7480159  0.8928571  0.23333333
  0.15  7          1.5    110      0.7480159  0.8928571  0.28333333
  0.15  7          1.5    120      0.7424603  0.8928571  0.23333333
  0.15  7          1.5    130      0.7353175  0.8928571  0.23333333
  0.15  7          1.5    140      0.7353175  0.8928571  0.23333333
  0.15  7          1.5    150      0.7424603  0.8928571  0.23333333
  0.15  7          1.5    160      0.7424603  0.8928571  0.23333333
  0.15  7          1.5    170      0.7472222  0.9071429  0.26666667
  0.15  7          1.5    180      0.7472222  0.9071429  0.26666667
  0.15  7          1.5    190      0.7472222  0.9071429  0.26666667
  0.15  7          1.5    200      0.7424603  0.8928571  0.23333333
  0.20  3          0.5     10      0.6976190  0.9523810  0.20000000
  0.20  3          0.5     20      0.8007937  0.9380952  0.26666667
  0.20  3          0.5     30      0.7884921  0.9238095  0.23333333
  0.20  3          0.5     40      0.7944444  0.9238095  0.26666667
  0.20  3          0.5     50      0.7968254  0.9380952  0.26666667
  0.20  3          0.5     60      0.7920635  0.9380952  0.26666667
  0.20  3          0.5     70      0.7908730  0.9380952  0.26666667
  0.20  3          0.5     80      0.7956349  0.9380952  0.26666667
  0.20  3          0.5     90      0.7956349  0.9380952  0.26666667
  0.20  3          0.5    100      0.7956349  0.9380952  0.26666667
  0.20  3          0.5    110      0.7956349  0.9380952  0.26666667
  0.20  3          0.5    120      0.7956349  0.9380952  0.26666667
  0.20  3          0.5    130      0.7956349  0.9380952  0.26666667
  0.20  3          0.5    140      0.7956349  0.9380952  0.26666667
  0.20  3          0.5    150      0.7956349  0.9380952  0.26666667
  0.20  3          0.5    160      0.7956349  0.9380952  0.26666667
  0.20  3          0.5    170      0.7956349  0.9380952  0.26666667
  0.20  3          0.5    180      0.7956349  0.9380952  0.26666667
  0.20  3          0.5    190      0.7956349  0.9380952  0.26666667
  0.20  3          0.5    200      0.7956349  0.9380952  0.26666667
  0.20  3          1.0     10      0.5892857  0.8785714  0.13333333
  0.20  3          1.0     20      0.6460317  0.8571429  0.13333333
  0.20  3          1.0     30      0.6658730  0.8571429  0.16666667
  0.20  3          1.0     40      0.6753968  0.9047619  0.16666667
  0.20  3          1.0     50      0.6837302  0.9047619  0.16666667
  0.20  3          1.0     60      0.6908730  0.9047619  0.16666667
  0.20  3          1.0     70      0.6861111  0.9047619  0.16666667
  0.20  3          1.0     80      0.6964286  0.9214286  0.16666667
  0.20  3          1.0     90      0.6964286  0.9047619  0.16666667
  0.20  3          1.0    100      0.6964286  0.9047619  0.16666667
  0.20  3          1.0    110      0.6964286  0.9071429  0.16666667
  0.20  3          1.0    120      0.6964286  0.8904762  0.16666667
  0.20  3          1.0    130      0.6964286  0.9047619  0.16666667
  0.20  3          1.0    140      0.6964286  0.9047619  0.16666667
  0.20  3          1.0    150      0.6964286  0.9047619  0.16666667
  0.20  3          1.0    160      0.6964286  0.9047619  0.16666667
  0.20  3          1.0    170      0.6964286  0.8904762  0.16666667
  0.20  3          1.0    180      0.6964286  0.9047619  0.16666667
  0.20  3          1.0    190      0.6964286  0.9047619  0.16666667
  0.20  3          1.0    200      0.6964286  0.8904762  0.16666667
  0.20  3          1.5     10      0.6785714  0.8571429  0.23333333
  0.20  3          1.5     20      0.6682540  0.8904762  0.28333333
  0.20  3          1.5     30      0.6849206  0.8904762  0.31666667
  0.20  3          1.5     40      0.6908730  0.8904762  0.31666667
  0.20  3          1.5     50      0.7123016  0.8904762  0.26666667
  0.20  3          1.5     60      0.7170635  0.8904762  0.26666667
  0.20  3          1.5     70      0.7123016  0.8904762  0.26666667
  0.20  3          1.5     80      0.7218254  0.8904762  0.26666667
  0.20  3          1.5     90      0.7265873  0.8904762  0.26666667
  0.20  3          1.5    100      0.7321429  0.9071429  0.26666667
  0.20  3          1.5    110      0.7265873  0.8904762  0.26666667
  0.20  3          1.5    120      0.7265873  0.8904762  0.26666667
  0.20  3          1.5    130      0.7265873  0.8904762  0.26666667
  0.20  3          1.5    140      0.7265873  0.8904762  0.26666667
  0.20  3          1.5    150      0.7265873  0.8904762  0.26666667
  0.20  3          1.5    160      0.7265873  0.8904762  0.26666667
  0.20  3          1.5    170      0.7265873  0.8904762  0.31666667
  0.20  3          1.5    180      0.7194444  0.8904762  0.31666667
  0.20  3          1.5    190      0.7194444  0.8904762  0.26666667
  0.20  3          1.5    200      0.7194444  0.8904762  0.26666667
  0.20  4          0.5     10      0.6726190  0.9095238  0.20000000
  0.20  4          0.5     20      0.7472222  0.9095238  0.26666667
  0.20  4          0.5     30      0.7650794  0.8809524  0.20000000
  0.20  4          0.5     40      0.7515873  0.8809524  0.20000000
  0.20  4          0.5     50      0.7599206  0.8809524  0.20000000
  0.20  4          0.5     60      0.7623016  0.8809524  0.20000000
  0.20  4          0.5     70      0.7623016  0.8809524  0.20000000
  0.20  4          0.5     80      0.7623016  0.8809524  0.23333333
  0.20  4          0.5     90      0.7551587  0.8809524  0.23333333
  0.20  4          0.5    100      0.7468254  0.8809524  0.23333333
  0.20  4          0.5    110      0.7468254  0.8809524  0.23333333
  0.20  4          0.5    120      0.7468254  0.8809524  0.23333333
  0.20  4          0.5    130      0.7468254  0.8809524  0.23333333
  0.20  4          0.5    140      0.7468254  0.8809524  0.23333333
  0.20  4          0.5    150      0.7468254  0.8809524  0.23333333
  0.20  4          0.5    160      0.7468254  0.8809524  0.23333333
  0.20  4          0.5    170      0.7396825  0.8809524  0.23333333
  0.20  4          0.5    180      0.7396825  0.8809524  0.23333333
  0.20  4          0.5    190      0.7396825  0.8809524  0.23333333
  0.20  4          0.5    200      0.7396825  0.8809524  0.23333333
  0.20  4          1.0     10      0.6579365  0.9071429  0.13333333
  0.20  4          1.0     20      0.6869048  0.9047619  0.11666667
  0.20  4          1.0     30      0.6928571  0.9047619  0.15000000
  0.20  4          1.0     40      0.7027778  0.9047619  0.18333333
  0.20  4          1.0     50      0.7075397  0.9047619  0.18333333
  0.20  4          1.0     60      0.7027778  0.9047619  0.18333333
  0.20  4          1.0     70      0.6932540  0.9071429  0.15000000
  0.20  4          1.0     80      0.6932540  0.8761905  0.15000000
  0.20  4          1.0     90      0.6932540  0.8761905  0.15000000
  0.20  4          1.0    100      0.6932540  0.8761905  0.15000000
  0.20  4          1.0    110      0.7123016  0.8904762  0.18333333
  0.20  4          1.0    120      0.7123016  0.9047619  0.15000000
  0.20  4          1.0    130      0.7178571  0.8904762  0.15000000
  0.20  4          1.0    140      0.7178571  0.9047619  0.15000000
  0.20  4          1.0    150      0.7178571  0.9047619  0.15000000
  0.20  4          1.0    160      0.7178571  0.8904762  0.15000000
  0.20  4          1.0    170      0.7226190  0.9047619  0.15000000
  0.20  4          1.0    180      0.7226190  0.9047619  0.15000000
  0.20  4          1.0    190      0.7226190  0.9047619  0.15000000
  0.20  4          1.0    200      0.7226190  0.9047619  0.15000000
  0.20  4          1.5     10      0.7182540  0.9238095  0.28333333
  0.20  4          1.5     20      0.7170635  0.9380952  0.20000000
  0.20  4          1.5     30      0.7142857  0.8761905  0.16666667
  0.20  4          1.5     40      0.7055556  0.8738095  0.20000000
  0.20  4          1.5     50      0.7150794  0.8738095  0.20000000
  0.20  4          1.5     60      0.7055556  0.8595238  0.20000000
  0.20  4          1.5     70      0.7055556  0.8595238  0.20000000
  0.20  4          1.5     80      0.7055556  0.8571429  0.20000000
  0.20  4          1.5     90      0.7055556  0.8595238  0.20000000
  0.20  4          1.5    100      0.7055556  0.8595238  0.20000000
  0.20  4          1.5    110      0.7055556  0.8904762  0.20000000
  0.20  4          1.5    120      0.7055556  0.8904762  0.20000000
  0.20  4          1.5    130      0.7055556  0.8904762  0.20000000
  0.20  4          1.5    140      0.7055556  0.8904762  0.20000000
  0.20  4          1.5    150      0.7055556  0.8904762  0.20000000
  0.20  4          1.5    160      0.7055556  0.8904762  0.20000000
  0.20  4          1.5    170      0.7055556  0.8904762  0.20000000
  0.20  4          1.5    180      0.7055556  0.9047619  0.20000000
  0.20  4          1.5    190      0.7055556  0.8904762  0.20000000
  0.20  4          1.5    200      0.7055556  0.8904762  0.20000000
  0.20  5          0.5     10      0.6753968  0.8928571  0.13333333
  0.20  5          0.5     20      0.7115079  0.8761905  0.15000000
  0.20  5          0.5     30      0.7337302  0.9238095  0.18333333
  0.20  5          0.5     40      0.7186508  0.9238095  0.21666667
  0.20  5          0.5     50      0.7269841  0.9095238  0.21666667
  0.20  5          0.5     60      0.7515873  0.8928571  0.21666667
  0.20  5          0.5     70      0.7611111  0.8928571  0.21666667
  0.20  5          0.5     80      0.7611111  0.8928571  0.21666667
  0.20  5          0.5     90      0.7611111  0.9095238  0.21666667
  0.20  5          0.5    100      0.7611111  0.8928571  0.21666667
  0.20  5          0.5    110      0.7611111  0.8928571  0.21666667
  0.20  5          0.5    120      0.7611111  0.8928571  0.21666667
  0.20  5          0.5    130      0.7611111  0.8928571  0.21666667
  0.20  5          0.5    140      0.7611111  0.9095238  0.21666667
  0.20  5          0.5    150      0.7611111  0.9095238  0.21666667
  0.20  5          0.5    160      0.7611111  0.9095238  0.21666667
  0.20  5          0.5    170      0.7611111  0.9095238  0.21666667
  0.20  5          0.5    180      0.7563492  0.9095238  0.21666667
  0.20  5          0.5    190      0.7563492  0.9095238  0.21666667
  0.20  5          0.5    200      0.7563492  0.9095238  0.21666667
  0.20  5          1.0     10      0.7261905  0.9071429  0.18333333
  0.20  5          1.0     20      0.7591270  0.9214286  0.18333333
  0.20  5          1.0     30      0.8047619  0.9214286  0.26666667
  0.20  5          1.0     40      0.8023810  0.9214286  0.31666667
  0.20  5          1.0     50      0.8023810  0.9071429  0.35000000
  0.20  5          1.0     60      0.8023810  0.9071429  0.35000000
  0.20  5          1.0     70      0.8023810  0.9071429  0.31666667
  0.20  5          1.0     80      0.8023810  0.9071429  0.31666667
  0.20  5          1.0     90      0.7952381  0.9071429  0.31666667
  0.20  5          1.0    100      0.7952381  0.9071429  0.31666667
  0.20  5          1.0    110      0.7952381  0.9071429  0.35000000
  0.20  5          1.0    120      0.7952381  0.9071429  0.35000000
  0.20  5          1.0    130      0.7952381  0.9071429  0.35000000
  0.20  5          1.0    140      0.7952381  0.9071429  0.35000000
  0.20  5          1.0    150      0.7952381  0.9071429  0.35000000
  0.20  5          1.0    160      0.7952381  0.9071429  0.35000000
  0.20  5          1.0    170      0.7952381  0.9071429  0.35000000
  0.20  5          1.0    180      0.7952381  0.9071429  0.35000000
  0.20  5          1.0    190      0.7952381  0.9071429  0.35000000
  0.20  5          1.0    200      0.7952381  0.9071429  0.35000000
  0.20  5          1.5     10      0.6829365  0.8928571  0.18333333
  0.20  5          1.5     20      0.7289683  0.9071429  0.20000000
  0.20  5          1.5     30      0.7396825  0.9071429  0.23333333
  0.20  5          1.5     40      0.7543651  0.8904762  0.23333333
  0.20  5          1.5     50      0.7591270  0.8904762  0.23333333
  0.20  5          1.5     60      0.7722222  0.8904762  0.23333333
  0.20  5          1.5     70      0.7722222  0.8904762  0.23333333
  0.20  5          1.5     80      0.7722222  0.8761905  0.23333333
  0.20  5          1.5     90      0.7638889  0.8904762  0.23333333
  0.20  5          1.5    100      0.7591270  0.8904762  0.20000000
  0.20  5          1.5    110      0.7543651  0.8761905  0.20000000
  0.20  5          1.5    120      0.7615079  0.8761905  0.23333333
  0.20  5          1.5    130      0.7662698  0.8761905  0.20000000
  0.20  5          1.5    140      0.7746032  0.8761905  0.26666667
  0.20  5          1.5    150      0.7746032  0.8928571  0.26666667
  0.20  5          1.5    160      0.7746032  0.8928571  0.26666667
  0.20  5          1.5    170      0.7698413  0.8928571  0.26666667
  0.20  5          1.5    180      0.7698413  0.8761905  0.26666667
  0.20  5          1.5    190      0.7698413  0.8761905  0.23333333
  0.20  5          1.5    200      0.7698413  0.8928571  0.23333333
  0.20  6          0.5     10      0.6781746  0.9047619  0.20000000
  0.20  6          0.5     20      0.8051587  0.9190476  0.31666667
  0.20  6          0.5     30      0.8003968  0.9357143  0.28333333
  0.20  6          0.5     40      0.8083333  0.8904762  0.31666667
  0.20  6          0.5     50      0.8019841  0.9047619  0.31666667
  0.20  6          0.5     60      0.8051587  0.9071429  0.31666667
  0.20  6          0.5     70      0.8051587  0.9071429  0.31666667
  0.20  6          0.5     80      0.8051587  0.9214286  0.31666667
  0.20  6          0.5     90      0.8051587  0.9214286  0.31666667
  0.20  6          0.5    100      0.7980159  0.9214286  0.36666667
  0.20  6          0.5    110      0.7980159  0.9214286  0.36666667
  0.20  6          0.5    120      0.7980159  0.9214286  0.31666667
  0.20  6          0.5    130      0.8051587  0.9214286  0.36666667
  0.20  6          0.5    140      0.8107143  0.9214286  0.36666667
  0.20  6          0.5    150      0.8107143  0.9214286  0.36666667
  0.20  6          0.5    160      0.8107143  0.9214286  0.36666667
  0.20  6          0.5    170      0.8107143  0.9214286  0.36666667
  0.20  6          0.5    180      0.8107143  0.9214286  0.33333333
  0.20  6          0.5    190      0.8107143  0.9214286  0.36666667
  0.20  6          0.5    200      0.8107143  0.9214286  0.36666667
  0.20  6          1.0     10      0.7623016  0.9047619  0.36666667
  0.20  6          1.0     20      0.8027778  0.9357143  0.45000000
  0.20  6          1.0     30      0.7742063  0.9214286  0.36666667
  0.20  6          1.0     40      0.7805556  0.9214286  0.36666667
  0.20  6          1.0     50      0.7757937  0.9214286  0.36666667
  0.20  6          1.0     60      0.7757937  0.9214286  0.36666667
  0.20  6          1.0     70      0.7829365  0.9214286  0.36666667
  0.20  6          1.0     80      0.7829365  0.9214286  0.36666667
  0.20  6          1.0     90      0.7924603  0.9214286  0.36666667
  0.20  6          1.0    100      0.7924603  0.9214286  0.36666667
  0.20  6          1.0    110      0.7924603  0.9214286  0.36666667
  0.20  6          1.0    120      0.7924603  0.9214286  0.36666667
  0.20  6          1.0    130      0.7924603  0.9214286  0.36666667
  0.20  6          1.0    140      0.7924603  0.9214286  0.36666667
  0.20  6          1.0    150      0.7924603  0.9214286  0.36666667
  0.20  6          1.0    160      0.7924603  0.9214286  0.36666667
  0.20  6          1.0    170      0.7996032  0.9214286  0.36666667
  0.20  6          1.0    180      0.7996032  0.9214286  0.36666667
  0.20  6          1.0    190      0.7996032  0.9214286  0.36666667
  0.20  6          1.0    200      0.7996032  0.9214286  0.36666667
  0.20  6          1.5     10      0.5488095  0.8642857  0.11666667
  0.20  6          1.5     20      0.5932540  0.8904762  0.15000000
  0.20  6          1.5     30      0.6210317  0.9047619  0.13333333
  0.20  6          1.5     40      0.6531746  0.9214286  0.13333333
  0.20  6          1.5     50      0.6746032  0.9214286  0.13333333
  0.20  6          1.5     60      0.6746032  0.9071429  0.13333333
  0.20  6          1.5     70      0.6817460  0.9071429  0.16666667
  0.20  6          1.5     80      0.6888889  0.9071429  0.13333333
  0.20  6          1.5     90      0.6888889  0.8928571  0.16666667
  0.20  6          1.5    100      0.6888889  0.8928571  0.16666667
  0.20  6          1.5    110      0.6888889  0.8928571  0.11666667
  0.20  6          1.5    120      0.7019841  0.8928571  0.13333333
  0.20  6          1.5    130      0.6936508  0.8928571  0.16666667
  0.20  6          1.5    140      0.6853175  0.9071429  0.16666667
  0.20  6          1.5    150      0.6853175  0.9071429  0.13333333
  0.20  6          1.5    160      0.6805556  0.8928571  0.13333333
  0.20  6          1.5    170      0.6805556  0.8928571  0.13333333
  0.20  6          1.5    180      0.6805556  0.8928571  0.13333333
  0.20  6          1.5    190      0.6805556  0.8928571  0.13333333
  0.20  6          1.5    200      0.6805556  0.8928571  0.13333333
  0.20  7          0.5     10      0.6674603  0.8619048  0.08333333
  0.20  7          0.5     20      0.7464286  0.8904762  0.16666667
  0.20  7          0.5     30      0.7603175  0.8904762  0.13333333
  0.20  7          0.5     40      0.7698413  0.8904762  0.11666667
  0.20  7          0.5     50      0.7722222  0.8904762  0.11666667
  0.20  7          0.5     60      0.7793651  0.8904762  0.11666667
  0.20  7          0.5     70      0.7793651  0.8904762  0.11666667
  0.20  7          0.5     80      0.7793651  0.8904762  0.11666667
  0.20  7          0.5     90      0.7865079  0.8904762  0.11666667
  0.20  7          0.5    100      0.7865079  0.8904762  0.11666667
  0.20  7          0.5    110      0.7865079  0.8904762  0.11666667
  0.20  7          0.5    120      0.7865079  0.8904762  0.11666667
  0.20  7          0.5    130      0.7865079  0.8904762  0.11666667
  0.20  7          0.5    140      0.7865079  0.8904762  0.11666667
  0.20  7          0.5    150      0.7865079  0.8904762  0.11666667
  0.20  7          0.5    160      0.7865079  0.8904762  0.11666667
  0.20  7          0.5    170      0.7793651  0.8904762  0.11666667
  0.20  7          0.5    180      0.7793651  0.8904762  0.11666667
  0.20  7          0.5    190      0.7793651  0.8904762  0.11666667
  0.20  7          0.5    200      0.7793651  0.8904762  0.11666667
  0.20  7          1.0     10      0.6944444  0.9214286  0.16666667
  0.20  7          1.0     20      0.7158730  0.9214286  0.16666667
  0.20  7          1.0     30      0.7440476  0.9047619  0.13333333
  0.20  7          1.0     40      0.7440476  0.9047619  0.23333333
  0.20  7          1.0     50      0.7642857  0.9047619  0.23333333
  0.20  7          1.0     60      0.7607143  0.9047619  0.23333333
  0.20  7          1.0     70      0.7761905  0.9047619  0.23333333
  0.20  7          1.0     80      0.7726190  0.9047619  0.23333333
  0.20  7          1.0     90      0.7726190  0.9047619  0.23333333
  0.20  7          1.0    100      0.7797619  0.9047619  0.23333333
  0.20  7          1.0    110      0.7797619  0.8904762  0.23333333
  0.20  7          1.0    120      0.7797619  0.9047619  0.23333333
  0.20  7          1.0    130      0.7797619  0.9047619  0.23333333
  0.20  7          1.0    140      0.7797619  0.8904762  0.23333333
  0.20  7          1.0    150      0.7797619  0.8904762  0.23333333
  0.20  7          1.0    160      0.7797619  0.8904762  0.23333333
  0.20  7          1.0    170      0.7797619  0.8904762  0.23333333
  0.20  7          1.0    180      0.7797619  0.8904762  0.23333333
  0.20  7          1.0    190      0.7797619  0.8904762  0.23333333
  0.20  7          1.0    200      0.7797619  0.8904762  0.23333333
  0.20  7          1.5     10      0.6361111  0.9071429  0.20000000
  0.20  7          1.5     20      0.7000000  0.9238095  0.15000000
  0.20  7          1.5     30      0.6857143  0.9214286  0.20000000
  0.20  7          1.5     40      0.6829365  0.9071429  0.18333333
  0.20  7          1.5     50      0.6809524  0.8904762  0.18333333
  0.20  7          1.5     60      0.6654762  0.8904762  0.18333333
  0.20  7          1.5     70      0.6726190  0.8904762  0.18333333
  0.20  7          1.5     80      0.6726190  0.8904762  0.18333333
  0.20  7          1.5     90      0.6809524  0.8904762  0.18333333
  0.20  7          1.5    100      0.6809524  0.8904762  0.18333333
  0.20  7          1.5    110      0.6809524  0.8904762  0.18333333
  0.20  7          1.5    120      0.6726190  0.8904762  0.18333333
  0.20  7          1.5    130      0.6726190  0.9071429  0.18333333
  0.20  7          1.5    140      0.6726190  0.8904762  0.18333333
  0.20  7          1.5    150      0.6726190  0.8904762  0.18333333
  0.20  7          1.5    160      0.6726190  0.8904762  0.18333333
  0.20  7          1.5    170      0.6726190  0.8904762  0.18333333
  0.20  7          1.5    180      0.6726190  0.8904762  0.18333333
  0.20  7          1.5    190      0.6726190  0.8904762  0.18333333
  0.20  7          1.5    200      0.6726190  0.8904762  0.18333333
  0.35  3          0.5     10      0.7015873  0.8642857  0.25000000
  0.35  3          0.5     20      0.7047619  0.9047619  0.26666667
  0.35  3          0.5     30      0.7246032  0.9047619  0.21666667
  0.35  3          0.5     40      0.7555556  0.9047619  0.21666667
  0.35  3          0.5     50      0.7472222  0.9047619  0.26666667
  0.35  3          0.5     60      0.7519841  0.9214286  0.26666667
  0.35  3          0.5     70      0.7448413  0.9214286  0.26666667
  0.35  3          0.5     80      0.7448413  0.9047619  0.26666667
  0.35  3          0.5     90      0.7448413  0.9047619  0.26666667
  0.35  3          0.5    100      0.7448413  0.9047619  0.26666667
  0.35  3          0.5    110      0.7448413  0.9047619  0.26666667
  0.35  3          0.5    120      0.7448413  0.9047619  0.26666667
  0.35  3          0.5    130      0.7448413  0.9047619  0.26666667
  0.35  3          0.5    140      0.7448413  0.9047619  0.26666667
  0.35  3          0.5    150      0.7448413  0.9047619  0.26666667
  0.35  3          0.5    160      0.7531746  0.9047619  0.26666667
  0.35  3          0.5    170      0.7531746  0.9047619  0.26666667
  0.35  3          0.5    180      0.7531746  0.9047619  0.26666667
  0.35  3          0.5    190      0.7531746  0.9047619  0.26666667
  0.35  3          0.5    200      0.7531746  0.9047619  0.26666667
  0.35  3          1.0     10      0.6964286  0.8857143  0.11666667
  0.35  3          1.0     20      0.7079365  0.8904762  0.20000000
  0.35  3          1.0     30      0.7150794  0.8904762  0.20000000
  0.35  3          1.0     40      0.7222222  0.8904762  0.23333333
  0.35  3          1.0     50      0.7150794  0.8904762  0.23333333
  0.35  3          1.0     60      0.7150794  0.8904762  0.23333333
  0.35  3          1.0     70      0.7150794  0.8904762  0.23333333
  0.35  3          1.0     80      0.7365079  0.8904762  0.23333333
  0.35  3          1.0     90      0.7365079  0.8904762  0.23333333
  0.35  3          1.0    100      0.7365079  0.8904762  0.23333333
  0.35  3          1.0    110      0.7436508  0.8904762  0.26666667
  0.35  3          1.0    120      0.7436508  0.8904762  0.23333333
  0.35  3          1.0    130      0.7436508  0.8904762  0.23333333
  0.35  3          1.0    140      0.7436508  0.8904762  0.23333333
  0.35  3          1.0    150      0.7436508  0.8904762  0.23333333
  0.35  3          1.0    160      0.7436508  0.8904762  0.23333333
  0.35  3          1.0    170      0.7388889  0.8904762  0.23333333
  0.35  3          1.0    180      0.7388889  0.8904762  0.23333333
  0.35  3          1.0    190      0.7388889  0.8904762  0.23333333
  0.35  3          1.0    200      0.7388889  0.8904762  0.23333333
  0.35  3          1.5     10      0.6884921  0.8880952  0.16666667
  0.35  3          1.5     20      0.7571429  0.8595238  0.35000000
  0.35  3          1.5     30      0.7488095  0.8761905  0.28333333
  0.35  3          1.5     40      0.7488095  0.8761905  0.25000000
  0.35  3          1.5     50      0.7488095  0.8761905  0.28333333
  0.35  3          1.5     60      0.7416667  0.8761905  0.28333333
  0.35  3          1.5     70      0.7361111  0.8761905  0.25000000
  0.35  3          1.5     80      0.7361111  0.8761905  0.25000000
  0.35  3          1.5     90      0.7361111  0.8761905  0.25000000
  0.35  3          1.5    100      0.7361111  0.8904762  0.25000000
  0.35  3          1.5    110      0.7361111  0.8904762  0.25000000
  0.35  3          1.5    120      0.7361111  0.8904762  0.25000000
  0.35  3          1.5    130      0.7305556  0.8761905  0.25000000
  0.35  3          1.5    140      0.7234127  0.8904762  0.25000000
  0.35  3          1.5    150      0.7234127  0.8904762  0.28333333
  0.35  3          1.5    160      0.7234127  0.8904762  0.25000000
  0.35  3          1.5    170      0.7234127  0.8904762  0.25000000
  0.35  3          1.5    180      0.7234127  0.8904762  0.28333333
  0.35  3          1.5    190      0.7234127  0.8904762  0.25000000
  0.35  3          1.5    200      0.7234127  0.8904762  0.25000000
  0.35  4          0.5     10      0.7311508  0.8904762  0.26666667
  0.35  4          0.5     20      0.7805556  0.9214286  0.33333333
  0.35  4          0.5     30      0.7750000  0.9071429  0.26666667
  0.35  4          0.5     40      0.7750000  0.8904762  0.30000000
  0.35  4          0.5     50      0.7750000  0.8904762  0.30000000
  0.35  4          0.5     60      0.7750000  0.8761905  0.30000000
  0.35  4          0.5     70      0.7797619  0.8761905  0.30000000
  0.35  4          0.5     80      0.7797619  0.8761905  0.30000000
  0.35  4          0.5     90      0.7797619  0.9071429  0.30000000
  0.35  4          0.5    100      0.7797619  0.8761905  0.30000000
  0.35  4          0.5    110      0.7742063  0.8928571  0.30000000
  0.35  4          0.5    120      0.7742063  0.9071429  0.30000000
  0.35  4          0.5    130      0.7742063  0.9071429  0.30000000
  0.35  4          0.5    140      0.7742063  0.8761905  0.30000000
  0.35  4          0.5    150      0.7742063  0.8904762  0.33333333
  0.35  4          0.5    160      0.7742063  0.8904762  0.33333333
  0.35  4          0.5    170      0.7742063  0.8761905  0.33333333
  0.35  4          0.5    180      0.7742063  0.8904762  0.33333333
  0.35  4          0.5    190      0.7742063  0.8761905  0.33333333
  0.35  4          0.5    200      0.7742063  0.9071429  0.33333333
  0.35  4          1.0     10      0.6579365  0.8595238  0.28333333
  0.35  4          1.0     20      0.6623016  0.8595238  0.31666667
  0.35  4          1.0     30      0.6638889  0.8595238  0.23333333
  0.35  4          1.0     40      0.6686508  0.8595238  0.23333333
  0.35  4          1.0     50      0.6829365  0.8595238  0.23333333
  0.35  4          1.0     60      0.6734127  0.8595238  0.23333333
  0.35  4          1.0     70      0.6567460  0.8452381  0.28333333
  0.35  4          1.0     80      0.6567460  0.8309524  0.28333333
  0.35  4          1.0     90      0.6567460  0.8452381  0.28333333
  0.35  4          1.0    100      0.6567460  0.8452381  0.28333333
  0.35  4          1.0    110      0.6567460  0.8452381  0.28333333
  0.35  4          1.0    120      0.6567460  0.8452381  0.28333333
  0.35  4          1.0    130      0.6567460  0.8452381  0.28333333
  0.35  4          1.0    140      0.6567460  0.8309524  0.28333333
  0.35  4          1.0    150      0.6567460  0.8452381  0.28333333
  0.35  4          1.0    160      0.6567460  0.8309524  0.28333333
  0.35  4          1.0    170      0.6567460  0.8309524  0.28333333
  0.35  4          1.0    180      0.6567460  0.8452381  0.28333333
  0.35  4          1.0    190      0.6567460  0.8309524  0.28333333
  0.35  4          1.0    200      0.6567460  0.8309524  0.28333333
  0.35  4          1.5     10      0.7416667  0.9214286  0.28333333
  0.35  4          1.5     20      0.7349206  0.9047619  0.28333333
  0.35  4          1.5     30      0.7297619  0.9047619  0.31666667
  0.35  4          1.5     40      0.7297619  0.9047619  0.28333333
  0.35  4          1.5     50      0.7408730  0.9047619  0.31666667
  0.35  4          1.5     60      0.7456349  0.9047619  0.31666667
  0.35  4          1.5     70      0.7456349  0.9047619  0.31666667
  0.35  4          1.5     80      0.7384921  0.9047619  0.31666667
  0.35  4          1.5     90      0.7456349  0.9047619  0.31666667
  0.35  4          1.5    100      0.7456349  0.9047619  0.28333333
  0.35  4          1.5    110      0.7456349  0.9047619  0.31666667
  0.35  4          1.5    120      0.7456349  0.9047619  0.28333333
  0.35  4          1.5    130      0.7456349  0.9047619  0.28333333
  0.35  4          1.5    140      0.7456349  0.9047619  0.28333333
  0.35  4          1.5    150      0.7456349  0.9047619  0.31666667
  0.35  4          1.5    160      0.7456349  0.9047619  0.31666667
  0.35  4          1.5    170      0.7456349  0.9047619  0.31666667
  0.35  4          1.5    180      0.7456349  0.9047619  0.31666667
  0.35  4          1.5    190      0.7456349  0.9047619  0.31666667
  0.35  4          1.5    200      0.7456349  0.9047619  0.31666667
  0.35  5          0.5     10      0.7337302  0.9380952  0.43333333
  0.35  5          0.5     20      0.7666667  0.8928571  0.38333333
  0.35  5          0.5     30      0.7857143  0.8928571  0.35000000
  0.35  5          0.5     40      0.7821429  0.8928571  0.38333333
  0.35  5          0.5     50      0.7821429  0.8928571  0.38333333
  0.35  5          0.5     60      0.7773810  0.8928571  0.38333333
  0.35  5          0.5     70      0.7773810  0.8928571  0.38333333
  0.35  5          0.5     80      0.7773810  0.8761905  0.38333333
  0.35  5          0.5     90      0.7773810  0.8928571  0.38333333
  0.35  5          0.5    100      0.7773810  0.8928571  0.38333333
  0.35  5          0.5    110      0.7773810  0.8928571  0.38333333
  0.35  5          0.5    120      0.7773810  0.8928571  0.38333333
  0.35  5          0.5    130      0.7773810  0.8928571  0.38333333
  0.35  5          0.5    140      0.7773810  0.8761905  0.38333333
  0.35  5          0.5    150      0.7773810  0.8928571  0.38333333
  0.35  5          0.5    160      0.7773810  0.8928571  0.38333333
  0.35  5          0.5    170      0.7773810  0.8928571  0.38333333
  0.35  5          0.5    180      0.7773810  0.8928571  0.38333333
  0.35  5          0.5    190      0.7773810  0.8928571  0.38333333
  0.35  5          0.5    200      0.7773810  0.8928571  0.38333333
  0.35  5          1.0     10      0.7634921  0.8761905  0.28333333
  0.35  5          1.0     20      0.7420635  0.8928571  0.28333333
  0.35  5          1.0     30      0.7436508  0.8904762  0.25000000
  0.35  5          1.0     40      0.7444444  0.8904762  0.31666667
  0.35  5          1.0     50      0.7527778  0.8904762  0.28333333
  0.35  5          1.0     60      0.7599206  0.8904762  0.28333333
  0.35  5          1.0     70      0.7599206  0.8904762  0.31666667
  0.35  5          1.0     80      0.7599206  0.8904762  0.31666667
  0.35  5          1.0     90      0.7599206  0.8904762  0.31666667
  0.35  5          1.0    100      0.7599206  0.8904762  0.31666667
  0.35  5          1.0    110      0.7599206  0.8904762  0.31666667
  0.35  5          1.0    120      0.7599206  0.8904762  0.28333333
  0.35  5          1.0    130      0.7599206  0.8904762  0.31666667
  0.35  5          1.0    140      0.7599206  0.8904762  0.28333333
  0.35  5          1.0    150      0.7599206  0.8904762  0.28333333
  0.35  5          1.0    160      0.7599206  0.8904762  0.31666667
  0.35  5          1.0    170      0.7599206  0.8904762  0.28333333
  0.35  5          1.0    180      0.7599206  0.8904762  0.28333333
  0.35  5          1.0    190      0.7599206  0.8904762  0.31666667
  0.35  5          1.0    200      0.7599206  0.8904762  0.28333333
  0.35  5          1.5     10      0.6714286  0.8928571  0.23333333
  0.35  5          1.5     20      0.6956349  0.8738095  0.23333333
  0.35  5          1.5     30      0.7170635  0.8738095  0.28333333
  0.35  5          1.5     40      0.7242063  0.8904762  0.28333333
  0.35  5          1.5     50      0.7289683  0.8738095  0.28333333
  0.35  5          1.5     60      0.7337302  0.8904762  0.28333333
  0.35  5          1.5     70      0.7337302  0.8904762  0.28333333
  0.35  5          1.5     80      0.7408730  0.8738095  0.28333333
  0.35  5          1.5     90      0.7408730  0.8738095  0.18333333
  0.35  5          1.5    100      0.7408730  0.8904762  0.23333333
  0.35  5          1.5    110      0.7408730  0.8904762  0.23333333
  0.35  5          1.5    120      0.7408730  0.8738095  0.23333333
  0.35  5          1.5    130      0.7408730  0.8904762  0.23333333
  0.35  5          1.5    140      0.7361111  0.8738095  0.23333333
  0.35  5          1.5    150      0.7361111  0.8738095  0.18333333
  0.35  5          1.5    160      0.7361111  0.8738095  0.23333333
  0.35  5          1.5    170      0.7361111  0.8738095  0.18333333
  0.35  5          1.5    180      0.7361111  0.8738095  0.23333333
  0.35  5          1.5    190      0.7361111  0.8738095  0.23333333
  0.35  5          1.5    200      0.7361111  0.8904762  0.23333333
  0.35  6          0.5     10      0.7365079  0.8619048  0.28333333
  0.35  6          0.5     20      0.7591270  0.8761905  0.36666667
  0.35  6          0.5     30      0.7738095  0.8928571  0.35000000
  0.35  6          0.5     40      0.7738095  0.8928571  0.35000000
  0.35  6          0.5     50      0.7714286  0.8761905  0.38333333
  0.35  6          0.5     60      0.7714286  0.8761905  0.38333333
  0.35  6          0.5     70      0.7714286  0.8928571  0.38333333
  0.35  6          0.5     80      0.7714286  0.8761905  0.38333333
  0.35  6          0.5     90      0.7714286  0.8928571  0.38333333
  0.35  6          0.5    100      0.7714286  0.8928571  0.38333333
  0.35  6          0.5    110      0.7714286  0.8928571  0.38333333
  0.35  6          0.5    120      0.7714286  0.8928571  0.38333333
  0.35  6          0.5    130      0.7714286  0.8761905  0.38333333
  0.35  6          0.5    140      0.7714286  0.8928571  0.38333333
  0.35  6          0.5    150      0.7714286  0.8928571  0.38333333
  0.35  6          0.5    160      0.7714286  0.8928571  0.38333333
  0.35  6          0.5    170      0.7714286  0.8761905  0.38333333
  0.35  6          0.5    180      0.7769841  0.8761905  0.38333333
  0.35  6          0.5    190      0.7769841  0.8928571  0.38333333
  0.35  6          0.5    200      0.7769841  0.8928571  0.38333333
  0.35  6          1.0     10      0.6269841  0.8761905  0.30000000
  0.35  6          1.0     20      0.7051587  0.8928571  0.36666667
  0.35  6          1.0     30      0.7000000  0.8785714  0.36666667
  0.35  6          1.0     40      0.7281746  0.8785714  0.36666667
  0.35  6          1.0     50      0.7329365  0.8785714  0.36666667
  0.35  6          1.0     60      0.7329365  0.8952381  0.36666667
  0.35  6          1.0     70      0.7329365  0.8785714  0.36666667
  0.35  6          1.0     80      0.7329365  0.8952381  0.36666667
  0.35  6          1.0     90      0.7376984  0.9095238  0.36666667
  0.35  6          1.0    100      0.7293651  0.8952381  0.36666667
  0.35  6          1.0    110      0.7293651  0.8952381  0.36666667
  0.35  6          1.0    120      0.7293651  0.8785714  0.36666667
  0.35  6          1.0    130      0.7293651  0.8952381  0.36666667
  0.35  6          1.0    140      0.7293651  0.8952381  0.36666667
  0.35  6          1.0    150      0.7293651  0.8952381  0.36666667
  0.35  6          1.0    160      0.7222222  0.8952381  0.36666667
  0.35  6          1.0    170      0.7222222  0.8952381  0.36666667
  0.35  6          1.0    180      0.7222222  0.8785714  0.36666667
  0.35  6          1.0    190      0.7222222  0.8952381  0.36666667
  0.35  6          1.0    200      0.7293651  0.8785714  0.36666667
  0.35  6          1.5     10      0.6936508  0.8761905  0.28333333
  0.35  6          1.5     20      0.7075397  0.8619048  0.26666667
  0.35  6          1.5     30      0.7019841  0.8476190  0.23333333
  0.35  6          1.5     40      0.7091270  0.8476190  0.26666667
  0.35  6          1.5     50      0.7091270  0.8476190  0.26666667
  0.35  6          1.5     60      0.7138889  0.8619048  0.26666667
  0.35  6          1.5     70      0.7138889  0.8476190  0.26666667
  0.35  6          1.5     80      0.7055556  0.8619048  0.26666667
  0.35  6          1.5     90      0.7055556  0.8619048  0.23333333
  0.35  6          1.5    100      0.7055556  0.8785714  0.26666667
  0.35  6          1.5    110      0.7055556  0.8619048  0.26666667
  0.35  6          1.5    120      0.7055556  0.8619048  0.26666667
  0.35  6          1.5    130      0.7055556  0.8785714  0.26666667
  0.35  6          1.5    140      0.7055556  0.8785714  0.26666667
  0.35  6          1.5    150      0.7055556  0.8619048  0.26666667
  0.35  6          1.5    160      0.7055556  0.8619048  0.26666667
  0.35  6          1.5    170      0.7055556  0.8785714  0.26666667
  0.35  6          1.5    180      0.7055556  0.8619048  0.23333333
  0.35  6          1.5    190      0.7055556  0.8619048  0.23333333
  0.35  6          1.5    200      0.7055556  0.8785714  0.23333333
  0.35  7          0.5     10      0.6559524  0.8785714  0.28333333
  0.35  7          0.5     20      0.7091270  0.8785714  0.30000000
  0.35  7          0.5     30      0.7130952  0.8952381  0.28333333
  0.35  7          0.5     40      0.6738095  0.8952381  0.28333333
  0.35  7          0.5     50      0.6809524  0.8952381  0.28333333
  0.35  7          0.5     60      0.6738095  0.8952381  0.23333333
  0.35  7          0.5     70      0.6821429  0.8952381  0.20000000
  0.35  7          0.5     80      0.6876984  0.8952381  0.20000000
  0.35  7          0.5     90      0.6876984  0.8952381  0.20000000
  0.35  7          0.5    100      0.6876984  0.8952381  0.20000000
  0.35  7          0.5    110      0.6876984  0.8952381  0.20000000
  0.35  7          0.5    120      0.6876984  0.8952381  0.20000000
  0.35  7          0.5    130      0.6960317  0.8952381  0.20000000
  0.35  7          0.5    140      0.6960317  0.8952381  0.20000000
  0.35  7          0.5    150      0.6960317  0.8952381  0.20000000
  0.35  7          0.5    160      0.6960317  0.8952381  0.20000000
  0.35  7          0.5    170      0.6960317  0.8952381  0.20000000
  0.35  7          0.5    180      0.6960317  0.8952381  0.20000000
  0.35  7          0.5    190      0.6960317  0.8952381  0.25000000
  0.35  7          0.5    200      0.6960317  0.8952381  0.23333333
  0.35  7          1.0     10      0.6369048  0.9238095  0.28333333
  0.35  7          1.0     20      0.6829365  0.9071429  0.33333333
  0.35  7          1.0     30      0.6976190  0.9238095  0.33333333
  0.35  7          1.0     40      0.6892857  0.9238095  0.33333333
  0.35  7          1.0     50      0.7130952  0.9095238  0.33333333
  0.35  7          1.0     60      0.7130952  0.9071429  0.33333333
  0.35  7          1.0     70      0.7130952  0.9071429  0.33333333
  0.35  7          1.0     80      0.7130952  0.9071429  0.33333333
  0.35  7          1.0     90      0.7178571  0.9071429  0.33333333
  0.35  7          1.0    100      0.7178571  0.9071429  0.33333333
  0.35  7          1.0    110      0.7178571  0.9071429  0.33333333
  0.35  7          1.0    120      0.7178571  0.9071429  0.33333333
  0.35  7          1.0    130      0.7226190  0.9071429  0.33333333
  0.35  7          1.0    140      0.7226190  0.9071429  0.33333333
  0.35  7          1.0    150      0.7226190  0.9071429  0.33333333
  0.35  7          1.0    160      0.7226190  0.9071429  0.33333333
  0.35  7          1.0    170      0.7226190  0.9071429  0.33333333
  0.35  7          1.0    180      0.7226190  0.9071429  0.33333333
  0.35  7          1.0    190      0.7226190  0.9071429  0.33333333
  0.35  7          1.0    200      0.7226190  0.9071429  0.33333333
  0.35  7          1.5     10      0.7091270  0.8904762  0.20000000
  0.35  7          1.5     20      0.7095238  0.8595238  0.15000000
  0.35  7          1.5     30      0.6904762  0.8595238  0.16666667
  0.35  7          1.5     40      0.7071429  0.8595238  0.11666667
  0.35  7          1.5     50      0.7214286  0.8595238  0.11666667
  0.35  7          1.5     60      0.7222222  0.8595238  0.10000000
  0.35  7          1.5     70      0.7222222  0.8595238  0.15000000
  0.35  7          1.5     80      0.7222222  0.8595238  0.10000000
  0.35  7          1.5     90      0.7222222  0.8595238  0.15000000
  0.35  7          1.5    100      0.7222222  0.8595238  0.10000000
  0.35  7          1.5    110      0.7138889  0.8595238  0.15000000
  0.35  7          1.5    120      0.7138889  0.8595238  0.15000000
  0.35  7          1.5    130      0.7222222  0.8595238  0.20000000
  0.35  7          1.5    140      0.7222222  0.8595238  0.20000000
  0.35  7          1.5    150      0.7222222  0.8595238  0.23333333
  0.35  7          1.5    160      0.7222222  0.8595238  0.11666667
  0.35  7          1.5    170      0.7269841  0.8595238  0.15000000
  0.35  7          1.5    180      0.7269841  0.8595238  0.20000000
  0.35  7          1.5    190      0.7269841  0.8738095  0.15000000
  0.35  7          1.5    200      0.7269841  0.8738095  0.15000000
  0.40  3          0.5     10      0.7464286  0.8904762  0.23333333
  0.40  3          0.5     20      0.7309524  0.8904762  0.31666667
  0.40  3          0.5     30      0.7452381  0.8761905  0.26666667
  0.40  3          0.5     40      0.7630952  0.8761905  0.31666667
  0.40  3          0.5     50      0.7575397  0.8761905  0.31666667
  0.40  3          0.5     60      0.7646825  0.8761905  0.31666667
  0.40  3          0.5     70      0.7694444  0.8761905  0.31666667
  0.40  3          0.5     80      0.7694444  0.8761905  0.31666667
  0.40  3          0.5     90      0.7694444  0.8761905  0.31666667
  0.40  3          0.5    100      0.7694444  0.8761905  0.31666667
  0.40  3          0.5    110      0.7694444  0.8761905  0.31666667
  0.40  3          0.5    120      0.7694444  0.8761905  0.31666667
  0.40  3          0.5    130      0.7694444  0.8761905  0.31666667
  0.40  3          0.5    140      0.7694444  0.8761905  0.31666667
  0.40  3          0.5    150      0.7694444  0.8761905  0.31666667
  0.40  3          0.5    160      0.7694444  0.8761905  0.31666667
  0.40  3          0.5    170      0.7694444  0.8761905  0.31666667
  0.40  3          0.5    180      0.7694444  0.8761905  0.31666667
  0.40  3          0.5    190      0.7694444  0.8761905  0.31666667
  0.40  3          0.5    200      0.7694444  0.8761905  0.31666667
  0.40  3          1.0     10      0.7210317  0.8571429  0.26666667
  0.40  3          1.0     20      0.7817460  0.8738095  0.33333333
  0.40  3          1.0     30      0.7833333  0.8738095  0.31666667
  0.40  3          1.0     40      0.7750000  0.8738095  0.28333333
  0.40  3          1.0     50      0.7678571  0.8738095  0.28333333
  0.40  3          1.0     60      0.7630952  0.8738095  0.28333333
  0.40  3          1.0     70      0.7630952  0.8738095  0.28333333
  0.40  3          1.0     80      0.7630952  0.8738095  0.33333333
  0.40  3          1.0     90      0.7630952  0.8738095  0.28333333
  0.40  3          1.0    100      0.7630952  0.8738095  0.33333333
  0.40  3          1.0    110      0.7702381  0.8738095  0.28333333
  0.40  3          1.0    120      0.7702381  0.8738095  0.33333333
  0.40  3          1.0    130      0.7702381  0.8738095  0.33333333
  0.40  3          1.0    140      0.7702381  0.8738095  0.33333333
  0.40  3          1.0    150      0.7702381  0.8738095  0.28333333
  0.40  3          1.0    160      0.7702381  0.8738095  0.31666667
  0.40  3          1.0    170      0.7785714  0.8738095  0.36666667
  0.40  3          1.0    180      0.7785714  0.8738095  0.36666667
  0.40  3          1.0    190      0.7785714  0.8738095  0.36666667
  0.40  3          1.0    200      0.7785714  0.8738095  0.36666667
  0.40  3          1.5     10      0.7361111  0.9404762  0.28333333
  0.40  3          1.5     20      0.7611111  0.9071429  0.23333333
  0.40  3          1.5     30      0.7563492  0.8904762  0.23333333
  0.40  3          1.5     40      0.7563492  0.9071429  0.25000000
  0.40  3          1.5     50      0.7563492  0.9071429  0.25000000
  0.40  3          1.5     60      0.7563492  0.9071429  0.25000000
  0.40  3          1.5     70      0.7515873  0.9071429  0.25000000
  0.40  3          1.5     80      0.7515873  0.9071429  0.25000000
  0.40  3          1.5     90      0.7515873  0.9238095  0.25000000
  0.40  3          1.5    100      0.7444444  0.9071429  0.25000000
  0.40  3          1.5    110      0.7444444  0.9071429  0.25000000
  0.40  3          1.5    120      0.7444444  0.9071429  0.28333333
  0.40  3          1.5    130      0.7444444  0.9071429  0.28333333
  0.40  3          1.5    140      0.7444444  0.9238095  0.28333333
  0.40  3          1.5    150      0.7444444  0.9238095  0.28333333
  0.40  3          1.5    160      0.7444444  0.9238095  0.28333333
  0.40  3          1.5    170      0.7444444  0.9238095  0.28333333
  0.40  3          1.5    180      0.7444444  0.9238095  0.28333333
  0.40  3          1.5    190      0.7444444  0.9238095  0.28333333
  0.40  3          1.5    200      0.7444444  0.9238095  0.28333333
  0.40  4          0.5     10      0.7507937  0.9404762  0.38333333
  0.40  4          0.5     20      0.8265873  0.8761905  0.41666667
  0.40  4          0.5     30      0.8289683  0.8761905  0.41666667
  0.40  4          0.5     40      0.8206349  0.8761905  0.41666667
  0.40  4          0.5     50      0.8194444  0.8761905  0.41666667
  0.40  4          0.5     60      0.8194444  0.8761905  0.41666667
  0.40  4          0.5     70      0.8123016  0.8761905  0.41666667
  0.40  4          0.5     80      0.8123016  0.8761905  0.41666667
  0.40  4          0.5     90      0.8123016  0.8761905  0.41666667
  0.40  4          0.5    100      0.8170635  0.8761905  0.41666667
  0.40  4          0.5    110      0.8242063  0.8761905  0.41666667
  0.40  4          0.5    120      0.8242063  0.8761905  0.41666667
  0.40  4          0.5    130      0.8242063  0.8761905  0.41666667
  0.40  4          0.5    140      0.8242063  0.8761905  0.41666667
  0.40  4          0.5    150      0.8242063  0.8761905  0.41666667
  0.40  4          0.5    160      0.8242063  0.8761905  0.41666667
  0.40  4          0.5    170      0.8242063  0.8761905  0.41666667
  0.40  4          0.5    180      0.8242063  0.8761905  0.41666667
  0.40  4          0.5    190      0.8242063  0.8761905  0.41666667
  0.40  4          0.5    200      0.8242063  0.8761905  0.41666667
  0.40  4          1.0     10      0.6503968  0.8619048  0.15000000
  0.40  4          1.0     20      0.6650794  0.8595238  0.15000000
  0.40  4          1.0     30      0.6797619  0.8761905  0.15000000
  0.40  4          1.0     40      0.6797619  0.8476190  0.15000000
  0.40  4          1.0     50      0.7039683  0.8476190  0.20000000
  0.40  4          1.0     60      0.7039683  0.8476190  0.20000000
  0.40  4          1.0     70      0.7039683  0.8476190  0.20000000
  0.40  4          1.0     80      0.7039683  0.8619048  0.20000000
  0.40  4          1.0     90      0.7123016  0.8476190  0.20000000
  0.40  4          1.0    100      0.7123016  0.8476190  0.20000000
  0.40  4          1.0    110      0.7123016  0.8476190  0.20000000
  0.40  4          1.0    120      0.7123016  0.8476190  0.20000000
  0.40  4          1.0    130      0.7123016  0.8476190  0.20000000
  0.40  4          1.0    140      0.7123016  0.8619048  0.20000000
  0.40  4          1.0    150      0.7123016  0.8619048  0.20000000
  0.40  4          1.0    160      0.7123016  0.8761905  0.20000000
  0.40  4          1.0    170      0.7123016  0.8619048  0.23333333
  0.40  4          1.0    180      0.7123016  0.8619048  0.23333333
  0.40  4          1.0    190      0.7123016  0.8761905  0.23333333
  0.40  4          1.0    200      0.7123016  0.8619048  0.23333333
  0.40  4          1.5     10      0.6642857  0.9047619  0.23333333
  0.40  4          1.5     20      0.7492063  0.8904762  0.26666667
  0.40  4          1.5     30      0.7373016  0.8904762  0.23333333
  0.40  4          1.5     40      0.7444444  0.9047619  0.23333333
  0.40  4          1.5     50      0.7444444  0.8904762  0.23333333
  0.40  4          1.5     60      0.7500000  0.8904762  0.23333333
  0.40  4          1.5     70      0.7500000  0.8904762  0.23333333
  0.40  4          1.5     80      0.7452381  0.8904762  0.23333333
  0.40  4          1.5     90      0.7452381  0.8904762  0.26666667
  0.40  4          1.5    100      0.7452381  0.8904762  0.23333333
  0.40  4          1.5    110      0.7238095  0.8904762  0.26666667
  0.40  4          1.5    120      0.7238095  0.8904762  0.26666667
  0.40  4          1.5    130      0.7238095  0.8904762  0.26666667
  0.40  4          1.5    140      0.7285714  0.8904762  0.23333333
  0.40  4          1.5    150      0.7285714  0.8904762  0.23333333
  0.40  4          1.5    160      0.7285714  0.8904762  0.23333333
  0.40  4          1.5    170      0.7285714  0.8904762  0.26666667
  0.40  4          1.5    180      0.7285714  0.8904762  0.23333333
  0.40  4          1.5    190      0.7285714  0.8904762  0.23333333
  0.40  4          1.5    200      0.7285714  0.8904762  0.23333333
  0.40  5          0.5     10      0.7873016  0.8738095  0.36666667
  0.40  5          0.5     20      0.7638889  0.8904762  0.25000000
  0.40  5          0.5     30      0.7702381  0.9071429  0.30000000
  0.40  5          0.5     40      0.7750000  0.9071429  0.30000000
  0.40  5          0.5     50      0.7750000  0.8928571  0.33333333
  0.40  5          0.5     60      0.7750000  0.8928571  0.30000000
  0.40  5          0.5     70      0.7750000  0.8761905  0.33333333
  0.40  5          0.5     80      0.7750000  0.8761905  0.30000000
  0.40  5          0.5     90      0.7750000  0.8928571  0.33333333
  0.40  5          0.5    100      0.7750000  0.8928571  0.30000000
  0.40  5          0.5    110      0.7750000  0.8928571  0.30000000
  0.40  5          0.5    120      0.7750000  0.9071429  0.33333333
  0.40  5          0.5    130      0.7750000  0.8928571  0.30000000
  0.40  5          0.5    140      0.7750000  0.8928571  0.33333333
  0.40  5          0.5    150      0.7750000  0.8761905  0.30000000
  0.40  5          0.5    160      0.7750000  0.8928571  0.30000000
  0.40  5          0.5    170      0.7750000  0.8761905  0.30000000
  0.40  5          0.5    180      0.7750000  0.8928571  0.33333333
  0.40  5          0.5    190      0.7750000  0.8928571  0.33333333
  0.40  5          0.5    200      0.7750000  0.9071429  0.33333333
  0.40  5          1.0     10      0.7170635  0.9404762  0.28333333
  0.40  5          1.0     20      0.7269841  0.8952381  0.28333333
  0.40  5          1.0     30      0.7269841  0.8595238  0.28333333
  0.40  5          1.0     40      0.7380952  0.8452381  0.28333333
  0.40  5          1.0     50      0.7297619  0.8619048  0.28333333
  0.40  5          1.0     60      0.7380952  0.8619048  0.28333333
  0.40  5          1.0     70      0.7380952  0.8619048  0.33333333
  0.40  5          1.0     80      0.7380952  0.8619048  0.28333333
  0.40  5          1.0     90      0.7380952  0.8619048  0.33333333
  0.40  5          1.0    100      0.7380952  0.8619048  0.33333333
  0.40  5          1.0    110      0.7380952  0.8619048  0.33333333
  0.40  5          1.0    120      0.7380952  0.8619048  0.33333333
  0.40  5          1.0    130      0.7380952  0.8619048  0.33333333
  0.40  5          1.0    140      0.7380952  0.8452381  0.33333333
  0.40  5          1.0    150      0.7452381  0.8452381  0.33333333
  0.40  5          1.0    160      0.7452381  0.8452381  0.33333333
  0.40  5          1.0    170      0.7452381  0.8452381  0.33333333
  0.40  5          1.0    180      0.7452381  0.8452381  0.33333333
  0.40  5          1.0    190      0.7452381  0.8452381  0.33333333
  0.40  5          1.0    200      0.7452381  0.8452381  0.28333333
  0.40  5          1.5     10      0.6658730  0.8476190  0.11666667
  0.40  5          1.5     20      0.7277778  0.8928571  0.20000000
  0.40  5          1.5     30      0.7277778  0.9095238  0.23333333
  0.40  5          1.5     40      0.7253968  0.9095238  0.23333333
  0.40  5          1.5     50      0.7253968  0.9095238  0.20000000
  0.40  5          1.5     60      0.7206349  0.9238095  0.16666667
  0.40  5          1.5     70      0.7206349  0.9071429  0.16666667
  0.40  5          1.5     80      0.7253968  0.9071429  0.20000000
  0.40  5          1.5     90      0.7253968  0.9071429  0.20000000
  0.40  5          1.5    100      0.7253968  0.9071429  0.20000000
  0.40  5          1.5    110      0.7253968  0.9071429  0.20000000
  0.40  5          1.5    120      0.7253968  0.9071429  0.20000000
  0.40  5          1.5    130      0.7253968  0.9071429  0.20000000
  0.40  5          1.5    140      0.7253968  0.9071429  0.20000000
  0.40  5          1.5    150      0.7253968  0.9071429  0.20000000
  0.40  5          1.5    160      0.7253968  0.9071429  0.20000000
  0.40  5          1.5    170      0.7253968  0.9071429  0.20000000
  0.40  5          1.5    180      0.7253968  0.9071429  0.23333333
  0.40  5          1.5    190      0.7253968  0.9071429  0.23333333
  0.40  5          1.5    200      0.7253968  0.9071429  0.23333333
  0.40  6          0.5     10      0.6769841  0.9238095  0.20000000
  0.40  6          0.5     20      0.7349206  0.9238095  0.20000000
  0.40  6          0.5     30      0.7599206  0.9071429  0.18333333
  0.40  6          0.5     40      0.7444444  0.9071429  0.13333333
  0.40  6          0.5     50      0.7444444  0.9071429  0.13333333
  0.40  6          0.5     60      0.7444444  0.9071429  0.13333333
  0.40  6          0.5     70      0.7611111  0.9071429  0.18333333
  0.40  6          0.5     80      0.7611111  0.9238095  0.18333333
  0.40  6          0.5     90      0.7611111  0.9238095  0.23333333
  0.40  6          0.5    100      0.7539683  0.9071429  0.18333333
  0.40  6          0.5    110      0.7539683  0.9071429  0.18333333
  0.40  6          0.5    120      0.7539683  0.9071429  0.18333333
  0.40  6          0.5    130      0.7456349  0.9238095  0.18333333
  0.40  6          0.5    140      0.7456349  0.9238095  0.13333333
  0.40  6          0.5    150      0.7456349  0.9238095  0.18333333
  0.40  6          0.5    160      0.7456349  0.9071429  0.18333333
  0.40  6          0.5    170      0.7456349  0.9071429  0.18333333
  0.40  6          0.5    180      0.7456349  0.9238095  0.18333333
  0.40  6          0.5    190      0.7456349  0.9238095  0.18333333
  0.40  6          0.5    200      0.7456349  0.9071429  0.18333333
  0.40  6          1.0     10      0.7132937  0.8904762  0.30000000
  0.40  6          1.0     20      0.7341270  0.8904762  0.20000000
  0.40  6          1.0     30      0.7202381  0.8904762  0.20000000
  0.40  6          1.0     40      0.7202381  0.9071429  0.20000000
  0.40  6          1.0     50      0.7321429  0.9071429  0.20000000
  0.40  6          1.0     60      0.7392857  0.9071429  0.20000000
  0.40  6          1.0     70      0.7464286  0.9071429  0.25000000
  0.40  6          1.0     80      0.7464286  0.9071429  0.25000000
  0.40  6          1.0     90      0.7464286  0.9071429  0.20000000
  0.40  6          1.0    100      0.7464286  0.9071429  0.25000000
  0.40  6          1.0    110      0.7464286  0.9071429  0.25000000
  0.40  6          1.0    120      0.7416667  0.9071429  0.25000000
  0.40  6          1.0    130      0.7416667  0.9071429  0.25000000
  0.40  6          1.0    140      0.7416667  0.9071429  0.25000000
  0.40  6          1.0    150      0.7416667  0.9071429  0.25000000
  0.40  6          1.0    160      0.7416667  0.9071429  0.25000000
  0.40  6          1.0    170      0.7416667  0.9071429  0.25000000
  0.40  6          1.0    180      0.7416667  0.9071429  0.25000000
  0.40  6          1.0    190      0.7416667  0.9071429  0.25000000
  0.40  6          1.0    200      0.7416667  0.9071429  0.25000000
  0.40  6          1.5     10      0.7130952  0.8738095  0.28333333
  0.40  6          1.5     20      0.7194444  0.8309524  0.35000000
  0.40  6          1.5     30      0.7218254  0.8142857  0.31666667
  0.40  6          1.5     40      0.7273810  0.8309524  0.28333333
  0.40  6          1.5     50      0.7416667  0.8309524  0.28333333
  0.40  6          1.5     60      0.7416667  0.8619048  0.28333333
  0.40  6          1.5     70      0.7416667  0.8309524  0.28333333
  0.40  6          1.5     80      0.7416667  0.8619048  0.28333333
  0.40  6          1.5     90      0.7416667  0.8452381  0.28333333
  0.40  6          1.5    100      0.7416667  0.8619048  0.35000000
  0.40  6          1.5    110      0.7416667  0.8619048  0.28333333
  0.40  6          1.5    120      0.7416667  0.8619048  0.31666667
  0.40  6          1.5    130      0.7416667  0.8619048  0.31666667
  0.40  6          1.5    140      0.7416667  0.8619048  0.31666667
  0.40  6          1.5    150      0.7416667  0.8619048  0.35000000
  0.40  6          1.5    160      0.7416667  0.8476190  0.28333333
  0.40  6          1.5    170      0.7416667  0.8476190  0.31666667
  0.40  6          1.5    180      0.7416667  0.8476190  0.31666667
  0.40  6          1.5    190      0.7416667  0.8476190  0.35000000
  0.40  6          1.5    200      0.7416667  0.8476190  0.31666667
  0.40  7          0.5     10      0.7472222  0.8595238  0.35000000
  0.40  7          0.5     20      0.7710317  0.8595238  0.35000000
  0.40  7          0.5     30      0.7781746  0.8595238  0.30000000
  0.40  7          0.5     40      0.7698413  0.8595238  0.40000000
  0.40  7          0.5     50      0.7698413  0.8452381  0.35000000
  0.40  7          0.5     60      0.7698413  0.8595238  0.35000000
  0.40  7          0.5     70      0.7698413  0.8595238  0.40000000
  0.40  7          0.5     80      0.7698413  0.8595238  0.40000000
  0.40  7          0.5     90      0.7698413  0.8595238  0.40000000
  0.40  7          0.5    100      0.7746032  0.8595238  0.38333333
  0.40  7          0.5    110      0.7746032  0.8595238  0.43333333
  0.40  7          0.5    120      0.7746032  0.8595238  0.40000000
  0.40  7          0.5    130      0.7746032  0.8595238  0.35000000
  0.40  7          0.5    140      0.7746032  0.8595238  0.35000000
  0.40  7          0.5    150      0.7746032  0.8595238  0.40000000
  0.40  7          0.5    160      0.7746032  0.8595238  0.43333333
  0.40  7          0.5    170      0.7746032  0.8595238  0.35000000
  0.40  7          0.5    180      0.7746032  0.8738095  0.43333333
  0.40  7          0.5    190      0.7746032  0.8595238  0.40000000
  0.40  7          0.5    200      0.7746032  0.8595238  0.40000000
  0.40  7          1.0     10      0.6952381  0.8928571  0.33333333
  0.40  7          1.0     20      0.7380952  0.8928571  0.28333333
  0.40  7          1.0     30      0.7238095  0.8928571  0.28333333
  0.40  7          1.0     40      0.7238095  0.8928571  0.28333333
  0.40  7          1.0     50      0.7238095  0.8928571  0.28333333
  0.40  7          1.0     60      0.7142857  0.8928571  0.28333333
  0.40  7          1.0     70      0.7142857  0.8928571  0.28333333
  0.40  7          1.0     80      0.7142857  0.8928571  0.28333333
  0.40  7          1.0     90      0.7142857  0.8928571  0.28333333
  0.40  7          1.0    100      0.7142857  0.8928571  0.28333333
  0.40  7          1.0    110      0.7142857  0.8928571  0.28333333
  0.40  7          1.0    120      0.7142857  0.8785714  0.28333333
  0.40  7          1.0    130      0.7095238  0.8785714  0.28333333
  0.40  7          1.0    140      0.7095238  0.8785714  0.28333333
  0.40  7          1.0    150      0.7095238  0.8785714  0.28333333
  0.40  7          1.0    160      0.7095238  0.8785714  0.28333333
  0.40  7          1.0    170      0.7095238  0.8785714  0.28333333
  0.40  7          1.0    180      0.7095238  0.8785714  0.28333333
  0.40  7          1.0    190      0.7095238  0.8785714  0.28333333
  0.40  7          1.0    200      0.7095238  0.8785714  0.28333333
  0.40  7          1.5     10      0.6698413  0.8761905  0.25000000
  0.40  7          1.5     20      0.6924603  0.8619048  0.30000000
  0.40  7          1.5     30      0.6888889  0.8928571  0.31666667
  0.40  7          1.5     40      0.7115079  0.8785714  0.26666667
  0.40  7          1.5     50      0.7353175  0.8928571  0.26666667
  0.40  7          1.5     60      0.7353175  0.8761905  0.30000000
  0.40  7          1.5     70      0.7353175  0.8619048  0.30000000
  0.40  7          1.5     80      0.7353175  0.8619048  0.30000000
  0.40  7          1.5     90      0.7353175  0.8761905  0.30000000
  0.40  7          1.5    100      0.7353175  0.8761905  0.30000000
  0.40  7          1.5    110      0.7353175  0.8761905  0.30000000
  0.40  7          1.5    120      0.7353175  0.8761905  0.30000000
  0.40  7          1.5    130      0.7353175  0.8619048  0.30000000
  0.40  7          1.5    140      0.7353175  0.8761905  0.30000000
  0.40  7          1.5    150      0.7353175  0.8619048  0.30000000
  0.40  7          1.5    160      0.7353175  0.8761905  0.30000000
  0.40  7          1.5    170      0.7353175  0.8761905  0.30000000
  0.40  7          1.5    180      0.7353175  0.8476190  0.30000000
  0.40  7          1.5    190      0.7353175  0.8761905  0.30000000
  0.40  7          1.5    200      0.7257937  0.8761905  0.30000000

Tuning parameter &#39;colsample_bytree&#39; was held constant at a value of 0.8

Tuning parameter &#39;min_child_weight&#39; was held constant at a value of 2

Tuning parameter &#39;subsample&#39; was held constant at a value of 0.8
ROC was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 30, max_depth = 4, eta
 = 0.4, gamma = 0.5, colsample_bytree = 0.8, min_child_weight = 2 and
 subsample = 0.8.</code></pre>
<pre class="r"><code>#predict on test data
pred1.y &lt;- predict(model, pre17.testing.df1, type = &quot;prob&quot;)[,2]

# out of sample performance metrics
test1.y &lt;- as.numeric(pre17.testing.df1$Preeclampsia)-1

pROC::auc(test1.y, pred1.y)
Setting levels: control = 0, case = 1
Setting direction: controls &lt; cases
Area under the curve: 0.6245</code></pre>
<pre class="r"><code>
FNR(pred1.y, test1.y)
     class.pred
truth  0  1
    0 55  6
    1 19  7
[1] 0.7307692</code></pre>
<pre class="r"><code>FPR(pred1.y, test1.y)
     class.pred
truth  0  1
    0 55  6
    1 19  7
[1] 0.09836066</code></pre>
<pre class="r"><code># Add to output
res.testing1[1, ] &lt;- c(pROC::auc(test1.y, pred1.y), FNR(pred1.y, test1.y), FPR(pred1.y, test1.y))
Setting levels: control = 0, case = 1
Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>rownames(res.testing1)[nrow(res.testing1)] &lt;- &#39;XGBOOST&#39;</code></pre>
<pre class="r"><code>#predict on test data
pred2.y &lt;- predict(model, pre17.testing.df2, type = &quot;prob&quot;)[,2]

# out of sample performance metrics
test2.y &lt;- as.numeric(pre17.testing.df2$Preeclampsia)-1

pROC::auc(test2.y, pred2.y)
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases
Area under the curve: 0.625</code></pre>
<pre class="r"><code>
FNR(pred2.y, test2.y)
     class.pred
truth 0 1
    0 8 5
    1 7 1
[1] 0.875</code></pre>
<pre class="r"><code>FPR(pred2.y, test2.y)
     class.pred
truth 0 1
    0 8 5
    1 7 1
[1] 0.3846154</code></pre>
<pre class="r"><code># Add to output
res.testing2[1, ] &lt;- c(pROC::auc(test2.y, pred2.y), FNR(pred2.y, test2.y), FPR(pred2.y, test2.y))
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases
     class.pred
truth 0 1
    0 8 5
    1 7 1
     class.pred
truth 0 1
    0 8 5
    1 7 1</code></pre>
<pre class="r"><code>rownames(res.testing2)[nrow(res.testing2)] &lt;- &#39;XGBOOST&#39;</code></pre>
</div>
<div id="class-imbalance" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Class imbalance</h1>
<p>Class imbalance is identified since there are 27.5% normotensive and 72.5% preeclampsia pregnancies. Unbalanced problems should be addressed when applying SVM, since it aims to separate the space into two parts.</p>
<pre class="r"><code>remotes::install_github(&quot;cran/DMwR&quot;)
Using GitHub PAT from the git credential store.
Skipping install of &#39;DMwR&#39; from a github remote, the SHA1 (6fd4f0cd) has not changed since last install.
  Use `force = TRUE` to force installation</code></pre>
<pre class="r"><code>smote_dataset &lt;- as.data.frame(pre17.training.df)
smote_dataset$Preeclampsia &lt;- as.factor(smote_dataset$Preeclampsia)
table(pre17.training.df$Preeclampsia)

 no yes 
 66  24 </code></pre>
<p>When perc.over is 100, we create 1 new example (100/100 = 1)</p>
<pre class="r"><code>library(DMwR)
Loading required package: grid
Registered S3 method overwritten by &#39;quantmod&#39;:
  method            from
  as.zoo.data.frame zoo </code></pre>
<pre class="r"><code>set.seed(111)
resampled.training.df &lt;- SMOTE(Preeclampsia ~ ., smote_dataset, perc.over = 70, k =5)
table(resampled.training.df$Preeclampsia)

 no yes 
 32  40 </code></pre>
<pre class="r"><code>reweight &lt;- function(pi, q1, r1) {
  r0 &lt;- 1 - r1
  q0 &lt;- 1 - q1
  tot &lt;- pi * (q1 / r1) + (1 - pi) * (q0 / r0)
  w &lt;- pi * (q1 / r1) / tot
  return(w)
}</code></pre>
</div>
<div id="svm-models" class="section level1" number="6">
<h1><span class="header-section-number">6</span> SVM models</h1>
<div id="svmlinearweights-linear-kernel-class-weights" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> svmLinearWeights (linear kernel + class weights)</h2>
<pre class="r"><code>modelLookup(&quot;svmLinearWeights&quot;)
             model parameter        label forReg forClass probModel
1 svmLinearWeights      cost         Cost  FALSE     TRUE      TRUE
2 svmLinearWeights    weight Class Weight  FALSE     TRUE      TRUE</code></pre>
<pre class="r"><code>train_control = trainControl(method = &quot;cv&quot;, number = 10, search = &quot;grid&quot;,
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(50)
# Customzing the tuning grid


svmgrid &lt;-  expand.grid(cost = c(0.0001, 0.001,0.01, 0.05,0.1, 0.3,0.5,1,3, 5, 10), weight = c(0.01, 0.05, 0.1, 0.3,0.5, 0.7,1,3,5,10))


# auc = 0.58, 0.48 cost = 0.01 and weight = 3. cost = c(0.01, 0.05,0.5,1,3, 5, 10), weight = c(0.01, 0.05,0.5,1,3,5,10) 

# auc = 0.57, 0.48, fnr = 0.92, 0.83. cost = 0.05 and weight = 0.5. weight = c(0.01, 0.05,0.5)

# auc = 0.64, 0.52. fnr=0.71,0.75. cost = c(0.001, 0.01, 0.1, 1, 10, 100, 1000), weight = c(0.001, 0.01, 0.1, 1, 10, 100, 1000). optimal: cost 0.01, weight 1
# auc = 0.64, 0.52 fnr=0.53, 0.75 (si poso un weight menor empitjora). svmgrid &lt;-  expand.grid(cost = c(0.008, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1), weight = c(7,8,9,10,11,12))
# auc = 0.64, 0.52 fnr=0.53, 0.75. cost = c(0.001, 1, 10, 100, 1000), weight = c(0.5,1.5,2,3,4, 10, 100, 1000)). cost = 1, weight =0.5



# training a svm classifier with liearn kernel model while tuning parameters
model = caret::train(Preeclampsia~., data = resampled.training.df,
              method = &quot;svmLinearWeights&quot;,
              trControl = train_control,
              metric = &quot;ROC&quot;,
              tuneGrid = svmgrid)

# summarizing the results
print(model)
Linear Support Vector Machines with Class Weights 

 72 samples
912 predictors
  2 classes: &#39;no&#39;, &#39;yes&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 64, 65, 65, 65, 65, 65, ... 
Resampling results across tuning parameters:

  cost   weight  ROC        Sens       Spec 
  1e-04   0.01   0.9250000  1.0000000  0.000
  1e-04   0.05   0.9166667  1.0000000  0.000
  1e-04   0.10   0.9166667  1.0000000  0.000
  1e-04   0.30   0.8791667  1.0000000  0.000
  1e-04   0.50   0.8479167  1.0000000  0.000
  1e-04   0.70   0.8395833  0.8583333  0.550
  1e-04   1.00   0.7458333  0.4750000  0.950
  1e-04   3.00   0.9312500  0.0000000  1.000
  1e-04   5.00   0.9416667  0.0000000  1.000
  1e-04  10.00   0.9500000  0.0000000  1.000
  1e-03   0.01   0.9166667  1.0000000  0.000
  1e-03   0.05   0.9166667  1.0000000  0.000
  1e-03   0.10   0.9166667  1.0000000  0.000
  1e-03   0.30   0.9166667  0.8916667  0.675
  1e-03   0.50   0.9375000  0.8333333  0.875
  1e-03   0.70   0.9437500  0.8000000  0.875
  1e-03   1.00   0.9520833  0.8000000  0.950
  1e-03   3.00   0.9750000  0.8000000  1.000
  1e-03   5.00   0.9750000  0.8000000  1.000
  1e-03  10.00   0.9750000  0.8000000  1.000
  1e-02   0.01   0.9166667  1.0000000  0.000
  1e-02   0.05   0.9312500  0.9250000  0.775
  1e-02   0.10   0.9500000  0.8333333  0.900
  1e-02   0.30   0.9666667  0.8666667  0.950
  1e-02   0.50   0.9750000  0.8333333  0.975
  1e-02   0.70   0.9750000  0.8333333  1.000
  1e-02   1.00   0.9750000  0.8333333  1.000
  1e-02   3.00   0.9750000  0.8333333  1.000
  1e-02   5.00   0.9750000  0.8333333  1.000
  1e-02  10.00   0.9750000  0.8333333  1.000
  5e-02   0.01   0.9312500  0.9250000  0.775
  5e-02   0.05   0.9583333  0.8666667  0.925
  5e-02   0.10   0.9750000  0.8333333  0.975
  5e-02   0.30   0.9750000  0.8333333  1.000
  5e-02   0.50   0.9750000  0.8333333  1.000
  5e-02   0.70   0.9750000  0.8333333  1.000
  5e-02   1.00   0.9750000  0.8333333  1.000
  5e-02   3.00   0.9750000  0.8333333  1.000
  5e-02   5.00   0.9750000  0.8333333  1.000
  5e-02  10.00   0.9750000  0.8333333  1.000
  1e-01   0.01   0.9500000  0.8333333  0.900
  1e-01   0.05   0.9750000  0.8333333  0.975
  1e-01   0.10   0.9750000  0.8333333  1.000
  1e-01   0.30   0.9750000  0.8333333  1.000
  1e-01   0.50   0.9750000  0.8333333  1.000
  1e-01   0.70   0.9750000  0.8333333  1.000
  1e-01   1.00   0.9750000  0.8333333  1.000
  1e-01   3.00   0.9750000  0.8333333  1.000
  1e-01   5.00   0.9750000  0.8333333  1.000
  1e-01  10.00   0.9750000  0.8333333  1.000
  3e-01   0.01   0.9666667  0.8666667  0.925
  3e-01   0.05   0.9750000  0.8333333  1.000
  3e-01   0.10   0.9750000  0.8333333  1.000
  3e-01   0.30   0.9750000  0.8333333  1.000
  3e-01   0.50   0.9750000  0.8333333  1.000
  3e-01   0.70   0.9750000  0.8333333  1.000
  3e-01   1.00   0.9750000  0.8333333  1.000
  3e-01   3.00   0.9750000  0.8333333  1.000
  3e-01   5.00   0.9750000  0.8333333  1.000
  3e-01  10.00   0.9750000  0.8333333  1.000
  5e-01   0.01   0.9750000  0.8333333  0.975
  5e-01   0.05   0.9750000  0.8333333  1.000
  5e-01   0.10   0.9750000  0.8333333  1.000
  5e-01   0.30   0.9750000  0.8333333  1.000
  5e-01   0.50   0.9750000  0.8333333  1.000
  5e-01   0.70   0.9750000  0.8333333  1.000
  5e-01   1.00   0.9750000  0.8333333  1.000
  5e-01   3.00   0.9750000  0.8333333  1.000
  5e-01   5.00   0.9750000  0.8333333  1.000
  5e-01  10.00   0.9750000  0.8333333  1.000
  1e+00   0.01   0.9750000  0.8333333  1.000
  1e+00   0.05   0.9750000  0.8333333  1.000
  1e+00   0.10   0.9750000  0.8333333  1.000
  1e+00   0.30   0.9750000  0.8333333  1.000
  1e+00   0.50   0.9750000  0.8333333  1.000
  1e+00   0.70   0.9750000  0.8333333  1.000
  1e+00   1.00   0.9750000  0.8333333  1.000
  1e+00   3.00   0.9750000  0.8333333  1.000
  1e+00   5.00   0.9750000  0.8333333  1.000
  1e+00  10.00   0.9750000  0.8333333  1.000
  3e+00   0.01   0.9750000  0.8333333  1.000
  3e+00   0.05   0.9750000  0.8333333  1.000
  3e+00   0.10   0.9750000  0.8333333  1.000
  3e+00   0.30   0.9750000  0.8333333  1.000
  3e+00   0.50   0.9750000  0.8333333  1.000
  3e+00   0.70   0.9750000  0.8333333  1.000
  3e+00   1.00   0.9750000  0.8333333  1.000
  3e+00   3.00   0.9750000  0.8333333  1.000
  3e+00   5.00   0.9750000  0.8333333  1.000
  3e+00  10.00   0.9750000  0.8333333  1.000
  5e+00   0.01   0.9750000  0.8333333  1.000
  5e+00   0.05   0.9750000  0.8333333  1.000
  5e+00   0.10   0.9750000  0.8333333  1.000
  5e+00   0.30   0.9750000  0.8333333  1.000
  5e+00   0.50   0.9750000  0.8333333  1.000
  5e+00   0.70   0.9750000  0.8333333  1.000
  5e+00   1.00   0.9750000  0.8333333  1.000
  5e+00   3.00   0.9750000  0.8333333  1.000
  5e+00   5.00   0.9750000  0.8333333  1.000
  5e+00  10.00   0.9750000  0.8333333  1.000
  1e+01   0.01   0.9750000  0.8333333  1.000
  1e+01   0.05   0.9750000  0.8333333  1.000
  1e+01   0.10   0.9750000  0.8333333  1.000
  1e+01   0.30   0.9750000  0.8333333  1.000
  1e+01   0.50   0.9750000  0.8333333  1.000
  1e+01   0.70   0.9750000  0.8333333  1.000
  1e+01   1.00   0.9750000  0.8333333  1.000
  1e+01   3.00   0.9750000  0.8333333  1.000
  1e+01   5.00   0.9750000  0.8333333  1.000
  1e+01  10.00   0.9750000  0.8333333  1.000

ROC was used to select the optimal model using the largest value.
The final values used for the model were cost = 0.001 and weight = 3.</code></pre>
<pre class="r"><code>plot(model)</code></pre>
<p><img src="05_pr_pe_files/figure-html/unnamed-chunk-40-1.png" width="100%"  class="widefigure" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#predict on test data
pred1.y &lt;- predict(model, pre17.testing.df1, type = &quot;prob&quot;)[,2]

q1 &lt;- sum(pre17.training.df$Preeclampsia == &quot;yes&quot;) / length(pre17.training.df$Preeclampsia)
r1 &lt;- sum(resampled.training.df$Preeclampsia == &quot;yes&quot;) / length(resampled.training.df$Preeclampsia)

reweighted.probs1 &lt;- sapply(pred1.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test1.y &lt;- as.numeric(pre17.testing.df1$Preeclampsia) -1

pROC::auc(test1.y, reweighted.probs1)
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases
Area under the curve: 0.553</code></pre>
<pre class="r"><code>
FNR(reweighted.probs1, test1.y)
     class.pred
truth  0  1
    0 50 11
    1 19  7
[1] 0.7307692</code></pre>
<pre class="r"><code>FPR(reweighted.probs1, test1.y)
     class.pred
truth  0  1
    0 50 11
    1 19  7
[1] 0.1803279</code></pre>
<pre class="r"><code># Add to output
res.testing1 &lt;- rbind.data.frame(res.testing1, c(pROC::auc(test1.y, reweighted.probs1), FNR(reweighted.probs1, test1.y), FPR(reweighted.probs1, test1.y)))
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>rownames(res.testing1)[nrow(res.testing1)] &lt;- &#39;SVMLinear&#39;</code></pre>
<pre class="r"><code>#predict on test data
pred2.y &lt;- predict(model, pre17.testing.df2, type = &quot;prob&quot;)[,2]

q1 &lt;- sum(pre17.training.df$Preeclampsia == &quot;yes&quot;) / length(pre17.training.df$Preeclampsia)
r1 &lt;- sum(resampled.training.df$Preeclampsia == &quot;yes&quot;) / length(resampled.training.df$Preeclampsia)

reweighted.probs2 &lt;- sapply(pred2.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test2.y &lt;- as.numeric(pre17.testing.df2$Preeclampsia) -1

pROC::auc(test2.y, reweighted.probs2)
Setting levels: control = 0, case = 1
Setting direction: controls &lt; cases
Area under the curve: 0.5769</code></pre>
<pre class="r"><code>
FNR(reweighted.probs2, test2.y)
     class.pred
truth 0 1
    0 9 4
    1 6 2
[1] 0.75</code></pre>
<pre class="r"><code>FPR(reweighted.probs2, test2.y)
     class.pred
truth 0 1
    0 9 4
    1 6 2
[1] 0.3076923</code></pre>
<pre class="r"><code># Add to output
res.testing2 &lt;- rbind.data.frame(res.testing2, c(pROC::auc(test2.y, reweighted.probs2), FNR(reweighted.probs2, test2.y), FPR(reweighted.probs2, test2.y)))
Setting levels: control = 0, case = 1
Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>rownames(res.testing2)[nrow(res.testing2)] &lt;- &#39;SVMLinear&#39;</code></pre>
</div>
<div id="svmradial-support-vector-machines-with-radial-basis-function-kernel" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> svmRadial (Support Vector Machines with Radial Basis Function Kernel)</h2>
<pre class="r"><code>modelLookup(&quot;svmRadial&quot;)
      model parameter label forReg forClass probModel
1 svmRadial     sigma Sigma   TRUE     TRUE      TRUE
2 svmRadial         C  Cost   TRUE     TRUE      TRUE</code></pre>
<pre class="r"><code>train_control = trainControl(method = &quot;cv&quot;, number = 10, search = &quot;grid&quot;,
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(50)
# Customzing the tuning grid
svmgrid &lt;-  expand.grid(sigma = c(0.001, 0.1, 0.3, 0.5, 0.7, 1, 2, 3, 4, 5, 6, 7 ,10),
                        C = c(0.001, 0.1, 0.3, 0.5, 0.7, 1, 2, 3, 4, 5, 6, 7 ,10))

#svmgrid &lt;-  expand.grid(sigma=c(0.01, 0.05,0.5,1,3, 5, 10), C = c(0.01, 0.05,0.5,1,3,5,10))

#svmgrid &lt;-  expand.grid(sigma=c(1), C = c(3))


# training a svm with rbf kernel classifier model while tuning parameters
model = caret::train(Preeclampsia~., data = resampled.training.df,
              method = &quot;svmRadial&quot;,
              trControl = train_control,
              metric = &quot;ROC&quot;,
              tuneGrid = svmgrid)
maximum number of iterations reached 0.003366787 -0.003385415maximum number of iterations reached 0.003395925 -0.003397071maximum number of iterations reached 0.003298177 -0.003296213maximum number of iterations reached 0.001896378 -0.001895105maximum number of iterations reached 0.003236088 -0.00323428maximum number of iterations reached 0.00187366 -0.001872902maximum number of iterations reached 0.002327917 -0.0023264maximum number of iterations reached 0.002751428 -0.002749731maximum number of iterations reached 0.003396211 -0.003397359maximum number of iterations reached 0.003161333 -0.003159485maximum number of iterations reached 0.003395802 -0.003396948maximum number of iterations reached 0.00180389 -0.001803073maximum number of iterations reached 0.002327917 -0.0023264maximum number of iterations reached 0.003845123 -0.003851983maximum number of iterations reached 0.003127465 -0.00312619maximum number of iterations reached 0.002758107 -0.002758623maximum number of iterations reached 0.002785462 -0.002784206maximum number of iterations reached 0.002354708 -0.002353467maximum number of iterations reached 0.003031704 -0.003032933maximum number of iterations reached 0.00231208 -0.002310822maximum number of iterations reached 0.002297746 -0.00229648maximum number of iterations reached 0.00270836 -0.002707041maximum number of iterations reached 0.001867765 -0.001866642maximum number of iterations reached 0.002707913 -0.002706594maximum number of iterations reached 0.00311337 -0.003112068maximum number of iterations reached 0.003113361 -0.003112059maximum number of iterations reached 0.003451919 -0.003452548maximum number of iterations reached 0.003335598 -0.0033336maximum number of iterations reached 0.002502448 -0.002501085maximum number of iterations reached 0.002820538 -0.002819258maximum number of iterations reached 0.003219126 -0.003217903maximum number of iterations reached 0.002293394 -0.002292125maximum number of iterations reached 0.002369393 -0.002367569maximum number of iterations reached 0.002296508 -0.002295242maximum number of iterations reached 0.002708558 -0.00270724maximum number of iterations reached 0.002577614 -0.002577792maximum number of iterations reached 0.001867635 -0.001866512maximum number of iterations reached 0.001867592 -0.001866469maximum number of iterations reached 0.001914409 -0.001912997maximum number of iterations reached 0.003624371 -0.003630951maximum number of iterations reached 0.003254564 -0.003254183maximum number of iterations reached 0.003929827 -0.003940614maximum number of iterations reached 0.00310947 -0.003109563maximum number of iterations reached 0.003153181 -0.003158921maximum number of iterations reached 0.003629981 -0.003634571maximum number of iterations reached 0.003270229 -0.003273445maximum number of iterations reached 0.003037672 -0.003037564maximum number of iterations reached 0.001867584 -0.001866461maximum number of iterations reached 0.002641958 -0.002641437maximum number of iterations reached 0.002891769 -0.002893628maximum number of iterations reached 0.003262883 -0.003266042maximum number of iterations reached 0.003262883 -0.003266042maximum number of iterations reached 0.00397717 -0.003997573maximum number of iterations reached 0.003427191 -0.003427584maximum number of iterations reached 0.003055921 -0.003058402maximum number of iterations reached 0.002650002 -0.002651274maximum number of iterations reached 0.003717002 -0.003722236maximum number of iterations reached 0.002891769 -0.002893628maximum number of iterations reached 0.002654417 -0.002653924maximum number of iterations reached 0.003430479 -0.003430887maximum number of iterations reached 0.002642723 -0.002642203maximum number of iterations reached 0.002641959 -0.002641437maximum number of iterations reached 0.002707919 -0.0027066maximum number of iterations reached 0.00342699 -0.003427382maximum number of iterations reached 0.003037672 -0.003037564maximum number of iterations reached 0.004028414 -0.004037213maximum number of iterations reached 0.002774359 -0.002773088maximum number of iterations reached 0.001789042 -0.001788409maximum number of iterations reached 0.001464821 -0.001463815maximum number of iterations reached 0.002707874 -0.002706555maximum number of iterations reached 0.002182733 -0.002182372maximum number of iterations reached 0.002963823 -0.002964764maximum number of iterations reached 0.002963823 -0.002964764maximum number of iterations reached 0.001867584 -0.001866461maximum number of iterations reached 0.002293391 -0.002292122maximum number of iterations reached 0.003343931 -0.003345798maximum number of iterations reached 0.001463803 -0.001462798maximum number of iterations reached 0.001867584 -0.001866461maximum number of iterations reached 0.004454537 -0.004477888maximum number of iterations reached 0.00354257 -0.00354678maximum number of iterations reached 0.002788945 -0.002788596maximum number of iterations reached 0.003018171 -0.003020629maximum number of iterations reached 0.002265171 -0.002264968maximum number of iterations reached 0.002891769 -0.002893628maximum number of iterations reached 0.003116344 -0.00312171maximum number of iterations reached 0.002240023 -0.002239243maximum number of iterations reached 0.003038029 -0.003037922maximum number of iterations reached 0.00289188 -0.00289374maximum number of iterations reached 0.001428034 -0.001427149maximum number of iterations reached 0.002641869 -0.002641347maximum number of iterations reached 0.002641862 -0.00264134maximum number of iterations reached 0.0040054 -0.004032744maximum number of iterations reached 0.003483811 -0.003486824maximum number of iterations reached 0.002860415 -0.002859535maximum number of iterations reached 0.00321434 -0.003213837maximum number of iterations reached 0.002414114 -0.002412597maximum number of iterations reached 0.002751052 -0.002750262maximum number of iterations reached 0.002283582 -0.002282553maximum number of iterations reached 0.003315656 -0.003318124maximum number of iterations reached 0.002682573 -0.002681669maximum number of iterations reached 0.002616625 -0.002616415maximum number of iterations reached 0.003313271 -0.003315722maximum number of iterations reached 0.00227101 -0.002269967maximum number of iterations reached 0.002935884 -0.002937208maximum number of iterations reached 0.004155772 -0.004146656maximum number of iterations reached 0.003186643 -0.003187765maximum number of iterations reached 0.001874951 -0.001873829maximum number of iterations reached 0.002372211 -0.002370972maximum number of iterations reached 0.002707929 -0.00270661maximum number of iterations reached 0.00108013 -0.001079461maximum number of iterations reached 0.00230929 -0.002308031maximum number of iterations reached 0.002711157 -0.002709842maximum number of iterations reached 0.001464203 -0.001463198maximum number of iterations reached 0.003343931 -0.003345798maximum number of iterations reached 0.002963838 -0.002964779maximum number of iterations reached 0.002707874 -0.002706555maximum number of iterations reached 0.002293391 -0.002292122maximum number of iterations reached 0.004029511 -0.0040272maximum number of iterations reached 0.001874602 -0.001873479maximum number of iterations reached 0.002293406 -0.002292138maximum number of iterations reached 0.002708495 -0.002707177maximum number of iterations reached 0.001867644 -0.001866521maximum number of iterations reached 0.002293391 -0.002292122maximum number of iterations reached 0.002293391 -0.002292122maximum number of iterations reached 0.001867584 -0.001866461maximum number of iterations reached 0.001914409 -0.001912997maximum number of iterations reached 0.003113361 -0.003112059maximum number of iterations reached 0.002350826 -0.002349007maximum number of iterations reached 0.002350826 -0.002349007maximum number of iterations reached 0.003113361 -0.003112059</code></pre>
<pre class="r"><code>
# summarizing the results
print(model)
Support Vector Machines with Radial Basis Function Kernel 

 72 samples
912 predictors
  2 classes: &#39;no&#39;, &#39;yes&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 64, 65, 65, 65, 65, 65, ... 
Resampling results across tuning parameters:

  sigma  C      ROC        Sens       Spec 
  1e-03  1e-03  0.8895833  0.8833333  0.700
  1e-03  1e-01  0.8895833  0.8833333  0.725
  1e-03  3e-01  0.9020833  0.8833333  0.750
  1e-03  5e-01  0.9145833  0.7333333  0.875
  1e-03  7e-01  0.9312500  0.7666667  0.900
  1e-03  1e+00  0.9583333  0.7666667  0.925
  1e-03  2e+00  0.9750000  0.8333333  0.975
  1e-03  3e+00  0.9750000  0.8250000  0.975
  1e-03  4e+00  0.9687500  0.8583333  0.975
  1e-03  5e+00  0.9687500  0.8583333  0.950
  1e-03  6e+00  0.9687500  0.8000000  0.975
  1e-03  7e+00  0.9687500  0.8583333  0.950
  1e-03  1e+01  0.9687500  0.8583333  0.975
  1e-01  1e-03  0.8791667  0.4833333  1.000
  1e-01  1e-01  0.9072917  0.5166667  1.000
  1e-01  3e-01  0.9072917  0.5166667  1.000
  1e-01  5e-01  0.9072917  0.5166667  1.000
  1e-01  7e-01  0.9072917  0.5166667  1.000
  1e-01  1e+00  0.9072917  0.5166667  1.000
  1e-01  2e+00  0.9072917  0.5166667  1.000
  1e-01  3e+00  0.9072917  0.5166667  1.000
  1e-01  4e+00  0.9072917  0.5166667  1.000
  1e-01  5e+00  0.9072917  0.5166667  1.000
  1e-01  6e+00  0.9072917  0.5166667  1.000
  1e-01  7e+00  0.9072917  0.5166667  1.000
  1e-01  1e+01  0.9072917  0.5166667  1.000
  3e-01  1e-03  0.8375000  0.4166667  1.000
  3e-01  1e-01  0.8375000  0.5166667  1.000
  3e-01  3e-01  0.8375000  0.5166667  1.000
  3e-01  5e-01  0.8375000  0.5166667  1.000
  3e-01  7e-01  0.8375000  0.5166667  1.000
  3e-01  1e+00  0.8375000  0.5166667  1.000
  3e-01  2e+00  0.8375000  0.5166667  1.000
  3e-01  3e+00  0.8375000  0.5166667  1.000
  3e-01  4e+00  0.8375000  0.5166667  1.000
  3e-01  5e+00  0.8375000  0.5166667  1.000
  3e-01  6e+00  0.8375000  0.5166667  1.000
  3e-01  7e+00  0.8375000  0.5166667  1.000
  3e-01  1e+01  0.8375000  0.5166667  1.000
  5e-01  1e-03  0.7968750  0.4833333  1.000
  5e-01  1e-01  0.7968750  0.5166667  1.000
  5e-01  3e-01  0.7968750  0.5166667  1.000
  5e-01  5e-01  0.7968750  0.5166667  1.000
  5e-01  7e-01  0.7968750  0.5166667  1.000
  5e-01  1e+00  0.7968750  0.5166667  1.000
  5e-01  2e+00  0.8010417  0.5166667  1.000
  5e-01  3e+00  0.7968750  0.5166667  1.000
  5e-01  4e+00  0.7968750  0.5166667  1.000
  5e-01  5e+00  0.7968750  0.5166667  1.000
  5e-01  6e+00  0.7968750  0.5166667  1.000
  5e-01  7e+00  0.7968750  0.5166667  1.000
  5e-01  1e+01  0.8010417  0.5166667  1.000
  7e-01  1e-03  0.7968750  0.3583333  1.000
  7e-01  1e-01  0.7968750  0.5166667  1.000
  7e-01  3e-01  0.7968750  0.5166667  1.000
  7e-01  5e-01  0.7968750  0.5166667  1.000
  7e-01  7e-01  0.7968750  0.5166667  1.000
  7e-01  1e+00  0.7968750  0.5166667  1.000
  7e-01  2e+00  0.7968750  0.5166667  1.000
  7e-01  3e+00  0.7968750  0.5166667  1.000
  7e-01  4e+00  0.7968750  0.5166667  1.000
  7e-01  5e+00  0.7968750  0.5166667  1.000
  7e-01  6e+00  0.7968750  0.5166667  1.000
  7e-01  7e+00  0.7968750  0.5166667  1.000
  7e-01  1e+01  0.7968750  0.5166667  1.000
  1e+00  1e-03  0.7791667  0.3250000  1.000
  1e+00  1e-01  0.7968750  0.5166667  1.000
  1e+00  3e-01  0.7968750  0.5166667  1.000
  1e+00  5e-01  0.7968750  0.5166667  1.000
  1e+00  7e-01  0.7968750  0.5166667  1.000
  1e+00  1e+00  0.7968750  0.5166667  1.000
  1e+00  2e+00  0.7968750  0.5166667  1.000
  1e+00  3e+00  0.7968750  0.5166667  1.000
  1e+00  4e+00  0.7968750  0.5166667  1.000
  1e+00  5e+00  0.7968750  0.5166667  1.000
  1e+00  6e+00  0.7968750  0.5166667  1.000
  1e+00  7e+00  0.7968750  0.5166667  1.000
  1e+00  1e+01  0.7968750  0.5166667  1.000
  2e+00  1e-03  0.7791667  0.3583333  1.000
  2e+00  1e-01  0.7791667  0.5166667  1.000
  2e+00  3e-01  0.7791667  0.5166667  1.000
  2e+00  5e-01  0.7791667  0.5166667  1.000
  2e+00  7e-01  0.7791667  0.5166667  1.000
  2e+00  1e+00  0.7791667  0.5166667  1.000
  2e+00  2e+00  0.7791667  0.5166667  1.000
  2e+00  3e+00  0.7791667  0.5166667  1.000
  2e+00  4e+00  0.7791667  0.5166667  1.000
  2e+00  5e+00  0.7791667  0.5166667  1.000
  2e+00  6e+00  0.7791667  0.5166667  1.000
  2e+00  7e+00  0.7791667  0.5166667  1.000
  2e+00  1e+01  0.7791667  0.5166667  1.000
  3e+00  1e-03  0.7666667  0.3500000  1.000
  3e+00  1e-01  0.7666667  0.5166667  1.000
  3e+00  3e-01  0.7791667  0.5166667  1.000
  3e+00  5e-01  0.7791667  0.5166667  1.000
  3e+00  7e-01  0.7791667  0.5166667  1.000
  3e+00  1e+00  0.7791667  0.5166667  1.000
  3e+00  2e+00  0.7791667  0.5166667  1.000
  3e+00  3e+00  0.7791667  0.5166667  1.000
  3e+00  4e+00  0.7791667  0.5166667  1.000
  3e+00  5e+00  0.7791667  0.5166667  1.000
  3e+00  6e+00  0.7791667  0.5166667  1.000
  3e+00  7e+00  0.7791667  0.5166667  1.000
  3e+00  1e+01  0.7791667  0.5166667  1.000
  4e+00  1e-03  0.7666667  0.3583333  1.000
  4e+00  1e-01  0.7666667  0.5166667  1.000
  4e+00  3e-01  0.7666667  0.5166667  1.000
  4e+00  5e-01  0.7666667  0.5166667  1.000
  4e+00  7e-01  0.7666667  0.5166667  1.000
  4e+00  1e+00  0.7666667  0.5166667  1.000
  4e+00  2e+00  0.7666667  0.5166667  1.000
  4e+00  3e+00  0.7666667  0.5166667  1.000
  4e+00  4e+00  0.7666667  0.5166667  1.000
  4e+00  5e+00  0.7666667  0.5166667  1.000
  4e+00  6e+00  0.7666667  0.5166667  1.000
  4e+00  7e+00  0.7666667  0.5166667  1.000
  4e+00  1e+01  0.7666667  0.5166667  1.000
  5e+00  1e-03  0.7666667  0.4250000  1.000
  5e+00  1e-01  0.7666667  0.5166667  1.000
  5e+00  3e-01  0.7666667  0.5166667  1.000
  5e+00  5e-01  0.7666667  0.5166667  1.000
  5e+00  7e-01  0.7666667  0.5166667  1.000
  5e+00  1e+00  0.7666667  0.5166667  1.000
  5e+00  2e+00  0.7666667  0.5166667  1.000
  5e+00  3e+00  0.7666667  0.5166667  1.000
  5e+00  4e+00  0.7666667  0.5166667  1.000
  5e+00  5e+00  0.7666667  0.5166667  1.000
  5e+00  6e+00  0.7666667  0.5166667  1.000
  5e+00  7e+00  0.7666667  0.5166667  1.000
  5e+00  1e+01  0.7666667  0.5166667  1.000
  6e+00  1e-03  0.7666667  0.3833333  1.000
  6e+00  1e-01  0.7666667  0.5166667  1.000
  6e+00  3e-01  0.7666667  0.5166667  1.000
  6e+00  5e-01  0.7666667  0.5166667  1.000
  6e+00  7e-01  0.7666667  0.5166667  1.000
  6e+00  1e+00  0.7666667  0.5166667  1.000
  6e+00  2e+00  0.7666667  0.5166667  1.000
  6e+00  3e+00  0.7666667  0.5166667  1.000
  6e+00  4e+00  0.7666667  0.5166667  1.000
  6e+00  5e+00  0.7666667  0.5166667  1.000
  6e+00  6e+00  0.7666667  0.5166667  1.000
  6e+00  7e+00  0.7666667  0.5166667  1.000
  6e+00  1e+01  0.7666667  0.5166667  1.000
  7e+00  1e-03  0.7666667  0.3916667  1.000
  7e+00  1e-01  0.7666667  0.5166667  1.000
  7e+00  3e-01  0.7666667  0.5166667  1.000
  7e+00  5e-01  0.7666667  0.5166667  1.000
  7e+00  7e-01  0.7666667  0.5166667  1.000
  7e+00  1e+00  0.7666667  0.5166667  1.000
  7e+00  2e+00  0.7666667  0.5166667  1.000
  7e+00  3e+00  0.7666667  0.5166667  1.000
  7e+00  4e+00  0.7666667  0.5166667  1.000
  7e+00  5e+00  0.7666667  0.5166667  1.000
  7e+00  6e+00  0.7666667  0.5166667  1.000
  7e+00  7e+00  0.7666667  0.5166667  1.000
  7e+00  1e+01  0.7666667  0.5166667  1.000
  1e+01  1e-03  0.7666667  0.4166667  1.000
  1e+01  1e-01  0.7666667  0.5166667  1.000
  1e+01  3e-01  0.7666667  0.5166667  1.000
  1e+01  5e-01  0.7666667  0.5166667  1.000
  1e+01  7e-01  0.7666667  0.5166667  1.000
  1e+01  1e+00  0.7666667  0.5166667  1.000
  1e+01  2e+00  0.7666667  0.5166667  1.000
  1e+01  3e+00  0.7666667  0.5166667  1.000
  1e+01  4e+00  0.7666667  0.5166667  1.000
  1e+01  5e+00  0.7666667  0.5166667  1.000
  1e+01  6e+00  0.7666667  0.5166667  1.000
  1e+01  7e+00  0.7666667  0.5166667  1.000
  1e+01  1e+01  0.7666667  0.5166667  1.000

ROC was used to select the optimal model using the largest value.
The final values used for the model were sigma = 0.001 and C = 2.</code></pre>
<pre class="r"><code>plot(model)</code></pre>
<p><img src="05_pr_pe_files/figure-html/unnamed-chunk-47-1.png" width="100%"  class="widefigure" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#predict on test data
pred1.y &lt;- predict(model, pre17.testing.df1, type = &quot;prob&quot;)[,2]

q1 &lt;- sum(pre17.training.df$Preeclampsia == &quot;yes&quot;) / length(pre17.training.df$Preeclampsia)
r1 &lt;- sum(resampled.training.df$Preeclampsia == &quot;yes&quot;) / length(resampled.training.df$Preeclampsia)

reweighted.probs1 &lt;- sapply(pred1.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test1.y &lt;- as.numeric(pre17.testing.df1$Preeclampsia)-1

pROC::auc(test1.y, reweighted.probs1)
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases
Area under the curve: 0.4968</code></pre>
<pre class="r"><code>
FNR(reweighted.probs1, test1.y)
     class.pred
truth  0  1
    0 49 12
    1 19  7
[1] 0.7307692</code></pre>
<pre class="r"><code>FPR(reweighted.probs1, test1.y)
     class.pred
truth  0  1
    0 49 12
    1 19  7
[1] 0.1967213</code></pre>
<pre class="r"><code># Add to output
res.testing1 &lt;- rbind.data.frame(res.testing1, c(pROC::auc(test1.y, reweighted.probs1), FNR(reweighted.probs1, test1.y), FPR(reweighted.probs1, test1.y)))
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>rownames(res.testing1)[nrow(res.testing1)] &lt;- &#39;SVMRadial&#39;</code></pre>
<pre class="r"><code>#predict on test data
pred2.y &lt;- predict(model, pre17.testing.df2, type = &quot;prob&quot;)[,2]

q1 &lt;- sum(pre17.training.df$Preeclampsia == &quot;yes&quot;) / length(pre17.training.df$Preeclampsia)
r1 &lt;- sum(resampled.training.df$Preeclampsia == &quot;yes&quot;) / length(resampled.training.df$Preeclampsia)

reweighted.probs2 &lt;- sapply(pred2.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test2.y &lt;- as.numeric(pre17.testing.df2$Preeclampsia) -1

pROC::auc(test2.y, reweighted.probs2)
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases
Area under the curve: 0.6346</code></pre>
<pre class="r"><code>
FNR(reweighted.probs2, test2.y)
     class.pred
truth  0  1
    0 10  3
    1  7  1
[1] 0.875</code></pre>
<pre class="r"><code>FPR(reweighted.probs2, test2.y)
     class.pred
truth  0  1
    0 10  3
    1  7  1
[1] 0.2307692</code></pre>
<pre class="r"><code># Add to output
res.testing2 &lt;- rbind.data.frame(res.testing2, c(pROC::auc(test2.y, reweighted.probs2), FNR(reweighted.probs2, test2.y), FPR(reweighted.probs2, test2.y)))
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>rownames(res.testing2)[nrow(res.testing2)] &lt;- &#39;SVMRadial&#39;</code></pre>
</div>
<div id="svmpoly-support-vector-machines-with-polynomial-kernel" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> svmPoly (Support Vector Machines with Polynomial Kernel)</h2>
<pre class="r"><code>modelLookup(&quot;svmPoly&quot;)
    model parameter             label forReg forClass probModel
1 svmPoly    degree Polynomial Degree   TRUE     TRUE      TRUE
2 svmPoly     scale             Scale   TRUE     TRUE      TRUE
3 svmPoly         C              Cost   TRUE     TRUE      TRUE</code></pre>
<pre class="r"><code>train_control = trainControl(method = &quot;cv&quot;, number = 10, search = &quot;grid&quot;,
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
svmgrid &lt;-  expand.grid(degree = c(2,3,4,5),
                        scale = c(3,4,5),
                        C = c(0.1,0.5,1,3,5,10)
                        )

# training a svm with poly kernel classifier tree model while tuning parameters
model = caret::train(Preeclampsia~., data = resampled.training.df,
              method = &quot;svmPoly&quot;,
              trControl = train_control,
              metric = &quot;ROC&quot;,
              tuneGrid = svmgrid)

print(model)
Support Vector Machines with Polynomial Kernel 

 72 samples
912 predictors
  2 classes: &#39;no&#39;, &#39;yes&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 64, 65, 65, 65, 65, 65, ... 
Resampling results across tuning parameters:

  degree  scale  C     ROC        Sens       Spec 
  2       3       0.1  0.9166667  0.7250000  0.875
  2       3       0.5  0.9166667  0.7333333  0.875
  2       3       1.0  0.9166667  0.7833333  0.875
  2       3       3.0  0.9166667  0.7833333  0.875
  2       3       5.0  0.9166667  0.7833333  0.875
  2       3      10.0  0.9166667  0.7583333  0.875
  2       4       0.1  0.9166667  0.7583333  0.875
  2       4       0.5  0.9166667  0.7333333  0.875
  2       4       1.0  0.9166667  0.7583333  0.875
  2       4       3.0  0.9166667  0.7833333  0.875
  2       4       5.0  0.9166667  0.7583333  0.875
  2       4      10.0  0.9166667  0.7833333  0.875
  2       5       0.1  0.9166667  0.7916667  0.875
  2       5       0.5  0.9166667  0.7250000  0.875
  2       5       1.0  0.9166667  0.7833333  0.875
  2       5       3.0  0.9166667  0.7833333  0.875
  2       5       5.0  0.9166667  0.7333333  0.875
  2       5      10.0  0.9166667  0.7833333  0.875
  3       3       0.1  0.9833333  0.8833333  0.950
  3       3       0.5  0.9833333  0.8500000  0.950
  3       3       1.0  0.9833333  0.8833333  0.950
  3       3       3.0  0.9833333  0.8500000  0.950
  3       3       5.0  0.9833333  0.8166667  0.950
  3       3      10.0  0.9833333  0.8750000  0.950
  3       4       0.1  0.9833333  0.8750000  0.950
  3       4       0.5  0.9833333  0.8500000  0.950
  3       4       1.0  0.9833333  0.8500000  0.950
  3       4       3.0  0.9833333  0.8833333  0.950
  3       4       5.0  0.9833333  0.8833333  0.950
  3       4      10.0  0.9833333  0.9083333  0.950
  3       5       0.1  0.9833333  0.8833333  0.950
  3       5       0.5  0.9833333  0.8500000  0.950
  3       5       1.0  0.9833333  0.8583333  0.950
  3       5       3.0  0.9833333  0.9083333  0.950
  3       5       5.0  0.9833333  0.8500000  0.950
  3       5      10.0  0.9833333  0.9083333  0.950
  4       3       0.1  0.9416667  0.7583333  0.925
  4       3       0.5  0.9416667  0.8250000  0.900
  4       3       1.0  0.9416667  0.7000000  0.925
  4       3       3.0  0.9416667  0.7000000  0.925
  4       3       5.0  0.9416667  0.7250000  0.900
  4       3      10.0  0.9416667  0.7250000  0.900
  4       4       0.1  0.9416667  0.7250000  0.925
  4       4       0.5  0.9416667  0.7250000  0.925
  4       4       1.0  0.9416667  0.7416667  0.925
  4       4       3.0  0.9416667  0.6500000  0.925
  4       4       5.0  0.9416667  0.7833333  0.900
  4       4      10.0  0.9416667  0.7250000  0.925
  4       5       0.1  0.9416667  0.7583333  0.900
  4       5       0.5  0.9416667  0.7000000  0.900
  4       5       1.0  0.9416667  0.7000000  0.925
  4       5       3.0  0.9416667  0.7250000  0.925
  4       5       5.0  0.9416667  0.7500000  0.925
  4       5      10.0  0.9416667  0.7583333  0.900
  5       3       0.1  0.9750000  0.7916667  0.925
  5       3       0.5  0.9750000  0.7333333  0.950
  5       3       1.0  0.9750000  0.7666667  0.950
  5       3       3.0  0.9750000  0.7916667  0.950
  5       3       5.0  0.9750000  0.7666667  0.950
  5       3      10.0  0.9750000  0.7583333  0.950
  5       4       0.1  0.9750000  0.8000000  0.925
  5       4       0.5  0.9750000  0.7666667  0.950
  5       4       1.0  0.9750000  0.7250000  0.950
  5       4       3.0  0.9750000  0.7666667  0.950
  5       4       5.0  0.9750000  0.7666667  0.925
  5       4      10.0  0.9750000  0.7666667  0.950
  5       5       0.1  0.9750000  0.7916667  0.925
  5       5       0.5  0.9750000  0.8250000  0.950
  5       5       1.0  0.9750000  0.7666667  0.950
  5       5       3.0  0.9750000  0.8250000  0.950
  5       5       5.0  0.9750000  0.7333333  0.950
  5       5      10.0  0.9750000  0.7666667  0.925

ROC was used to select the optimal model using the largest value.
The final values used for the model were degree = 3, scale = 3 and C = 0.1.</code></pre>
<pre class="r"><code>plot(model)</code></pre>
<p><img src="05_pr_pe_files/figure-html/unnamed-chunk-54-1.png" width="100%"  class="widefigure" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#predict on test data
pred1.y &lt;- predict(model, pre17.testing.df1, type = &quot;prob&quot;)[,2]

q1 &lt;- sum(pre17.training.df$Preeclampsia == &quot;yes&quot;) / length(pre17.training.df$Preeclampsia)
r1 &lt;- sum(resampled.training.df$Preeclampsia == &quot;yes&quot;) / length(resampled.training.df$Preeclampsia)

reweighted.probs1 &lt;- sapply(pred1.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test1.y &lt;- as.numeric(pre17.testing.df1$Preeclampsia)-1

pROC::auc(test1.y, reweighted.probs1)
Setting levels: control = 0, case = 1
Setting direction: controls &lt; cases
Area under the curve: 0.5637</code></pre>
<pre class="r"><code>
FNR(reweighted.probs1, test1.y)
     class.pred
truth  0  1
    0 48 13
    1 21  5
[1] 0.8076923</code></pre>
<pre class="r"><code>FPR(reweighted.probs1, test1.y)
     class.pred
truth  0  1
    0 48 13
    1 21  5
[1] 0.2131148</code></pre>
<pre class="r"><code># Add to output
res.testing1 &lt;- rbind.data.frame(res.testing1, c(pROC::auc(test1.y, reweighted.probs1), FNR(reweighted.probs1, test1.y), FPR(reweighted.probs1, test1.y)))
Setting levels: control = 0, case = 1
Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>rownames(res.testing1)[nrow(res.testing1)] &lt;- &#39;SVMPoly&#39;</code></pre>
<pre class="r"><code>#predict on test data
pred2.y &lt;- predict(model, pre17.testing.df2, type = &quot;prob&quot;)[,2]

q1 &lt;- sum(pre17.training.df$Preeclampsia == &quot;yes&quot;) / length(pre17.training.df$Preeclampsia)
r1 &lt;- sum(resampled.training.df$Preeclampsia == &quot;yes&quot;) / length(resampled.training.df$Preeclampsia)

reweighted.probs2 &lt;- sapply(pred2.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test2.y &lt;- as.numeric(pre17.testing.df2$Preeclampsia) -1

pROC::auc(test2.y, reweighted.probs2)
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases
Area under the curve: 0.5481</code></pre>
<pre class="r"><code>
FNR(reweighted.probs2, test2.y)
     class.pred
truth  0  1
    0 10  3
    1  7  1
[1] 0.875</code></pre>
<pre class="r"><code>FPR(reweighted.probs2, test2.y)
     class.pred
truth  0  1
    0 10  3
    1  7  1
[1] 0.2307692</code></pre>
<pre class="r"><code># Add to output
res.testing2 &lt;- rbind.data.frame(res.testing2, c(pROC::auc(test2.y, reweighted.probs2), FNR(reweighted.probs2, test2.y), FPR(reweighted.probs2, test2.y)))
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>rownames(res.testing2)[nrow(res.testing2)] &lt;- &#39;SVMPoly&#39;</code></pre>
</div>
</div>
<div id="random-forest" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Random forest</h1>
<pre class="r"><code>modelLookup(&quot;rf&quot;)
  model parameter                         label forReg forClass probModel
1    rf      mtry #Randomly Selected Predictors   TRUE     TRUE      TRUE</code></pre>
<pre class="r"><code>train_control = trainControl(method = &quot;cv&quot;, number = 10, search = &quot;grid&quot;,
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
rfgrid &lt;-  expand.grid(mtry = c(1:10) #only parameter you can tune for rf in R
                        )
              
# training a randomForest classifier tree model while tuning parameters
model = caret::train(Preeclampsia~., data = pre17.training.df,
              method = &quot;rf&quot;,
              trControl = train_control,
              metric = &quot;ROC&quot;,
              importance = T,
              #manually set
              ntree = 250, 
              #maxnodes = 30,
              nodesize = 1, #default for classification
              tuneGrid = rfgrid)

print(model)
Random Forest 

 90 samples
912 predictors
  2 classes: &#39;no&#39;, &#39;yes&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 80, 81, 81, 82, 80, 81, ... 
Resampling results across tuning parameters:

  mtry  ROC        Sens       Spec      
   1    0.8051587  1.0000000  0.00000000
   2    0.7734127  0.9857143  0.03333333
   3    0.8186508  0.9690476  0.03333333
   4    0.8071429  0.9857143  0.03333333
   5    0.8003968  0.9690476  0.03333333
   6    0.7932540  0.9690476  0.03333333
   7    0.7928571  0.9690476  0.00000000
   8    0.8039683  0.9690476  0.06666667
   9    0.8293651  0.9690476  0.05000000
  10    0.8190476  0.9357143  0.06666667

ROC was used to select the optimal model using the largest value.
The final value used for the model was mtry = 9.</code></pre>
<pre class="r"><code>plot(model)</code></pre>
<p><img src="05_pr_pe_files/figure-html/unnamed-chunk-61-1.png" width="100%"  class="widefigure" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#predict on test data
pred1.y &lt;- predict(model, pre17.testing.df1, type = &quot;prob&quot;)[,2]

# out of sample performance metrics
test1.y &lt;- as.numeric(pre17.testing.df1$Preeclampsia)-1

pROC::auc(test1.y, pred1.y)
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases
Area under the curve: 0.5498</code></pre>
<pre class="r"><code>
FNR(pred1.y, test1.y)
     class.pred
truth  0  1
    0 43 18
    1 20  6
[1] 0.7692308</code></pre>
<pre class="r"><code>FPR(pred1.y, test1.y)
     class.pred
truth  0  1
    0 43 18
    1 20  6
[1] 0.295082</code></pre>
<pre class="r"><code># Add to output
res.testing1 &lt;- rbind.data.frame(res.testing1, c(pROC::auc(test1.y, pred1.y), FNR(pred1.y, test1.y), FPR(pred1.y, test1.y)))
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>rownames(res.testing1)[nrow(res.testing1)] &lt;- &#39;RandomForest&#39;</code></pre>
<pre class="r"><code>#predict on test data
pred2.y &lt;- predict(model, pre17.testing.df2, type = &quot;prob&quot;)[,2]

# out of sample performance metrics
test2.y &lt;- as.numeric(pre17.testing.df2$Preeclampsia) -1

pROC::auc(test2.y, pred2.y)
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases
Area under the curve: 0.625</code></pre>
<pre class="r"><code>
FNR(pred2.y, test2.y)
     class.pred
truth 0 1
    0 8 5
    1 6 2
[1] 0.75</code></pre>
<pre class="r"><code>FPR(pred2.y, test2.y)
     class.pred
truth 0 1
    0 8 5
    1 6 2
[1] 0.3846154</code></pre>
<pre class="r"><code># Add to output
res.testing2 &lt;- rbind.data.frame(res.testing2, c(pROC::auc(test2.y, pred2.y), FNR(pred2.y, test2.y), FPR(pred2.y, test2.y)))
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>rownames(res.testing2)[nrow(res.testing2)] &lt;- &#39;RandomForest&#39;</code></pre>
<pre class="r"><code>#feature_importance &lt;- randomForest::importance(model)
#sorted_importance &lt;- feature_importance[order(-feature_importance[, #&quot;MeanDecreaseGini&quot;]),                                        ]
#print(sorted_importance)</code></pre>
</div>
<div id="elastic-net" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Elastic net</h1>
<pre class="r"><code>modelLookup(&quot;glmnet&quot;)
   model parameter                    label forReg forClass probModel
1 glmnet     alpha        Mixing Percentage   TRUE     TRUE      TRUE
2 glmnet    lambda Regularization Parameter   TRUE     TRUE      TRUE</code></pre>
<pre class="r"><code>train_control = trainControl(method = &quot;cv&quot;, number = 10, search = &quot;grid&quot;,
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
netgrid &lt;-  expand.grid(alpha = c(0, 0.01, 0.015, 0.1, 0.2, 0.5, 0.7, 1),lambda = c(0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.7,1))


# auc = 0.63, 0.47. alpha = c(0),lambda = c(0.7)

# auc = 0.59. alpha = c(0, 0.005, 0.01, 0.015, 0.1, 0.2, 0.5, 0.7, 1),lambda = c(0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5,1). alpha = 0.005 and lambda = 1.
# auc = 0.70. lambda = c(0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5). alpha = 0.005 and lambda = 0.5.
# auc = 0.68. alpha = c(0.01, 0.015, 0.1, 0.2, 0.5, 0.7, 1) alpha = 0.01 and lambda = 0.5.
# auc = 0.70. alpha = c(0, 0.005, 0.01, 0.015, 0.1, 0.2, 0.5), lambda = c(0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4). alpha = 0.005 and lambda = 0.4.

# training a elastic net classifier tree model while tuning parameters
model = caret::train(Preeclampsia~., data = pre17.training.df,
              method = &quot;glmnet&quot;,
              trControl = train_control,
              metric = &quot;ROC&quot;,
              tuneGrid = netgrid)

# summarizing the results
print(model)
glmnet 

 90 samples
912 predictors
  2 classes: &#39;no&#39;, &#39;yes&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 80, 81, 81, 82, 80, 81, ... 
Resampling results across tuning parameters:

  alpha  lambda  ROC        Sens       Spec      
  0.000  0.01    0.9138889  0.9404762  0.33333333
  0.000  0.10    0.9138889  0.9404762  0.33333333
  0.000  0.15    0.9138889  0.9404762  0.33333333
  0.000  0.20    0.9138889  0.9404762  0.33333333
  0.000  0.25    0.9138889  0.9404762  0.33333333
  0.000  0.30    0.9138889  0.9404762  0.33333333
  0.000  0.35    0.9138889  0.9404762  0.33333333
  0.000  0.40    0.9138889  0.9404762  0.33333333
  0.000  0.50    0.9138889  0.9404762  0.33333333
  0.000  0.70    0.9138889  0.9404762  0.33333333
  0.000  1.00    0.9138889  0.9404762  0.33333333
  0.010  0.01    0.9309524  0.9119048  0.53333333
  0.010  0.10    0.9309524  0.9119048  0.53333333
  0.010  0.15    0.9309524  0.9119048  0.53333333
  0.010  0.20    0.9309524  0.9261905  0.53333333
  0.010  0.25    0.9309524  0.9261905  0.53333333
  0.010  0.30    0.9309524  0.9261905  0.53333333
  0.010  0.35    0.9126984  0.9261905  0.48333333
  0.010  0.40    0.9126984  0.9261905  0.48333333
  0.010  0.50    0.9174603  0.9261905  0.48333333
  0.010  0.70    0.9119048  0.9404762  0.43333333
  0.010  1.00    0.9055556  0.9404762  0.36666667
  0.015  0.01    0.9253968  0.9119048  0.53333333
  0.015  0.10    0.9253968  0.9119048  0.53333333
  0.015  0.15    0.9253968  0.9261905  0.53333333
  0.015  0.20    0.9253968  0.9261905  0.53333333
  0.015  0.25    0.9253968  0.9261905  0.53333333
  0.015  0.30    0.9126984  0.9261905  0.48333333
  0.015  0.35    0.9126984  0.9261905  0.48333333
  0.015  0.40    0.9174603  0.9261905  0.48333333
  0.015  0.50    0.9174603  0.9404762  0.43333333
  0.015  0.70    0.9035714  0.9404762  0.40000000
  0.015  1.00    0.8738095  0.9547619  0.36666667
  0.100  0.01    0.9206349  0.9261905  0.58333333
  0.100  0.10    0.9261905  0.9261905  0.50000000
  0.100  0.15    0.9190476  0.9404762  0.45000000
  0.100  0.20    0.9051587  0.9690476  0.40000000
  0.100  0.25    0.8912698  0.9690476  0.40000000
  0.100  0.30    0.8726190  0.9690476  0.31666667
  0.100  0.35    0.8670635  0.9547619  0.31666667
  0.100  0.40    0.8670635  0.9547619  0.26666667
  0.100  0.50    0.8305556  0.9547619  0.13333333
  0.100  0.70    0.7861111  0.9857143  0.06666667
  0.100  1.00    0.7587302  0.9857143  0.00000000
  0.200  0.01    0.9107143  0.9261905  0.66666667
  0.200  0.10    0.8829365  0.9404762  0.48333333
  0.200  0.15    0.8738095  0.9547619  0.31666667
  0.200  0.20    0.8527778  0.9547619  0.26666667
  0.200  0.25    0.8174603  0.9547619  0.13333333
  0.200  0.30    0.7936508  0.9547619  0.10000000
  0.200  0.35    0.7797619  0.9547619  0.06666667
  0.200  0.40    0.7694444  0.9714286  0.06666667
  0.200  0.50    0.7126984  0.9857143  0.00000000
  0.200  0.70    0.6817460  1.0000000  0.00000000
  0.200  1.00    0.4916667  1.0000000  0.00000000
  0.500  0.01    0.8690476  0.9119048  0.50000000
  0.500  0.10    0.7785714  0.9095238  0.13333333
  0.500  0.15    0.7440476  0.9547619  0.10000000
  0.500  0.20    0.7115079  0.9690476  0.00000000
  0.500  0.25    0.6662698  1.0000000  0.00000000
  0.500  0.30    0.6912698  1.0000000  0.00000000
  0.500  0.35    0.5126984  1.0000000  0.00000000
  0.500  0.40    0.4916667  1.0000000  0.00000000
  0.500  0.50    0.5000000  1.0000000  0.00000000
  0.500  0.70    0.5000000  1.0000000  0.00000000
  0.500  1.00    0.5000000  1.0000000  0.00000000
  0.700  0.01    0.8424603  0.9261905  0.50000000
  0.700  0.10    0.7142857  0.8785714  0.13333333
  0.700  0.15    0.6924603  0.9690476  0.00000000
  0.700  0.20    0.6757937  1.0000000  0.00000000
  0.700  0.25    0.5126984  1.0000000  0.00000000
  0.700  0.30    0.5000000  1.0000000  0.00000000
  0.700  0.35    0.5000000  1.0000000  0.00000000
  0.700  0.40    0.5000000  1.0000000  0.00000000
  0.700  0.50    0.5000000  1.0000000  0.00000000
  0.700  0.70    0.5000000  1.0000000  0.00000000
  0.700  1.00    0.5000000  1.0000000  0.00000000
  1.000  0.01    0.8126984  0.8785714  0.48333333
  1.000  0.10    0.6646825  0.9380952  0.00000000
  1.000  0.15    0.6805556  1.0000000  0.00000000
  1.000  0.20    0.4916667  1.0000000  0.00000000
  1.000  0.25    0.5000000  1.0000000  0.00000000
  1.000  0.30    0.5000000  1.0000000  0.00000000
  1.000  0.35    0.5000000  1.0000000  0.00000000
  1.000  0.40    0.5000000  1.0000000  0.00000000
  1.000  0.50    0.5000000  1.0000000  0.00000000
  1.000  0.70    0.5000000  1.0000000  0.00000000
  1.000  1.00    0.5000000  1.0000000  0.00000000

ROC was used to select the optimal model using the largest value.
The final values used for the model were alpha = 0.01 and lambda = 0.3.</code></pre>
<pre class="r"><code>plot(model)</code></pre>
<p><img src="05_pr_pe_files/figure-html/unnamed-chunk-69-1.png" width="100%"  class="widefigure" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#predict on test data
pred1.y &lt;- predict(model, pre17.testing.df1, type = &quot;prob&quot;)[,2]

# out of sample performance metrics
test1.y &lt;- as.numeric(pre17.testing.df1$Preeclampsia)-1

pROC::auc(test1.y, pred1.y)
Setting levels: control = 0, case = 1
Setting direction: controls &lt; cases
Area under the curve: 0.5151</code></pre>
<pre class="r"><code>
FNR(pred1.y, test1.y)
     class.pred
truth  0  1
    0 52  9
    1 21  5
[1] 0.8076923</code></pre>
<pre class="r"><code>FPR(pred1.y, test1.y)
     class.pred
truth  0  1
    0 52  9
    1 21  5
[1] 0.147541</code></pre>
<pre class="r"><code># Add to output
res.testing1 &lt;- rbind.data.frame(res.testing1, c(pROC::auc(test1.y, pred1.y), FNR(pred1.y, test1.y), FPR(pred1.y, test1.y)))
Setting levels: control = 0, case = 1
Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>rownames(res.testing1)[nrow(res.testing1)] &lt;- &#39;ElasticNet&#39;</code></pre>
<pre class="r"><code>#predict on test data
pred2.y &lt;- predict(model, pre17.testing.df2, type = &quot;prob&quot;)[,2]

# out of sample performance metrics
test2.y &lt;- as.numeric(pre17.testing.df2$Preeclampsia) -1

pROC::auc(test2.y, pred2.y)
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases
Area under the curve: 0.5</code></pre>
<pre class="r"><code>
FNR(pred2.y, test2.y)
     class.pred
truth 0 1
    0 8 5
    1 6 2
[1] 0.75</code></pre>
<pre class="r"><code>FPR(pred2.y, test2.y)
     class.pred
truth 0 1
    0 8 5
    1 6 2
[1] 0.3846154</code></pre>
<pre class="r"><code># Add to output
res.testing2 &lt;- rbind.data.frame(res.testing2, c(pROC::auc(test2.y, pred2.y), FNR(pred2.y, test2.y), FPR(pred2.y, test2.y)))
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>rownames(res.testing2)[nrow(res.testing2)] &lt;- &#39;ElasticNet&#39;</code></pre>
</div>
<div id="keras-nn" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Keras NN</h1>
<pre class="r"><code>library(tensorflow)
library(keras)
library(tfruns)
tensorflow::set_random_seed(111)</code></pre>
<pre class="r"><code>#c(c(x_train, y_train), c(x_test, y_test)) %&lt;-% keras::dataset_mnist()
#x_train &lt;- x_train / 255
#x_test &lt;-  x_test / 255</code></pre>
<pre class="r"><code>x_train &lt;- as.matrix(pre17.training.df[,-1])
y_train &lt;- as.matrix(as.numeric(pre17.training.df$Preeclampsia)-1)

x_test1&lt;- as.matrix(pre17.testing.df1[,-1])
y_test1 &lt;- as.matrix(as.numeric(pre17.testing.df1$Preeclampsia)-1)

x_test2&lt;- as.matrix(pre17.testing.df2[,-1])
y_test2 &lt;- as.matrix(as.numeric(pre17.testing.df2$Preeclampsia)-1)

x_train_shape &lt;- length(colnames(x_train))</code></pre>
<pre class="r"><code>#constraint_maxnorm(max_value = 2, axis = 0)
# bias_regularizer = regularizer_l2(0.01)
model &lt;- keras_model_sequential()
model %&gt;%
  layer_dense(units = 1000, activation = &#39;relu&#39;,
              input_shape = c(x_train_shape),
              kernel_regularizer = regularizer_l1_l2(l1 = 0.0000001, l2 = 0.000001),
              bias_regularizer = regularizer_l1_l2(l1 = 0.00001, l2 = 0.0001),
              kernel_constraint =constraint_maxnorm(max_value = 2, axis = 0),
              #bias_constraint =constraint_maxnorm(max_value = 3, axis = 0),
              activity_regularizer= regularizer_l1_l2(l1 = 0.01, l2 = 0.00001),
              ) %&gt;%  
  layer_dropout(rate = 0.7) %&gt;% 
  layer_batch_normalization() %&gt;%
  layer_dense(units = 350, activation = &#39;relu&#39;,
              kernel_regularizer = regularizer_l1_l2(l1 = 0.1, l2 = 0.1),
              kernel_constraint = constraint_minmaxnorm(max_value = 2, min_value = 0, axis = 1),
              bias_regularizer = regularizer_l1_l2(l1 = 0.00001, l2 = 0.000001),
              #bias_constraint =constraint_maxnorm(max_value = 3, axis = 0),
              activity_regularizer = regularizer_l1_l2(l1 = 0.1, l2 = 0.000001),
              ) %&gt;%
  layer_dropout(rate = 0.3) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dense(units = 125, activation = &#39;relu&#39;,
              kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01),
              kernel_constraint = constraint_minmaxnorm(max_value = 2, min_value = 0, axis = 1),
              #bias_regularizer = regularizer_l1_l2(l1 = 0.00001, l2 = 0.000001),
              #bias_constraint =constraint_maxnorm(max_value = 3, axis = 0),
              activity_regularizer = regularizer_l1_l2(l1 = 0.001, l2 = 0.000001),
              ) %&gt;%
  layer_dropout(rate = 0.3) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dense(units = 1, activation = &#39;sigmoid&#39;)
  </code></pre>
<pre class="r"><code>loss_fn &lt;- loss_binary_crossentropy()
auc &lt;- metric_auc()
adam &lt;- optimizer_adam(learning_rate = 0.0001, ema_momentum = 0.8)

model %&gt;% compile(
  optimizer = adam,
  loss = loss_fn,
  metrics = &quot;AUC&quot;
)</code></pre>
<pre class="r"><code>model %&gt;% fit(x_train, y_train, epochs = 75, batch_size =3)
Epoch 1/75
30/30 - 1s - loss: 1212.7374 - auc: 0.3586 - 1s/epoch - 34ms/step
Epoch 2/75
30/30 - 0s - loss: 1121.8818 - auc: 0.4343 - 142ms/epoch - 5ms/step
Epoch 3/75
30/30 - 0s - loss: 1040.8395 - auc: 0.4713 - 121ms/epoch - 4ms/step
Epoch 4/75
30/30 - 0s - loss: 965.6377 - auc: 0.3636 - 131ms/epoch - 4ms/step
Epoch 5/75
30/30 - 0s - loss: 895.5657 - auc: 0.5098 - 121ms/epoch - 4ms/step
Epoch 6/75
30/30 - 0s - loss: 829.4556 - auc: 0.4574 - 126ms/epoch - 4ms/step
Epoch 7/75
30/30 - 0s - loss: 767.9225 - auc: 0.4946 - 122ms/epoch - 4ms/step
Epoch 8/75
30/30 - 0s - loss: 710.6428 - auc: 0.5707 - 127ms/epoch - 4ms/step
Epoch 9/75
30/30 - 0s - loss: 657.9073 - auc: 0.4880 - 129ms/epoch - 4ms/step
Epoch 10/75
30/30 - 0s - loss: 608.5178 - auc: 0.4233 - 125ms/epoch - 4ms/step
Epoch 11/75
30/30 - 0s - loss: 562.5856 - auc: 0.5884 - 122ms/epoch - 4ms/step
Epoch 12/75
30/30 - 0s - loss: 519.3457 - auc: 0.5739 - 129ms/epoch - 4ms/step
Epoch 13/75
30/30 - 0s - loss: 478.9791 - auc: 0.4164 - 127ms/epoch - 4ms/step
Epoch 14/75
30/30 - 0s - loss: 441.3080 - auc: 0.5975 - 121ms/epoch - 4ms/step
Epoch 15/75
30/30 - 0s - loss: 406.0068 - auc: 0.5660 - 125ms/epoch - 4ms/step
Epoch 16/75
30/30 - 0s - loss: 373.0776 - auc: 0.5748 - 122ms/epoch - 4ms/step
Epoch 17/75
30/30 - 0s - loss: 342.4595 - auc: 0.6089 - 119ms/epoch - 4ms/step
Epoch 18/75
30/30 - 0s - loss: 314.3165 - auc: 0.5874 - 124ms/epoch - 4ms/step
Epoch 19/75
30/30 - 0s - loss: 288.8100 - auc: 0.5211 - 120ms/epoch - 4ms/step
Epoch 20/75
30/30 - 0s - loss: 265.2379 - auc: 0.5518 - 126ms/epoch - 4ms/step
Epoch 21/75
30/30 - 0s - loss: 242.8678 - auc: 0.6733 - 127ms/epoch - 4ms/step
Epoch 22/75
30/30 - 0s - loss: 221.9369 - auc: 0.5379 - 124ms/epoch - 4ms/step
Epoch 23/75
30/30 - 0s - loss: 202.2571 - auc: 0.7206 - 123ms/epoch - 4ms/step
Epoch 24/75
30/30 - 0s - loss: 185.0652 - auc: 0.6824 - 123ms/epoch - 4ms/step
Epoch 25/75
30/30 - 0s - loss: 169.9867 - auc: 0.5792 - 124ms/epoch - 4ms/step
Epoch 26/75
30/30 - 0s - loss: 155.4210 - auc: 0.6446 - 141ms/epoch - 5ms/step
Epoch 27/75
30/30 - 0s - loss: 141.7789 - auc: 0.6477 - 126ms/epoch - 4ms/step
Epoch 28/75
30/30 - 0s - loss: 129.8752 - auc: 0.7036 - 124ms/epoch - 4ms/step
Epoch 29/75
30/30 - 0s - loss: 119.5779 - auc: 0.6061 - 119ms/epoch - 4ms/step
Epoch 30/75
30/30 - 0s - loss: 110.8534 - auc: 0.6976 - 123ms/epoch - 4ms/step
Epoch 31/75
30/30 - 0s - loss: 102.5207 - auc: 0.7475 - 119ms/epoch - 4ms/step
Epoch 32/75
30/30 - 0s - loss: 94.7231 - auc: 0.7131 - 119ms/epoch - 4ms/step
Epoch 33/75
30/30 - 0s - loss: 86.2182 - auc: 0.7604 - 121ms/epoch - 4ms/step
Epoch 34/75
30/30 - 0s - loss: 81.3674 - auc: 0.6679 - 126ms/epoch - 4ms/step
Epoch 35/75
30/30 - 0s - loss: 76.1240 - auc: 0.7610 - 119ms/epoch - 4ms/step
Epoch 36/75
30/30 - 0s - loss: 68.9204 - auc: 0.8425 - 120ms/epoch - 4ms/step
Epoch 37/75
30/30 - 0s - loss: 63.8519 - auc: 0.7232 - 121ms/epoch - 4ms/step
Epoch 38/75
30/30 - 0s - loss: 61.2316 - auc: 0.7093 - 121ms/epoch - 4ms/step
Epoch 39/75
30/30 - 0s - loss: 57.3724 - auc: 0.7569 - 122ms/epoch - 4ms/step
Epoch 40/75
30/30 - 0s - loss: 54.2523 - auc: 0.7667 - 125ms/epoch - 4ms/step
Epoch 41/75
30/30 - 0s - loss: 52.9186 - auc: 0.7153 - 126ms/epoch - 4ms/step
Epoch 42/75
30/30 - 0s - loss: 50.7594 - auc: 0.6881 - 121ms/epoch - 4ms/step
Epoch 43/75
30/30 - 0s - loss: 48.7962 - auc: 0.7639 - 127ms/epoch - 4ms/step
Epoch 44/75
30/30 - 0s - loss: 46.0757 - auc: 0.7680 - 120ms/epoch - 4ms/step
Epoch 45/75
30/30 - 0s - loss: 43.4301 - auc: 0.7004 - 119ms/epoch - 4ms/step
Epoch 46/75
30/30 - 0s - loss: 41.2334 - auc: 0.8052 - 117ms/epoch - 4ms/step
Epoch 47/75
30/30 - 0s - loss: 39.7151 - auc: 0.8245 - 119ms/epoch - 4ms/step
Epoch 48/75
30/30 - 0s - loss: 37.7690 - auc: 0.7541 - 125ms/epoch - 4ms/step
Epoch 49/75
30/30 - 0s - loss: 36.6638 - auc: 0.7835 - 120ms/epoch - 4ms/step
Epoch 50/75
30/30 - 0s - loss: 35.3833 - auc: 0.7885 - 120ms/epoch - 4ms/step
Epoch 51/75
30/30 - 0s - loss: 32.9754 - auc: 0.7967 - 126ms/epoch - 4ms/step
Epoch 52/75
30/30 - 0s - loss: 31.4633 - auc: 0.7588 - 121ms/epoch - 4ms/step
Epoch 53/75
30/30 - 0s - loss: 30.6963 - auc: 0.7566 - 118ms/epoch - 4ms/step
Epoch 54/75
30/30 - 0s - loss: 30.7604 - auc: 0.7418 - 127ms/epoch - 4ms/step
Epoch 55/75
30/30 - 0s - loss: 29.6527 - auc: 0.8403 - 123ms/epoch - 4ms/step
Epoch 56/75
30/30 - 0s - loss: 28.5191 - auc: 0.7730 - 120ms/epoch - 4ms/step
Epoch 57/75
30/30 - 0s - loss: 27.9015 - auc: 0.8024 - 117ms/epoch - 4ms/step
Epoch 58/75
30/30 - 0s - loss: 26.7663 - auc: 0.7809 - 123ms/epoch - 4ms/step
Epoch 59/75
30/30 - 0s - loss: 25.2162 - auc: 0.9164 - 127ms/epoch - 4ms/step
Epoch 60/75
30/30 - 0s - loss: 24.0924 - auc: 0.8134 - 123ms/epoch - 4ms/step
Epoch 61/75
30/30 - 0s - loss: 23.9938 - auc: 0.8046 - 119ms/epoch - 4ms/step
Epoch 62/75
30/30 - 0s - loss: 23.9036 - auc: 0.7872 - 123ms/epoch - 4ms/step
Epoch 63/75
30/30 - 0s - loss: 22.9962 - auc: 0.8324 - 121ms/epoch - 4ms/step
Epoch 64/75
30/30 - 0s - loss: 22.4603 - auc: 0.7860 - 122ms/epoch - 4ms/step
Epoch 65/75
30/30 - 0s - loss: 21.6385 - auc: 0.8772 - 119ms/epoch - 4ms/step
Epoch 66/75
30/30 - 0s - loss: 20.5288 - auc: 0.9066 - 123ms/epoch - 4ms/step
Epoch 67/75
30/30 - 0s - loss: 19.7524 - auc: 0.8267 - 124ms/epoch - 4ms/step
Epoch 68/75
30/30 - 0s - loss: 19.2907 - auc: 0.8646 - 120ms/epoch - 4ms/step
Epoch 69/75
30/30 - 0s - loss: 18.7537 - auc: 0.8497 - 120ms/epoch - 4ms/step
Epoch 70/75
30/30 - 0s - loss: 18.3516 - auc: 0.7652 - 122ms/epoch - 4ms/step
Epoch 71/75
30/30 - 0s - loss: 17.9097 - auc: 0.9066 - 121ms/epoch - 4ms/step
Epoch 72/75
30/30 - 0s - loss: 17.5968 - auc: 0.8819 - 119ms/epoch - 4ms/step
Epoch 73/75
30/30 - 0s - loss: 17.1632 - auc: 0.9056 - 115ms/epoch - 4ms/step
Epoch 74/75
30/30 - 0s - loss: 16.6873 - auc: 0.8845 - 117ms/epoch - 4ms/step
Epoch 75/75
30/30 - 0s - loss: 16.1361 - auc: 0.8374 - 118ms/epoch - 4ms/step</code></pre>
<pre class="r"><code>#history &lt;- model %&gt;% fit(
#  x_train,y_train,
#  epochs = 30, batch_size = 256, 
#  validation_split = 0.2
#)

#plot(history)</code></pre>
<p>TESTING 1</p>
<pre class="r"><code>model %&gt;% evaluate(x_test1,  y_test1, verbose = 2)
3/3 - 0s - loss: 17.2937 - auc: 0.7071 - 227ms/epoch - 76ms/step
      loss        auc 
17.2936935  0.7071249 </code></pre>
<p>TESTING 2</p>
<pre class="r"><code>model %&gt;% evaluate(x_test2,  y_test2, verbose = 2)
1/1 - 0s - loss: 21.4332 - auc: 0.4135 - 14ms/epoch - 14ms/step
      loss        auc 
21.4331818  0.4134615 </code></pre>
<pre class="r"><code>#model %&gt;% predict(x_test) %&gt;% k_argmax() #only for softmax
b.1&lt;-model %&gt;% predict(x_test1) #%&gt;% `&gt;`(0.5) %&gt;% k_cast(&quot;int32&quot;) #for sigmoid.0
3/3 - 0s - 77ms/epoch - 26ms/step</code></pre>
<pre class="r"><code>b.1 &lt;- as.numeric(b.1)

b.2&lt;-model %&gt;% predict(x_test2) #%&gt;% `&gt;`(0.5) %&gt;% k_cast(&quot;int32&quot;) #for sigmoid.0
1/1 - 0s - 11ms/epoch - 11ms/step</code></pre>
<pre class="r"><code>b.2 &lt;- as.numeric(b.2)</code></pre>
<p>testing 1</p>
<pre class="r"><code>pROC::auc(as.numeric(y_test1), b.1)
Setting levels: control = 0, case = 1
Setting direction: controls &lt; cases
Area under the curve: 0.7037</code></pre>
<pre class="r"><code>
FNR(b.1, y_test1)
     class.pred
truth  0  1
    0 38 23
    1  7 19
[1] 0.2692308</code></pre>
<pre class="r"><code>FPR(b.1, y_test1)
     class.pred
truth  0  1
    0 38 23
    1  7 19
[1] 0.3770492</code></pre>
<pre class="r"><code># Add to output
res.testing1 &lt;- rbind.data.frame(res.testing1, c(pROC::auc(as.numeric(y_test1), b.1), FNR(b.1, y_test1), FPR(b.1, y_test1)))
Setting levels: control = 0, case = 1
Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>rownames(res.testing1)[nrow(res.testing1)] &lt;- &#39;KerasNN&#39;</code></pre>
<p>testing 2</p>
<pre class="r"><code>pROC::auc(as.numeric(y_test2), b.2)
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases
Area under the curve: 0.6154</code></pre>
<pre class="r"><code>
FNR(b.2, y_test2)
     class.pred
truth 0 1
    0 8 5
    1 6 2
[1] 0.75</code></pre>
<pre class="r"><code>FPR(b.2, y_test2)
     class.pred
truth 0 1
    0 8 5
    1 6 2
[1] 0.3846154</code></pre>
<pre class="r"><code># Add to output
res.testing2 &lt;- rbind.data.frame(res.testing2, c(pROC::auc(as.numeric(y_test2), b.2), FNR(b.2, y_test2), FPR(b.2, y_test2)))
Setting levels: control = 0, case = 1
Setting direction: controls &gt; cases</code></pre>
<pre class="r"><code>rownames(res.testing2)[nrow(res.testing2)] &lt;- &#39;KerasNN&#39;</code></pre>
</div>
<div id="results" class="section level1" number="10">
<h1><span class="header-section-number">10</span> Results</h1>
<pre class="r"><code>(res.testing1)
                   AUC       FNR        FPR
XGBOOST      0.6245271 0.7307692 0.09836066
SVMLinear    0.5529634 0.7307692 0.18032787
SVMRadial    0.4968474 0.7307692 0.19672131
SVMPoly      0.5636822 0.8076923 0.21311475
RandomForest 0.5498108 0.7692308 0.29508197
ElasticNet   0.5151324 0.8076923 0.14754098
KerasNN      0.7036570 0.2692308 0.37704918</code></pre>
<pre class="r"><code>(res.testing2)
                   AUC   FNR       FPR
XGBOOST      0.6250000 0.875 0.3846154
SVMLinear    0.5769231 0.750 0.3076923
SVMRadial    0.6346154 0.875 0.2307692
SVMPoly      0.5480769 0.875 0.2307692
RandomForest 0.6250000 0.750 0.3846154
ElasticNet   0.5000000 0.750 0.3846154
KerasNN      0.6153846 0.750 0.3846154</code></pre>
</div>
<div id="session-information" class="section level1" number="11">
<h1><span class="header-section-number">11</span> Session information</h1>
<pre class="r"><code>sessionInfo()
R version 4.4.0 (2024-04-24)
Platform: x86_64-pc-linux-gnu
Running under: Ubuntu 22.04.4 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=es_ES.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=es_ES.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=es_ES.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=es_ES.UTF-8 LC_IDENTIFICATION=C       

time zone: Europe/Madrid
tzcode source: system (glibc)

attached base packages:
[1] grid      stats4    stats     graphics  grDevices utils     datasets 
[8] methods   base     

other attached packages:
 [1] tfruns_1.5.3                keras_2.15.0               
 [3] tensorflow_2.16.0.9000      DMwR_0.4.1                 
 [5] dplyr_1.1.4                 edgeR_4.2.0                
 [7] limma_3.60.2                SummarizedExperiment_1.34.0
 [9] Biobase_2.64.0              GenomicRanges_1.56.0       
[11] GenomeInfoDb_1.40.1         IRanges_2.38.0             
[13] S4Vectors_0.42.0            BiocGenerics_0.50.0        
[15] MatrixGenerics_1.16.0       matrixStats_1.3.0          
[17] caret_6.0-94                lattice_0.22-5             
[19] ggplot2_3.5.1               kableExtra_1.4.0           
[21] knitr_1.46                  BiocStyle_2.32.0           

loaded via a namespace (and not attached):
  [1] pROC_1.18.5             rlang_1.1.3             magrittr_2.0.3         
  [4] compiler_4.4.0          png_0.1-8               systemfonts_1.0.5      
  [7] vctrs_0.6.5             reshape2_1.4.4          stringr_1.5.1          
 [10] shape_1.4.6             pkgconfig_2.0.3         crayon_1.5.2           
 [13] fastmap_1.2.0           XVector_0.44.0          utf8_1.2.4             
 [16] rmarkdown_2.27          prodlim_2023.08.28      UCSC.utils_1.0.0       
 [19] tinytex_0.49            purrr_1.0.2             glmnet_4.1-8           
 [22] xfun_0.44               randomForest_4.7-1.1    zlibbioc_1.50.0        
 [25] cachem_1.0.8            jsonlite_1.8.8          recipes_1.0.10         
 [28] highr_0.9               DelayedArray_0.30.1     parallel_4.4.0         
 [31] R6_2.5.1                bslib_0.7.0             stringi_1.8.3          
 [34] reticulate_1.37.0       parallelly_1.36.0       rpart_4.1.23           
 [37] lubridate_1.9.3         jquerylib_0.1.4         Rcpp_1.0.12            
 [40] bookdown_0.39           iterators_1.0.14        future.apply_1.11.1    
 [43] zoo_1.8-12              base64enc_0.1-3         Matrix_1.6-5           
 [46] splines_4.4.0           nnet_7.3-19             timechange_0.3.0       
 [49] tidyselect_1.2.1        rstudioapi_0.16.0       abind_1.4-5            
 [52] yaml_2.3.8              timeDate_4032.109       codetools_0.2-19       
 [55] curl_5.2.1              listenv_0.9.1           tibble_3.2.1           
 [58] plyr_1.8.9              quantmod_0.4.26         withr_3.0.0            
 [61] ROCR_1.0-11             evaluate_0.23           future_1.33.1          
 [64] survival_3.5-8          xts_0.13.2              xml2_1.3.6             
 [67] kernlab_0.9-32          pillar_1.9.0            BiocManager_1.30.23    
 [70] whisker_0.4.1           foreach_1.5.2           TTR_0.24.4             
 [73] generics_0.1.3          rprojroot_2.0.4         munsell_0.5.0          
 [76] scales_1.3.0            globals_0.16.2          class_7.3-22           
 [79] glue_1.7.0              tools_4.4.0             data.table_1.15.0      
 [82] locfit_1.5-9.9          ModelMetrics_1.2.2.2    gower_1.0.0            
 [85] ipred_0.9-14            colorspace_2.1-0        nlme_3.1-163           
 [88] GenomeInfoDbData_1.2.12 cli_3.6.2               rappdirs_0.3.3         
 [91] fansi_1.0.6             S4Arrays_1.4.1          viridisLite_0.4.2      
 [94] svglite_2.1.3           lava_1.7.3              gtable_0.3.4           
 [97] zeallot_0.1.0           sass_0.4.9              digest_0.6.35          
[100] SparseArray_1.4.8       htmltools_0.5.8.1       lifecycle_1.0.4        
[103] here_1.0.1              hardhat_1.3.1           httr_1.4.7             
[106] statmod_1.4.37          MASS_7.3-60.0.1        </code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Roskams2022" class="csl-entry">
Roskams-Hieter, Breeshey, Hyun Ji Kim, Pavana Anur, Josiah T. Wagner, Rowan Callahan, Elias Spiliotopoulos, Charles Ward Kirschbaum, et al. 2022. <span>‚ÄúPlasma Cell-Free RNA Profiling Distinguishes Cancers from Pre-Malignant Conditions in Solid and Hematologic Malignancies.‚Äù</span> <em>Npj Precision Oncology</em> 6. <a href="https://doi.org/10.1038/s41698-022-00270-y">https://doi.org/10.1038/s41698-022-00270-y</a>.
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
      styles: {
        ".MathJax_Display": {
           "text-align": "center",
           padding: "0px 150px 0px 65px",
           margin: "0px 0px 0.5em"
        },
        "@media screen and (max-width: 991px)": {
            ".MathJax_Display": {
               "text-align": "center",
               padding: "0 0 0 0"
            }
         }
      }
    }
  });
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<script type="text/javascript">
$(document).ready(function ()  {
  
  // Map "enter" keypress to the same action as a cursor click
  function navigateLink(e) {
    if (e.key === "Enter") {
      $(this).trigger("click");
    }
  }

  var toc_items = document.querySelectorAll(".tocify-item");
  for (var i = 0; i < toc_items.length; i++) {
    // The link role tells screen readers this is for navigation
    toc_items.item(i).setAttribute("role", "link");
    // tabindex = 0 allows selection via keyboard tab presses
    toc_items.item(i).setAttribute("tabindex", "0");
    // Listen for "Enter" keypress when item is selected
    toc_items.item(i).addEventListener("keydown", navigateLink);
  }
});
</script>

</body>
</html>
