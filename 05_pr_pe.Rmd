---
title: Biomarker prediction in preeclampsia data
author:
- name: Berta Canal SimÃ³n
  affiliation:
  - &id Barcelona School of Economics
  email: berta.canal@bse.eu
- name: Adam Olivares Canal
  affiliation: *id
  email: adam.olivares@bse.eu
date: "`r format(Sys.time(), '%B %e, %Y')`"
abstract: >
  Here we perform a prediction of candidate biomarkers in preeclampsia cfRNA sequencing data.
output:
  BiocStyle::html_document:
    toc: true
    toc_float: true
    fig_captions: yes
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: _bibliography.bib
---

```{r setup, echo=FALSE, cache=FALSE}
library(knitr)
library(kableExtra)
library(caret)


knitr::opts_chunk$set(
  collapse=TRUE,
  comment="",
  fig.align="center",
  fig.wide=TRUE,
  cache=TRUE,
  cache.path="_cache/__PR_pe",
  cache.extra=R.version.string,
  autodep=TRUE
)
```

# Importing processed and filtered data

We start by importing the previously filtered and normalized RNA-seq data.

```{r, message=FALSE}
library(SummarizedExperiment)
library(edgeR)

dgeM.filt <- readRDS(file.path("_processed_data", "dgeM.filt.rds"))
seM.filt <- readRDS(file.path("_processed_data", "seM.filt.rds"))
dgeD.filt <- readRDS(file.path("_processed_data", "dgeD.filt.rds"))
seD.filt <- readRDS(file.path("_processed_data", "seD.filt.rds"))
dgeM.filt.training <- readRDS(file.path("_processed_data",
                                        "dgeM.filt.training.rds"))
seM.filt.training <- readRDS(file.path("_processed_data",
                                       "seM.filt.training.rds"))
dgeM.filt.testing <- readRDS(file.path("_processed_data",
                                       "dgeM.filt.testing.rds"))
seM.filt.testing <- readRDS(file.path("_processed_data",
                                      "seM.filt.testing.rds"))
dgeD.filt.subset <- readRDS(file.path("_processed_data",
                                      "dgeD.filt.subset.rds"))
seD.filt.subset <- readRDS(file.path("_processed_data",
                                     "seD.filt.subset.rds"))
DEgenes.trainingM <- readRDS(file.path("_processed_data", 
                                      "DEgenes.trainingM.rds"))
DEgenes.testingM <- readRDS(file.path("_processed_data", 
                                      "DEgenes.testingM.rds"))
DEgenes.testingD <- readRDS(file.path("_processed_data", 
                                      "DEgenes.testingD.rds"))
```

Train-testing subset creation: Intersection between differential expressed genes from training set from @Roskams2022 and lowly expressed genes from testing set.

```{r}
set.seed(111)
intersection.genes <- Reduce(intersect, list(DEgenes.trainingM, rownames(dgeM.filt.testing), rownames(dgeD.filt.subset)))
length(intersection.genes)

#intersection.genes <- intersect(intersect(DEgenes.trainingM, rownames(dgeM.filt.testing)), rownames(dgeD.filt.subset))
#length(intersection.genes)

dgeM.intercept.training <- dgeM.filt.training[intersection.genes,]
dim(dgeM.intercept.training)
seM.intercept.training <- seM.filt.training[intersection.genes,]
dim(seM.intercept.training)

dgeM.intercept.testing <- dgeM.filt.testing[intersection.genes,]
dim(dgeM.intercept.testing)
seM.intercept.testing <- seM.filt.testing[intersection.genes,]
dim(seM.intercept.testing)

dgeD.intercept.testing <- dgeD.filt.subset[intersection.genes,]
dim(dgeD.intercept.testing)
seD.intercept.testing <- seD.filt.subset[intersection.genes,]
dim(seD.intercept.testing)
```
## Pregnancy trimesters

Gestational age is used to classify the samples into its corresponding trimester of pregnancy according to [NIH](https://www.nichd.nih.gov/health/topics/factsheets/pregnancy): first trimester (week 1 to week 12), second trimester (week 13 to week 28) and third trimester (week 29 to week 40).

### Training data

```{r}
table(as.factor(seM.intercept.training$SamplingGAgroup))
```

```{r}
mask <- seM.intercept.training$SamplingGA <= 16
seM.intercept.training$SamplingGAgroup17[mask] <- "Pre17weeks"
dgeM.intercept.training$samples$SamplingGAgroup17[mask] <- "Pre17weeks"

mask <- seM.intercept.training$SamplingGA >= 17
seM.intercept.training$SamplingGAgroup17[mask] <- "Post17weeks"
dgeM.intercept.training$samples$SamplingGAgroup17[mask] <- "Post17weeks"
```

```{r}
table(as.factor(seM.intercept.training$SamplingGAgroup17))
```


### Testing data 1

```{r}
table(as.factor(seM.intercept.testing$SamplingGAgroup))
```


```{r}
mask <- seM.intercept.testing$SamplingGA <= 16
seM.intercept.testing$SamplingGAgroup17[mask] <- "Pre17weeks"
dgeM.intercept.testing$samples$SamplingGAgroup17[mask] <- "Pre17weeks"

mask <- seM.intercept.testing$SamplingGA >= 17
seM.intercept.testing$SamplingGAgroup17[mask] <- "Post17weeks"
dgeM.intercept.testing$samples$SamplingGAgroup17[mask] <- "Post17weeks"
```

```{r}
table(as.factor(seM.intercept.testing$SamplingGAgroup17))
```

### Testing data 2

```{r}
table(as.factor(seD.intercept.testing$SamplingGAgroup))
```

```{r}
mask <- seD.intercept.testing$SamplingGA <= 16
seD.intercept.testing$SamplingGAgroup17[mask] <- "Pre17weeks"
dgeD.intercept.testing$samples$SamplingGAgroup17[mask] <- "Pre17weeks"

mask <- seD.intercept.testing$SamplingGA >= 17
seD.intercept.testing$SamplingGAgroup17[mask] <- "Post17weeks"
dgeD.intercept.testing$samples$SamplingGAgroup17[mask] <- "Post17weeks"
```

```{r}
table(as.factor(seD.intercept.testing$SamplingGAgroup17))
```

## Dataframes creation

### Training data



```{r}
training.df <- data.frame(Preeclampsia = seM.intercept.training$Preeclampsia,
                          SamplingGA = scale(seM.intercept.training$SamplingGA, scale = TRUE, center = TRUE),
                          SamplingGAgroup17 = seM.intercept.training$SamplingGAgroup17,
                          scale(t(assays(seM.intercept.training)$logCPM), scale = TRUE, center = TRUE),
                          MotherID = seM.intercept.training$MotherID
                          )
```



```{r}
mask <- seM.intercept.training$SamplingGAgroup17 == "Pre17weeks"
pre17.training.df <- training.df[mask,]
```


```{r}
library(dplyr)

pre17.training.df <- pre17.training.df %>%
group_by(MotherID) %>%
slice_min(order_by = SamplingGA) %>%
ungroup() %>%
#select(-MotherID, -SamplingGAgroup17) 
select(-MotherID, -SamplingGAgroup17, -SamplingGA)
```


### Testing data 1

Created from Discovery and Validation 2 dataset in the study by @moufarrej2022early.


```{r}
testing.df1 <- data.frame(Preeclampsia = seM.intercept.testing$Preeclampsia,
                          SamplingGA = scale(seM.intercept.testing$SamplingGA, scale = TRUE, center = TRUE),
                          SamplingGAgroup17 = seM.intercept.testing$SamplingGAgroup17,
                          scale(t(assays(seM.intercept.testing)$logCPM), scale = TRUE, center = TRUE),
                          MotherID = seM.intercept.testing$MotherID
                          )

```


```{r}
mask <- seM.intercept.testing$SamplingGAgroup17 == "Pre17weeks"
pre17.testing.df1 <- testing.df1[mask,]
```

```{r}
pre17.testing.df1 <- pre17.testing.df1 %>%
group_by(MotherID) %>%
slice_min(order_by = SamplingGA) %>%
ungroup() %>%
select(-MotherID, -SamplingGAgroup17, -SamplingGA)
```


### Testing data 2

Created from the dataset used in the study by @delvecchio2021cell.


```{r}
testing.df2 <- data.frame(Preeclampsia = seD.intercept.testing$Preeclampsia,
                          SamplingGA = scale(seD.intercept.testing$SamplingGA, scale = TRUE, center = TRUE),
                          SamplingGAgroup17 = seD.intercept.testing$SamplingGAgroup17,
                          scale(t(assays(seD.intercept.testing)$logCPM), scale = TRUE, center = TRUE),
                          MotherID = seD.intercept.testing$MotherID
                          )
```

```{r}
mask <- seD.intercept.testing$SamplingGAgroup17 == "Pre17weeks"
pre17.testing.df2 <- testing.df2[mask,]
```

```{r}
pre17.testing.df2 <- pre17.testing.df2 %>%
group_by(MotherID) %>%
slice_min(order_by = SamplingGA) %>%
ungroup() %>%
select(-MotherID, -SamplingGAgroup17, -SamplingGA)
```

```{r, echo=FALSE}
#Discarded Target encoding for additional variables. Neither of them helped to improve predictions.

#library(dataPreparation)

#target_encoding <- build_target_encoding(
#  training.df,
#  'SamplingGAgroup',
#  'Preeclampsia',
#  functions = "mean",
#  verbose = TRUE
#)

#target_encoding
```


```{r, echo=FALSE}
#training.df <- as.data.frame(target_encode(training.df, target_encoding, drop = TRUE, verbose = TRUE))
#testing.df1 <- as.data.frame(target_encode(testing.df1, target_encoding, drop = TRUE, verbose = TRUE))
#testing.df2 <- as.data.frame(target_encode(testing.df2, target_encoding, drop = TRUE, verbose = TRUE))
```

```{r, echo=FALSE}
#training.df$Preeclampsia <- factor(training.df$Preeclampsia, labels = c('yes', 'no'))
```

# Performance metrics 

Given the application of the current paper, the False Negative Rate (FNR) metric is a particularly relevant metric, since it would imply classifying as healthy an individual with cancer. Therefore, that patients will not receive treatment, which will cause serious consequences. Furthermore, it could also be considered the False Positive Rate (FPR), which results in an undesirable situation where a proportion of healthy individuals are categorized as ill. This would subject a healthy patient to unnecessary treatment and its potential side effects. However, since the expected consequences are not that severe, FNR is prioritized in the analysis.

```{r}
FNR <- function(proba.pred, truth){
  class.pred <- as.numeric(proba.pred > 0.35)
  conf <- table(truth, class.pred)
  print(conf)
  FNR <- conf[2, 1] / sum(conf[2, 1], conf[2, 2])
  return(FNR)
}
```

```{r}
FPR <- function(proba.pred, truth){
  class.pred <- as.numeric(proba.pred > 0.35)
  conf <- table(truth, class.pred)
  print(conf)
  FPR <- conf[1, 2] / sum(conf[1, 1], conf[1, 2])
  return(FPR)
}
```

```{r, include = FALSE}
# Ouput matrix
res.testing1 <- data.frame(
  AUC = NA,
  FNR = NA,
  FPR = NA)
```

```{r, include = FALSE}
# Ouput matrix
res.testing2 <- data.frame(
  AUC = NA,
  FNR = NA,
  FPR = NA)
```


# XGBOOST

```{r}
modelLookup("xgbTree")
```


```{r}
# CV technique which will be passed into the train() function
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

# tuning grid
set.seed(111)

#xgboostGrid <- expand.grid(max_depth = c(3, 4, 5, 6, 7), nrounds = (1:20)*10, eta = c(0.4, 0.5),gamma = c(0.5,1, 1.5),subsample = #c(0.8),min_child_weight = c(2),colsample_bytree = c(0.8))

xgboostGrid <- expand.grid(max_depth = c(3, 4, 5, 6, 7), nrounds = (1:20)*10, eta = c(0.2, 0.3, 0.4, 0.5),gamma = c(0.5,1, 1.5),subsample = c(0.8),min_child_weight = c(2),colsample_bytree = c(0.8))

# hyperparaemeter search for XGboost classifier tree model
model = caret::train(Preeclampsia~., data = pre17.training.df,
              method = "xgbTree",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = xgboostGrid,
              verbosity = 0,
              verbose = FALSE,
              #num.threads = 18 #cores in use
              )

#print(model)
```

```{r}
#predict on test data
pred1.y <- predict(model, pre17.testing.df1, type = "prob")[,2]

# out of sample performance metrics
test1.y <- as.numeric(pre17.testing.df1$Preeclampsia)-1

pROC::auc(test1.y, pred1.y, direction = "<")

FNR(pred1.y, test1.y)
FPR(pred1.y, test1.y)

roc_xgboost_pe_test1 <- ROCit::rocit(score=pred1.y,class=test1.y)
```

```{r, results = "hide"}
# Add to output
res.testing1[1, ] <- c(pROC::auc(test1.y, pred1.y, direction = "<"), FNR(pred1.y, test1.y), FPR(pred1.y, test1.y))
rownames(res.testing1)[nrow(res.testing1)] <- 'XGBOOST'
```


```{r}
#predict on test data
pred2.y <- predict(model, pre17.testing.df2, type = "prob")[,2]

# out of sample performance metrics
test2.y <- as.numeric(pre17.testing.df2$Preeclampsia)-1

pROC::auc(test2.y, pred2.y, direction = "<")

FNR(pred2.y, test2.y)
FPR(pred2.y, test2.y)

roc_xgboost_pe_test2 <- ROCit::rocit(score=pred2.y,class=test2.y)
```

```{r}
# Add to output
res.testing2[1, ] <- c(pROC::auc(test2.y, pred2.y, direction = "<"), FNR(pred2.y, test2.y), FPR(pred2.y, test2.y))
rownames(res.testing2)[nrow(res.testing2)] <- 'XGBOOST'
```

# Class imbalance

Class imbalance is identified since there are 27.5% normotensive and 72.5% preeclampsia pregnancies. Unbalanced problems should be addressed when applying SVM, since it aims to separate the space into two parts. 

```{r, results = "hide"}
#remotes::install_github("cran/DMwR")
```

```{r}
smote_dataset <- as.data.frame(pre17.training.df)
smote_dataset$Preeclampsia <- as.factor(smote_dataset$Preeclampsia)
table(pre17.training.df$Preeclampsia)
```

```{r}
#When perc.over is 100, we create 1 new example (100/100 = 1)
library(DMwR)
set.seed(111)
resampled.training.df <- SMOTE(Preeclampsia ~ ., smote_dataset, perc.over = 70, k =5)
table(resampled.training.df$Preeclampsia)
```

```{r}
reweight <- function(pi, q1, r1) {
  r0 <- 1 - r1
  q0 <- 1 - q1
  tot <- pi * (q1 / r1) + (1 - pi) * (q0 / r0)
  w <- pi * (q1 / r1) / tot
  return(w)
}
```

# SVM models

## svmLinearWeights (linear kernel + class weights)

```{r}
modelLookup("svmLinearWeights")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(50)

svmgrid <-  expand.grid(cost = c(0.0001, 0.001,0.01,0.1,1,3, 5, 10), weight = c(0.01, 0.05, 0.1,0.5, 0.7,1,5,10))


# training a svm classifier with liearn kernel model while tuning parameters
model = caret::train(Preeclampsia~., data = resampled.training.df,
              method = "svmLinearWeights",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

# summarizing the results
#print(model)
```

```{r, include = FALSE}
plot(model)
```

```{r}
#predict on test data
pred1.y <- predict(model, pre17.testing.df1, type = "prob")[,2]

q1 <- sum(pre17.training.df$Preeclampsia == "yes") / length(pre17.training.df$Preeclampsia)
r1 <- sum(resampled.training.df$Preeclampsia == "yes") / length(resampled.training.df$Preeclampsia)

reweighted.probs1 <- sapply(pred1.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test1.y <- as.numeric(pre17.testing.df1$Preeclampsia) -1

pROC::auc(test1.y, reweighted.probs1, direction = "<")

FNR(reweighted.probs1, test1.y)
FPR(reweighted.probs1, test1.y)

roc_SVMlinear_pe_test1 <- ROCit::rocit(score=reweighted.probs1,class=test1.y)
```


```{r, results = "hide"}
# Add to output
res.testing1 <- rbind.data.frame(res.testing1, c(pROC::auc(test1.y, reweighted.probs1, direction = "<"), FNR(reweighted.probs1, test1.y), FPR(reweighted.probs1, test1.y)))
rownames(res.testing1)[nrow(res.testing1)] <- 'SVMLinear'
```


```{r}
#predict on test data
pred2.y <- predict(model, pre17.testing.df2, type = "prob")[,2]

q1 <- sum(pre17.training.df$Preeclampsia == "yes") / length(pre17.training.df$Preeclampsia)
r1 <- sum(resampled.training.df$Preeclampsia == "yes") / length(resampled.training.df$Preeclampsia)

reweighted.probs2 <- sapply(pred2.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test2.y <- as.numeric(pre17.testing.df2$Preeclampsia) -1

pROC::auc(test2.y, reweighted.probs2, direction = "<")

FNR(reweighted.probs2, test2.y)
FPR(reweighted.probs2, test2.y)

roc_SVMlinear_pe_test2 <- ROCit::rocit(score=reweighted.probs2,class=test2.y)
```

```{r, results = "hide"}
# Add to output
res.testing2 <- rbind.data.frame(res.testing2, c(pROC::auc(test2.y, reweighted.probs2, direction = "<"), FNR(reweighted.probs2, test2.y), FPR(reweighted.probs2, test2.y)))
rownames(res.testing2)[nrow(res.testing2)] <- 'SVMLinear'
```

## svmRadial (Support Vector Machines with Radial Basis Function Kernel)

```{r}
modelLookup("svmRadial")
```

```{r, results="hide",}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(50)
# Customzing the tuning grid
svmgrid <-  expand.grid(sigma = c(1, 0.1, 0.01, 0.001, 0.0001, 0.00001,0.000001),
                        C = c(0.01,0.1,1, 5, 10, 50, 80, 90, 100, 105, 110, 200, 1000))

# training a svm with rbf kernel classifier model while tuning parameters
model = caret::train(Preeclampsia~., data = resampled.training.df,
              method = "svmRadial",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

# summarizing the results
#print(model)
```

```{r, include = FALSE}
plot(model)
```


```{r}
#predict on test data
pred1.y <- predict(model, pre17.testing.df1, type = "prob")[,2]

q1 <- sum(pre17.training.df$Preeclampsia == "yes") / length(pre17.training.df$Preeclampsia)
r1 <- sum(resampled.training.df$Preeclampsia == "yes") / length(resampled.training.df$Preeclampsia)

reweighted.probs1 <- sapply(pred1.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test1.y <- as.numeric(pre17.testing.df1$Preeclampsia)-1

pROC::auc(test1.y, reweighted.probs1, direction = "<")

FNR(reweighted.probs1, test1.y)
FPR(reweighted.probs1, test1.y)

roc_SVMlrbf_pe_test1 <- ROCit::rocit(score=reweighted.probs1,class=test1.y)
```

```{r, results = "hide"}
# Add to output
res.testing1 <- rbind.data.frame(res.testing1, c(pROC::auc(test1.y, reweighted.probs1, direction = "<"), FNR(reweighted.probs1, test1.y), FPR(reweighted.probs1, test1.y)))
rownames(res.testing1)[nrow(res.testing1)] <- 'SVMRadial'
```


```{r}
#predict on test data
pred2.y <- predict(model, pre17.testing.df2, type = "prob")[,2]

q1 <- sum(pre17.training.df$Preeclampsia == "yes") / length(pre17.training.df$Preeclampsia)
r1 <- sum(resampled.training.df$Preeclampsia == "yes") / length(resampled.training.df$Preeclampsia)

reweighted.probs2 <- sapply(pred2.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test2.y <- as.numeric(pre17.testing.df2$Preeclampsia) -1

pROC::auc(test2.y, pred2.y, direction = "<")

FNR(pred2.y, test2.y)
FPR(pred2.y, test2.y)

roc_SVMlrbf_pe_test2 <- ROCit::rocit(score=reweighted.probs2,class=test2.y)
```

```{r, results = "hide"}
# Add to output
res.testing2 <- rbind.data.frame(res.testing2, c(pROC::auc(test2.y, reweighted.probs2, direction = "<"), FNR(reweighted.probs2, test2.y), FPR(reweighted.probs2, test2.y)))
rownames(res.testing2)[nrow(res.testing2)] <- 'SVMRadial'
```

## svmPoly (Support Vector Machines with Polynomial Kernel)

```{r}
modelLookup("svmPoly")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
svmgrid <-  expand.grid(degree = c(2,3,4,5),
                        scale = c(2,3,4,5),
                        C = c(0.001,0.01,0.1,0.5,1,3)
                        )

# training a svm with poly kernel classifier tree model while tuning parameters
model = caret::train(Preeclampsia~., data = resampled.training.df,
              method = "svmPoly",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = svmgrid)

#print(model)
```

```{r, include = FALSE}
plot(model)
```

```{r}
#predict on test data
pred1.y <- predict(model, pre17.testing.df1, type = "prob")[,2]

q1 <- sum(pre17.training.df$Preeclampsia == "yes") / length(pre17.training.df$Preeclampsia)
r1 <- sum(resampled.training.df$Preeclampsia == "yes") / length(resampled.training.df$Preeclampsia)

reweighted.probs1 <- sapply(pred1.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test1.y <- as.numeric(pre17.testing.df1$Preeclampsia)-1

pROC::auc(test1.y, reweighted.probs1, direction = "<")

FNR(reweighted.probs1, test1.y)
FPR(reweighted.probs1, test1.y)

roc_SVMpoly_pe_test1 <- ROCit::rocit(score=reweighted.probs1,class=test1.y)
```

```{r, results = "hide"}
# Add to output
res.testing1 <- rbind.data.frame(res.testing1, c(pROC::auc(test1.y, reweighted.probs1, direction = "<"), FNR(reweighted.probs1, test1.y), FPR(reweighted.probs1, test1.y)))
rownames(res.testing1)[nrow(res.testing1)] <- 'SVMPoly'
```

```{r}
#predict on test data
pred2.y <- predict(model, pre17.testing.df2, type = "prob")[,2]

q1 <- sum(pre17.training.df$Preeclampsia == "yes") / length(pre17.training.df$Preeclampsia)
r1 <- sum(resampled.training.df$Preeclampsia == "yes") / length(resampled.training.df$Preeclampsia)

reweighted.probs2 <- sapply(pred2.y, reweight, q1 = q1, r1 = r1)

# out of sample performance metrics
test2.y <- as.numeric(pre17.testing.df2$Preeclampsia) -1

pROC::auc(test2.y, reweighted.probs2, direction = "<")

FNR(reweighted.probs2, test2.y)
FPR(reweighted.probs2, test2.y)

roc_SVMpoly_pe_test2 <- ROCit::rocit(score=reweighted.probs2,class=test2.y)
```

```{r, results = "hide"}
# Add to output
res.testing2 <- rbind.data.frame(res.testing2, c(pROC::auc(test2.y, reweighted.probs2, direction = "<"), FNR(reweighted.probs2, test2.y), FPR(reweighted.probs2, test2.y)))
rownames(res.testing2)[nrow(res.testing2)] <- 'SVMPoly'
```

# Random forest

```{r}
modelLookup("rf")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
rfgrid <-  expand.grid(mtry = c(1:18)
                        )
              
# training a randomForest classifier tree model while tuning parameters
model = caret::train(Preeclampsia~., data = pre17.training.df,
              method = "rf",
              trControl = train_control,
              metric = "ROC",
              importance = T,
              ntree = 80, 
              #maxnodes = 30,
              nodesize = 1, #default for classification
              tuneGrid = rfgrid)

#print(model)
```

```{r, include = FALSE}
plot(model)
```

```{r}
#predict on test data
pred1.y <- predict(model, pre17.testing.df1, type = "prob")[,2]

# out of sample performance metrics
test1.y <- as.numeric(pre17.testing.df1$Preeclampsia)-1

pROC::auc(test1.y, pred1.y, direction = "<")

FNR(pred1.y, test1.y)
FPR(pred1.y, test1.y)

roc_rf_pe_test1 <- ROCit::rocit(score=pred1.y,class=test1.y)
```

```{r, results = "hide"}
# Add to output
res.testing1 <- rbind.data.frame(res.testing1, c(pROC::auc(test1.y, pred1.y, direction = "<"), FNR(pred1.y, test1.y), FPR(pred1.y, test1.y)))
rownames(res.testing1)[nrow(res.testing1)] <- 'RandomForest'
```

```{r}
#predict on test data
pred2.y <- predict(model, pre17.testing.df2, type = "prob")[,2]

# out of sample performance metrics
test2.y <- as.numeric(pre17.testing.df2$Preeclampsia) -1

pROC::auc(test2.y, pred2.y, direction = "<")

FNR(pred2.y, test2.y)
FPR(pred2.y, test2.y)

roc_rf_pe_test2 <- ROCit::rocit(score=pred2.y,class=test2.y)
```

```{r, results = "hide"}
# Add to output
res.testing2 <- rbind.data.frame(res.testing2, c(pROC::auc(test2.y, pred2.y, direction = "<"), FNR(pred2.y, test2.y), FPR(pred2.y, test2.y)))
rownames(res.testing2)[nrow(res.testing2)] <- 'RandomForest'
```

#  Elastic net

```{r}
modelLookup("glmnet")
```

```{r}
train_control = trainControl(method = "cv", number = 10, search = "grid",
                             summaryFunction = twoClassSummary,
                             allowParallel = TRUE,
                             # Estimate class probabilities
                             classProbs=TRUE)

set.seed(111)
netgrid <-  expand.grid(alpha = c(0,0.1, 0.2, 0.5, 0.7, 1),lambda = c(0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.7,1))

# training a elastic net classifier tree model while tuning parameters
model = caret::train(Preeclampsia~., data = pre17.training.df,
              method = "glmnet",
              trControl = train_control,
              metric = "ROC",
              tuneGrid = netgrid)

# summarizing the results
#print(model)
```

```{r, include = FALSE}
plot(model)
```

```{r}
#predict on test data
pred1.y <- predict(model, pre17.testing.df1, type = "prob")[,2]

# out of sample performance metrics
test1.y <- as.numeric(pre17.testing.df1$Preeclampsia)-1

pROC::auc(test1.y, pred1.y, direction = "<")

FNR(pred1.y, test1.y)
FPR(pred1.y, test1.y)

roc_elastic_pe_test1 <- ROCit::rocit(score=pred1.y,class=test1.y)
```

```{r, results = "hide"}
# Add to output
res.testing1 <- rbind.data.frame(res.testing1, c(pROC::auc(test1.y, pred1.y, direction = "<"), FNR(pred1.y, test1.y), FPR(pred1.y, test1.y)))
rownames(res.testing1)[nrow(res.testing1)] <- 'ElasticNet'
```

```{r}
#predict on test data
pred2.y <- predict(model, pre17.testing.df2, type = "prob")[,2]

# out of sample performance metrics
test2.y <- as.numeric(pre17.testing.df2$Preeclampsia) -1

pROC::auc(test2.y, pred2.y, direction = "<")

FNR(pred2.y, test2.y)
FPR(pred2.y, test2.y)

roc_elastic_pe_test2 <- ROCit::rocit(score=pred2.y,class=test2.y)
```

```{r, results = "hide"}
# Add to output
res.testing2 <- rbind.data.frame(res.testing2, c(pROC::auc(test2.y, pred2.y, direction = "<"), FNR(pred2.y, test2.y), FPR(pred2.y, test2.y)))
rownames(res.testing2)[nrow(res.testing2)] <- 'ElasticNet'
```


# Keras NN

```{r}
library(tensorflow)
library(keras)
library(tfruns)
tensorflow::set_random_seed(111)
```

```{r}
x_train <- as.matrix(pre17.training.df[,-1])
y_train <- as.matrix(as.numeric(pre17.training.df$Preeclampsia)-1)

x_test1<- as.matrix(pre17.testing.df1[,-1])
y_test1 <- as.matrix(as.numeric(pre17.testing.df1$Preeclampsia)-1)

x_test2<- as.matrix(pre17.testing.df2[,-1])
y_test2 <- as.matrix(as.numeric(pre17.testing.df2$Preeclampsia)-1)

x_train_shape <- length(colnames(x_train))
```


```{r}
model <- keras_model_sequential()
model %>%
  layer_dense(units = 1000, activation = 'relu',
              input_shape = c(x_train_shape),
              kernel_regularizer = regularizer_l1_l2(l1 = 0.0000001, l2 = 0.000001),
              bias_regularizer = regularizer_l1_l2(l1 = 0.00001, l2 = 0.0001),
              kernel_constraint =constraint_maxnorm(max_value = 2, axis = 0),
              #bias_constraint =constraint_maxnorm(max_value = 3, axis = 0),
              activity_regularizer= regularizer_l1_l2(l1 = 0.01, l2 = 0.00001),
              ) %>%  
  layer_dropout(rate = 0.7) %>% 
  layer_batch_normalization() %>%
  layer_dense(units = 350, activation = 'relu',
              kernel_regularizer = regularizer_l1_l2(l1 = 0.1, l2 = 0.1),
              kernel_constraint = constraint_minmaxnorm(max_value = 2, min_value = 0, axis = 1),
              bias_regularizer = regularizer_l1_l2(l1 = 0.00001, l2 = 0.000001),
              #bias_constraint =constraint_maxnorm(max_value = 3, axis = 0),
              activity_regularizer = regularizer_l1_l2(l1 = 0.1, l2 = 0.000001),
              ) %>%
  layer_dropout(rate = 0.3) %>%
  layer_batch_normalization() %>%
  layer_dense(units = 125, activation = 'relu',
              kernel_regularizer = regularizer_l1_l2(l1 = 0.01, l2 = 0.01),
              kernel_constraint = constraint_minmaxnorm(max_value = 2, min_value = 0, axis = 1),
              #bias_regularizer = regularizer_l1_l2(l1 = 0.00001, l2 = 0.000001),
              #bias_constraint =constraint_maxnorm(max_value = 3, axis = 0),
              activity_regularizer = regularizer_l1_l2(l1 = 0.001, l2 = 0.000001),
              ) %>%
  layer_dropout(rate = 0.3) %>%
  layer_batch_normalization() %>%
  layer_dense(units = 1, activation = 'sigmoid')
```



```{r}
loss_fn <- loss_binary_crossentropy()
auc <- metric_auc()
adam <- optimizer_adam(learning_rate = 0.0001, ema_momentum = 0.8)

model %>% compile(
  optimizer = adam,
  loss = loss_fn,
  metrics = "AUC"
)
```

```{r, results= "hide"}
model %>% fit(x_train, y_train, epochs = 75, batch_size =3)
```

TESTING 1
```{r}
model %>% evaluate(x_test1,  y_test1, verbose = 2)
```
TESTING 2
```{r}
model %>% evaluate(x_test2,  y_test2, verbose = 2)
```


```{r}
b.1<-model %>% predict(x_test1) #%>% `>`(0.5) %>% k_cast("int32") 
b.1 <- as.numeric(b.1)

b.2<-model %>% predict(x_test2) #%>% `>`(0.5) %>% k_cast("int32") 
b.2 <- as.numeric(b.2)
```

Testing 1

```{r}
pROC::auc(as.numeric(y_test1), b.1, direction = "<")

FNR(b.1, y_test1)
FPR(b.1, y_test1)

roc_nnet_pe_test1 <- ROCit::rocit(score=b.1,class=as.numeric(y_test1))

```

```{r, results = "hide"}
# Add to output
res.testing1 <- rbind.data.frame(res.testing1, c(pROC::auc(as.numeric(y_test1), b.1), FNR(b.1, y_test1), FPR(b.1, y_test1)))
rownames(res.testing1)[nrow(res.testing1)] <- 'KerasNN'
```

Testing 2 

```{r}
pROC::auc(as.numeric(y_test2), b.2, direction = "<")

FNR(b.2, y_test2)
FPR(b.2, y_test2)

roc_nnet_pe_test2 <- ROCit::rocit(score=b.2,class=as.numeric(y_test2))
```


```{r, results = "hide"}
# Add to output
res.testing2 <- rbind.data.frame(res.testing2, c(pROC::auc(as.numeric(y_test2), b.2,  direction = "<"), FNR(b.2, y_test2), FPR(b.2, y_test2)))
rownames(res.testing2)[nrow(res.testing2)] <- 'KerasNN'
```

# ROC-AUC curve for all models

Testing 1

```{r, echo=FALSE}
plot(roc_xgboost_pe_test1$TPR ~ roc_xgboost_pe_test1$FPR, type = "n",  # "n" means no plotting of data
      xlab = "1 - Specificity (FPR)", ylab = "Sensitivity (TPR)")
abline(0, 1, col = "gray", lty = 2, lwd = 2)

lines(roc_xgboost_pe_test1$TPR + 0.01 ~ roc_xgboost_pe_test1$FPR, col = 1, lwd = 2, lty = 1) 
lines(roc_SVMlinear_pe_test1$TPR - 0.01 ~ roc_SVMlinear_pe_test1$FPR, col = 2, lwd = 2, lty = 1)
lines(roc_SVMlrbf_pe_test1$TPR + 0.02 ~ roc_SVMlrbf_pe_test1$FPR, col = 3,  lwd = 2, lty = 1) 
lines(roc_SVMpoly_pe_test1$TPR ~ roc_SVMpoly_pe_test1$FPR, col = 4,  lwd = 2, lty = 1)
lines(roc_rf_pe_test1$TPR ~ roc_rf_pe_test1$FPR, col = 5,  lwd = 2, lty = 1)
lines(roc_elastic_pe_test1$TPR ~ roc_elastic_pe_test1$FPR, col = 6, lwd = 2, lty = 1)
lines(roc_nnet_pe_test1$TPR  ~ roc_nnet_pe_test1$FPR, col = 7, lwd = 2, lty = 1) 

legend("bottomright", col = c(1,2,3,4,5,6,7),
       legend = c(paste("XGboost","AUC:",round(roc_xgboost_pe_test1$AUC,2)),
                  paste("SVM linear","AUC:",round(roc_SVMlinear_pe_test1$AUC,2)),
                  paste("SVM rbf","AUC:",round(roc_SVMlrbf_pe_test1$AUC,2)),
                  paste("SVM poly","AUC:",round(roc_SVMpoly_pe_test1$AUC,2)),
                  paste("Random forest","AUC:",round(roc_rf_pe_test1$AUC,2)),
                  paste("Elastic net","AUC:",round(roc_elastic_pe_test1$AUC,2)),
                  paste("Neural network","AUC:",round(roc_nnet_pe_test1$AUC,2))), 
       lwd = 2, lty = 1,
       cex = 0.8, 
       pt.cex = 0.8, 
       x.intersp = 1, y.intersp = 1)
```

Testing 2

```{r, echo=FALSE}
plot(roc_xgboost_pe_test2$TPR ~ roc_xgboost_pe_test2$FPR, type = "n",  # "n" means no plotting of data
   xlab = "1 - Specificity (FPR)", ylab = "Sensitivity (TPR)")
abline(0, 1, col = "gray", lty = 2, lwd = 2)

lines(roc_SVMlrbf_pe_test2$TPR + 0.005 ~ roc_SVMlrbf_pe_test2$FPR, col = 3, lwd = 2, lty = 1)
lines(roc_xgboost_pe_test2$TPR - 0.015 ~ roc_xgboost_pe_test2$FPR, col = 1, lwd = 2, lty = 1)
lines(roc_SVMlinear_pe_test2$TPR + 0.000 ~ roc_SVMlinear_pe_test2$FPR, col = 2, lwd = 2, lty = 1)
lines(roc_SVMpoly_pe_test2$TPR + 0.010 ~ roc_SVMpoly_pe_test2$FPR, col = 4, lwd = 2, lty = 1)
lines(roc_rf_pe_test2$TPR - 0.005 ~ roc_rf_pe_test2$FPR, col = 5, lwd = 2, lty = 1)
lines(roc_elastic_pe_test2$TPR + 0.020 ~ roc_elastic_pe_test2$FPR, col = 6, lwd = 2, lty = 1)
lines(roc_nnet_pe_test2$TPR - 0.010 ~ roc_nnet_pe_test2$FPR, col = 7, lwd = 2, lty = 1)

legend("bottomright", col = c(1,2,3,4,5,6,7),
       legend = c(paste("XGboost","AUC:",round(roc_xgboost_pe_test2$AUC,2)),
                  paste("SVM linear","AUC:",round(roc_SVMlinear_pe_test2$AUC,2)),
                  paste("SVM rbf","AUC:",round(roc_SVMlrbf_pe_test2$AUC,2)),
                  paste("SVM poly","AUC:",round(roc_SVMpoly_pe_test2$AUC,2)),
                  paste("Random forest","AUC:",round(roc_rf_pe_test2$AUC,2)),
                  paste("Elastic net","AUC:",round(roc_elastic_pe_test2$AUC,2)),
                  paste("Neural network","AUC:",round(roc_nnet_pe_test2$AUC,2))), 
       lwd = 2, lty = 1,
       cex = 0.8, 
       pt.cex = 0.8, 
       x.intersp = 1, y.intersp = 1)
```

# Model perfomances/results in tables

```{r}
(res.testing1)
```

```{r}
(res.testing2)
```
# Session information

```{r, child="_session-info.Rmd"}
```

# References

